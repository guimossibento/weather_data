{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acafb40",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — TFT Temporal Chunking\n",
    "\n",
    "Compares **NeuralForecast TFT** vs **PyTorch-Forecasting TFT** (daytime-only) across:\n",
    "1. **Simple Sliding** — 2 chunks train, 1 val, 1 test\n",
    "2. **Expanding Rolling** — growing train, 1 val, 1 test\n",
    "3. **Split-Gap** — 40% early + 40% late (20% gap), simulating 2022+2025 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATHS ===\n",
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/tft_chunks\"\n",
    "\n",
    "# === SAMPLING ===\n",
    "SAMPLE_FRAC = 1.0\n",
    "MAX_BEACHES = None\n",
    "\n",
    "# === TFT PARAMETERS ===\n",
    "MAX_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 48\n",
    "HORIZON = 168  # 1 week = 7 days\n",
    "\n",
    "# === TIME ===\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6\n",
    "\n",
    "# === FLAGS ===\n",
    "RUN_SKLEARN = False\n",
    "RUN_NEURALFORECAST = True\n",
    "RUN_OPTUNA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"neuralforecast\", \"utilsforecast\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaae9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "import torch\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import TFT\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "\n",
    "HAS_NF = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ACCELERATOR = 'gpu'\n",
    "    DEVICES = 1\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    ACCELERATOR = 'mps'\n",
    "    DEVICES = 1\n",
    "else:\n",
    "    ACCELERATOR = 'cpu'\n",
    "    DEVICES = 1\n",
    "\n",
    "print(f\"Device: {ACCELERATOR} | PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13539953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, max_count):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_mae = (mae / max_count) * 100 if max_count > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE': rel_mae}\n",
    "\n",
    "def eval_per_beach(df, y_pred, beach_col='unique_id'):\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        y_true = df.loc[mask, 'y'].values if 'y' in df.columns else df.loc[mask, 'count'].values\n",
    "        y_p = y_pred[mask.values] if hasattr(mask, 'values') else y_pred[mask]\n",
    "        max_count = y_true.max()\n",
    "        m = calc_metrics(y_true, y_p, max_count)\n",
    "        m['camera'] = b\n",
    "        m['max_count'] = max_count\n",
    "        m['n'] = mask.sum()\n",
    "        results.append(m)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980944f",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(cache_dir, model):\n",
    "    cache_path = Path(cache_dir) / model\n",
    "    records = []\n",
    "    for jf in cache_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(jf) as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    \n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {\n",
    "            'beach': r.get('beach') or r.get('beach_folder'),\n",
    "            'beach_folder': r.get('beach_folder'),\n",
    "            'datetime': r.get('datetime'),\n",
    "            'count': r.get('count')\n",
    "        }\n",
    "        for k, v in r.get('weather', {}).items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, COUNTING_MODEL)\n",
    "print(f\"Loaded: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26af23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols']\n",
    "EXCLUDE_PREFIX = ['ibred', 'ClubNauticSoller', 'Guenthoer', 'youtube']\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw['beach_folder'].isin(EXCLUDE)]\n",
    "for p in EXCLUDE_PREFIX:\n",
    "    df_raw = df_raw[~df_raw['beach_folder'].str.startswith(p, na=False)]\n",
    "print(f\"Filtered: {before} -> {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53950688",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_FRAC < 1.0:\n",
    "    df_raw = df_raw.sample(frac=SAMPLE_FRAC, random_state=42).sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "if MAX_BEACHES:\n",
    "    top = df_raw['beach'].value_counts().head(MAX_BEACHES).index.tolist()\n",
    "    df_raw = df_raw[df_raw['beach'].isin(top)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "WEATHER_COLS = [c for c in df.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_COLS = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night']\n",
    "ALL_FEATURES = WEATHER_COLS + TEMPORAL_COLS\n",
    "\n",
    "df = df.dropna(subset=ALL_FEATURES + ['count']).reset_index(drop=True)\n",
    "good = df.groupby('beach')['count'].max()\n",
    "good = good[good > 20].index.tolist()\n",
    "df = df[df['beach'].isin(good)].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {len(df)} rows, {len(good)} beaches\")\n",
    "print(f\"Features: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff774dc",
   "metadata": {},
   "source": [
    "## Create Three Dataset Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ff4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_daytime = df[df['is_night'] == 0].copy().reset_index(drop=True)\n",
    "ds_full24h = df.copy()\n",
    "ds_night0 = df.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "ds_nightq1 = df.copy()\n",
    "q1_per_beach = ds_nightq1[ds_nightq1['is_night'] == 0].groupby('beach')['count'].quantile(0.25)\n",
    "ds_nightq1.loc[ds_nightq1['is_night'] == 1, 'count'] = ds_nightq1.loc[ds_nightq1['is_night'] == 1, 'beach'].map(q1_per_beach).fillna(0)\n",
    "\n",
    "ds_nightmin = df.copy()\n",
    "min_per_beach = ds_nightmin[ds_nightmin['is_night'] == 0].groupby('beach')['count'].min()\n",
    "ds_nightmin.loc[ds_nightmin['is_night'] == 1, 'count'] = ds_nightmin.loc[ds_nightmin['is_night'] == 1, 'beach'].map(min_per_beach).fillna(0)\n",
    "\n",
    "datasets = {'Daytime': ds_daytime, 'Full24h': ds_full24h, 'Night0': ds_night0, 'NightQ1': ds_nightq1, 'NightMin': ds_nightmin}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, d in datasets.items():\n",
    "    night_rows = d[d['is_night'] == 1] if 'is_night' in d.columns else pd.DataFrame()\n",
    "    day_rows = d[d['is_night'] == 0] if 'is_night' in d.columns else d\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total rows:     {len(d)}\")\n",
    "    print(f\"  Beaches:        {d['beach'].nunique()}\")\n",
    "    print(f\"  Night rows:     {len(night_rows)} ({len(night_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Day rows:       {len(day_rows)} ({len(day_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Count mean:     {d['count'].mean():.1f}\")\n",
    "    print(f\"  Count max:      {d['count'].max():.1f}\")\n",
    "    print(f\"  Zeros:          {(d['count'] == 0).sum()} ({(d['count'] == 0).sum()/len(d)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.preprocessing import fill_gaps\n",
    "\n",
    "def to_nf_format(df, id_col='beach_folder'):\n",
    "    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    nf_df = df[cols].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def prepare_nf_data(df, freq='h'):\n",
    "    nf = to_nf_format(df)\n",
    "    nf = nf.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    nf = fill_gaps(nf, freq=freq)\n",
    "    for col in nf.select_dtypes(include=[np.number]).columns:\n",
    "        nf[col] = nf.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "    return nf\n",
    "\n",
    "def prepare_nf_data_sequential(df, id_col='beach_folder'):\n",
    "    \"\"\"For daytime-only data: replace datetime with sequential integer index.\n",
    "    NeuralForecast needs a 'ds' column — we use integers so there are no gaps.\n",
    "    Temporal features (hour, day_of_week, etc.) are kept as exogenous variables.\"\"\"\n",
    "    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    nf = df[cols].copy()\n",
    "    nf = nf.rename(columns={id_col: 'unique_id', 'count': 'y'})\n",
    "    nf = nf.sort_values(['unique_id', 'datetime']).reset_index(drop=True)\n",
    "    nf = nf.groupby(['unique_id', 'datetime']).mean(numeric_only=True).reset_index()\n",
    "    nf['ds'] = nf.groupby('unique_id').cumcount()\n",
    "    nf = nf.drop(columns=['datetime'])\n",
    "    for col in nf.select_dtypes(include=[np.number]).columns:\n",
    "        if col != 'ds':\n",
    "            nf[col] = nf.groupby('unique_id')[col].transform(\n",
    "                lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "            )\n",
    "    return nf\n",
    "\n",
    "def create_temporal_chunks(df, n_chunks=6):\n",
    "    date_min = df['datetime'].min()\n",
    "    date_max = df['datetime'].max()\n",
    "    total_days = (date_max - date_min).days\n",
    "    chunk_days = total_days // n_chunks\n",
    "    boundaries = []\n",
    "    for i in range(n_chunks + 1):\n",
    "        boundaries.append(date_min + pd.Timedelta(days=i * chunk_days))\n",
    "    boundaries[-1] = date_max + pd.Timedelta(hours=1)\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        mask = (df['datetime'] >= boundaries[i]) & (df['datetime'] < boundaries[i+1])\n",
    "        chunk = df[mask].copy()\n",
    "        chunks.append({\n",
    "            'data': chunk,\n",
    "            'start': boundaries[i],\n",
    "            'end': boundaries[i+1] - pd.Timedelta(hours=1),\n",
    "            'label': f\"C{i+1}: {boundaries[i].strftime('%b %d')}\\u2192{(boundaries[i+1]-pd.Timedelta(hours=1)).strftime('%b %d')}\"\n",
    "        })\n",
    "    return chunks, boundaries\n",
    "\n",
    "N_CHUNKS = 6\n",
    "\n",
    "date_min = df['datetime'].min()\n",
    "date_max = df['datetime'].max()\n",
    "total_days = (date_max - date_min).days\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TEMPORAL CHUNKING: {total_days} days \\u2192 {N_CHUNKS} chunks of ~{total_days//N_CHUNKS} days\")\n",
    "print(f\"Date range: {date_min.date()} to {date_max.date()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_chunks = {}\n",
    "for ds_name in datasets.keys():\n",
    "    chunks, boundaries = create_temporal_chunks(datasets[ds_name], N_CHUNKS)\n",
    "    all_chunks[ds_name] = chunks\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    for c in chunks:\n",
    "        print(f\"  {c['label']} | {len(c['data'])} rows\")\n",
    "\n",
    "# --- SIMPLE SLIDING: train=2 chunks, val=1, test=1 ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SIMPLE SLIDING WINDOW (train=2 chunks, val=1, test=1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "simple_windows_all = {}\n",
    "for ds_name in datasets.keys():\n",
    "    chunks = all_chunks[ds_name]\n",
    "    windows = []\n",
    "    for i in range(len(chunks) - 3):\n",
    "        train_data = pd.concat([chunks[i]['data'], chunks[i+1]['data']])\n",
    "        windows.append({\n",
    "            'name': f\"SW{i+1}\",\n",
    "            'train_chunks': [chunks[i], chunks[i+1]],\n",
    "            'train_data': train_data,\n",
    "            'val_data': chunks[i+2]['data'],\n",
    "            'test_data': chunks[i+3]['data'],\n",
    "            'train_label': f\"{chunks[i]['label'].split(':')[0]}+{chunks[i+1]['label'].split(':')[0]} (2 chunks)\",\n",
    "            'val_label': chunks[i+2]['label'],\n",
    "            'test_label': chunks[i+3]['label'],\n",
    "        })\n",
    "    simple_windows_all[ds_name] = windows\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    for w in windows:\n",
    "        print(f\"  {w['name']}: Train={w['train_label']} ({len(w['train_data'])} rows) | \"\n",
    "              f\"Val={w['val_label']} | Test={w['test_label']}\")\n",
    "\n",
    "# --- ROLLING WINDOWS: train=expanding, val=1, test=1 ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROLLING WINDOW EXPERIMENTS (train=expanding, val=1 chunk, test=1 chunk)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rolling_windows = {}\n",
    "for ds_name in datasets.keys():\n",
    "    chunks = all_chunks[ds_name]\n",
    "    windows = []\n",
    "    for test_idx in range(2, N_CHUNKS):\n",
    "        val_idx = test_idx - 1\n",
    "        train_chunks = chunks[:val_idx]\n",
    "        windows.append({\n",
    "            'name': f\"W{len(windows)+1}\",\n",
    "            'train_chunks': train_chunks,\n",
    "            'train_data': pd.concat([c['data'] for c in train_chunks]),\n",
    "            'val_data': chunks[val_idx]['data'],\n",
    "            'test_data': chunks[test_idx]['data'],\n",
    "            'train_label': f\"{train_chunks[0]['label'].split(':')[0]}\\u2192{train_chunks[-1]['label'].split(':')[0]} ({len(train_chunks)} chunks)\",\n",
    "            'val_label': chunks[val_idx]['label'],\n",
    "            'test_label': chunks[test_idx]['label'],\n",
    "        })\n",
    "    rolling_windows[ds_name] = windows\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    for w in windows:\n",
    "        print(f\"  {w['name']}: Train={w['train_label']} ({len(w['train_data'])} rows) | \"\n",
    "              f\"Val={w['val_label']} | Test={w['test_label']}\")\n",
    "\n",
    "# --- SPLIT-GAP: first 40% + last 40%, skip middle 20% ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SPLIT-GAP EXPERIMENT (40% early | 20% gap | 40% late)\")\n",
    "print(\"Simulates: 2022 data + 2025 data, no data in between\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "split_gap_windows = {}\n",
    "for ds_name in datasets.keys():\n",
    "    ds = datasets[ds_name].sort_values('datetime').reset_index(drop=True)\n",
    "    n = len(ds)\n",
    "    early = ds.iloc[:int(n * 0.4)]\n",
    "    late = ds.iloc[int(n * 0.6):]\n",
    "    gap_start = ds.iloc[int(n * 0.4)]['datetime']\n",
    "    gap_end = ds.iloc[int(n * 0.6)]['datetime']\n",
    "\n",
    "    def split_period(period_df, period_name):\n",
    "        np_ = len(period_df)\n",
    "        tr = period_df.iloc[:int(np_ * 0.6)]\n",
    "        va = period_df.iloc[int(np_ * 0.6):int(np_ * 0.8)]\n",
    "        te = period_df.iloc[int(np_ * 0.8):]\n",
    "        return {\n",
    "            'name': period_name,\n",
    "            'train_data': tr, 'val_data': va, 'test_data': te,\n",
    "            'train_label': f\"{period_name} train ({tr['datetime'].min().strftime('%b %d')}\\u2192{tr['datetime'].max().strftime('%b %d')})\",\n",
    "            'val_label': f\"{period_name} val ({va['datetime'].min().strftime('%b %d')}\\u2192{va['datetime'].max().strftime('%b %d')})\",\n",
    "            'test_label': f\"{period_name} test ({te['datetime'].min().strftime('%b %d')}\\u2192{te['datetime'].max().strftime('%b %d')})\",\n",
    "        }\n",
    "\n",
    "    early_split = split_period(early, \"Early\")\n",
    "    late_split = split_period(late, \"Late\")\n",
    "    combined_train = pd.concat([early_split['train_data'], late_split['train_data']])\n",
    "    combined = {\n",
    "        'name': \"Combined\",\n",
    "        'train_data': combined_train,\n",
    "        'val_data': late_split['val_data'],\n",
    "        'test_data': late_split['test_data'],\n",
    "        'train_label': f\"Early+Late train ({len(combined_train)} rows, gap={gap_start.strftime('%b %d')}\\u2192{gap_end.strftime('%b %d')})\",\n",
    "        'val_label': late_split['val_label'],\n",
    "        'test_label': late_split['test_label'],\n",
    "    }\n",
    "    split_gap_windows[ds_name] = {\n",
    "        'early': early_split, 'late': late_split, 'combined': combined,\n",
    "        'gap_start': gap_start, 'gap_end': gap_end,\n",
    "    }\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    print(f\"  Early: {early['datetime'].min().strftime('%b %d')} \\u2192 {early['datetime'].max().strftime('%b %d')} ({len(early)} rows)\")\n",
    "    print(f\"  Gap:   {gap_start.strftime('%b %d')} \\u2192 {gap_end.strftime('%b %d')}\")\n",
    "    print(f\"  Late:  {late['datetime'].min().strftime('%b %d')} \\u2192 {late['datetime'].max().strftime('%b %d')} ({len(late)} rows)\")\n",
    "    for exp in [early_split, late_split, combined]:\n",
    "        print(f\"    {exp['name']:10s} | train={len(exp['train_data']):5d} | val={len(exp['val_data']):5d} | test={len(exp['test_data']):5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. SIMPLE SLIDING WINDOW (2 chunks train) ===\n",
    "chunks = all_chunks['Night0']\n",
    "simple_windows = simple_windows_all['Night0']\n",
    "example_uid = df['beach_folder'].value_counts().idxmax()\n",
    "ds_show = datasets['Night0']\n",
    "example = ds_show[ds_show['beach_folder'] == example_uid].sort_values('datetime')\n",
    "\n",
    "c_train = '#2196F3'\n",
    "c_val = '#FF9800'\n",
    "c_test = '#F44336'\n",
    "\n",
    "fig, axes = plt.subplots(len(simple_windows) + 1, 1,\n",
    "                         figsize=(18, 3 * (len(simple_windows) + 1)), sharex=True)\n",
    "axes[0].plot(example['datetime'], example['count'], linewidth=0.6, color='gray')\n",
    "for c in chunks:\n",
    "    axes[0].axvline(c['start'], color='black', linestyle=':', linewidth=0.8, alpha=0.5)\n",
    "    axes[0].text(c['start'], example['count'].max() * 0.95, c['label'][:15], fontsize=7, va='top')\n",
    "axes[0].set_title(f'Full Series \\u2014 {example_uid[:40]} | {N_CHUNKS} chunks', fontsize=10, loc='left')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "for w_idx, w in enumerate(simple_windows):\n",
    "    ax = axes[w_idx + 1]\n",
    "    ax.plot(example['datetime'], example['count'], linewidth=0.3, color='lightgray', zorder=1)\n",
    "    for tc in w['train_chunks']:\n",
    "        ax.axvspan(tc['start'], tc['end'], alpha=0.25, color=c_train, zorder=2)\n",
    "    val_chunk = chunks[w_idx + 2]\n",
    "    test_chunk = chunks[w_idx + 3]\n",
    "    ax.axvspan(val_chunk['start'], val_chunk['end'], alpha=0.25, color=c_val, zorder=2)\n",
    "    ax.axvspan(test_chunk['start'], test_chunk['end'], alpha=0.3, color=c_test, zorder=2)\n",
    "    t_patch = mpatches.Patch(color=c_train, alpha=0.4, label=f\"Train: {w['train_label']}\")\n",
    "    v_patch = mpatches.Patch(color=c_val, alpha=0.4, label=f\"Val: {w['val_label']}\")\n",
    "    te_patch = mpatches.Patch(color=c_test, alpha=0.4, label=f\"Test: {w['test_label']}\")\n",
    "    ax.legend(handles=[t_patch, v_patch, te_patch], loc='upper right', fontsize=7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f\"{w['name']}: {w['train_label']} \\u2192 val \\u2192 test\", fontsize=10, loc='left')\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.suptitle('Simple Sliding Window \\u2014 Train=2 Chunks, Val=1, Test=1', fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 2. EXPANDING ROLLING WINDOW ===\n",
    "rw = rolling_windows['Night0']\n",
    "fig, axes = plt.subplots(len(rw) + 1, 1, figsize=(18, 3 * (len(rw) + 1)), sharex=True)\n",
    "axes[0].plot(example['datetime'], example['count'], linewidth=0.6, color='gray')\n",
    "for c in chunks:\n",
    "    axes[0].axvline(c['start'], color='black', linestyle=':', linewidth=0.8, alpha=0.5)\n",
    "    axes[0].text(c['start'], example['count'].max() * 0.95, c['label'][:15], fontsize=7, va='top')\n",
    "axes[0].set_title(f'Full Series \\u2014 {example_uid[:40]}', fontsize=10, loc='left')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "for w_idx, w in enumerate(rw):\n",
    "    ax = axes[w_idx + 1]\n",
    "    ax.plot(example['datetime'], example['count'], linewidth=0.3, color='lightgray', zorder=1)\n",
    "    for tc in w['train_chunks']:\n",
    "        ax.axvspan(tc['start'], tc['end'], alpha=0.25, color=c_train, zorder=2)\n",
    "    val_chunk = chunks[[i for i, c in enumerate(chunks) if c['label'] == w['val_label']][0]]\n",
    "    test_chunk = chunks[[i for i, c in enumerate(chunks) if c['label'] == w['test_label']][0]]\n",
    "    ax.axvspan(val_chunk['start'], val_chunk['end'], alpha=0.25, color=c_val, zorder=2)\n",
    "    ax.axvspan(test_chunk['start'], test_chunk['end'], alpha=0.3, color=c_test, zorder=2)\n",
    "    t_patch = mpatches.Patch(color=c_train, alpha=0.4, label=f\"Train: {w['train_label']}\")\n",
    "    v_patch = mpatches.Patch(color=c_val, alpha=0.4, label=f\"Val: {w['val_label']}\")\n",
    "    te_patch = mpatches.Patch(color=c_test, alpha=0.4, label=f\"Test: {w['test_label']}\")\n",
    "    ax.legend(handles=[t_patch, v_patch, te_patch], loc='upper right', fontsize=7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f\"{w['name']}: train={len(w['train_chunks'])} chunks (expanding)\", fontsize=10, loc='left')\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.suptitle('Expanding Rolling Window', fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 3. SPLIT-GAP VISUALIZATION ===\n",
    "sg = split_gap_windows['Night0']\n",
    "c_gap = '#9E9E9E'\n",
    "fig, axes = plt.subplots(4, 1, figsize=(18, 12), sharex=True)\n",
    "axes[0].plot(example['datetime'], example['count'], linewidth=0.6, color='gray')\n",
    "axes[0].axvspan(sg['gap_start'], sg['gap_end'], alpha=0.3, color=c_gap, zorder=2)\n",
    "axes[0].text(sg['gap_start'] + (sg['gap_end'] - sg['gap_start'])/2, example['count'].max() * 0.9,\n",
    "            'GAP (20%)', ha='center', fontsize=9, fontweight='bold', color='gray')\n",
    "axes[0].set_title('Full Series \\u2014 Split-Gap Layout', fontsize=10, loc='left')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "for ax_idx, (label, key) in enumerate(zip(['Early Only', 'Late Only', 'Combined'], ['early', 'late', 'combined'])):\n",
    "    ax = axes[ax_idx + 1]\n",
    "    ax.plot(example['datetime'], example['count'], linewidth=0.3, color='lightgray', zorder=1)\n",
    "    ax.axvspan(sg['gap_start'], sg['gap_end'], alpha=0.15, color=c_gap, zorder=2)\n",
    "    exp = sg[key]\n",
    "    tr, va, te = exp['train_data'], exp['val_data'], exp['test_data']\n",
    "    ax.axvspan(tr['datetime'].min(), tr['datetime'].max(), alpha=0.25, color=c_train, zorder=3)\n",
    "    ax.axvspan(va['datetime'].min(), va['datetime'].max(), alpha=0.25, color=c_val, zorder=3)\n",
    "    ax.axvspan(te['datetime'].min(), te['datetime'].max(), alpha=0.3, color=c_test, zorder=3)\n",
    "    t_patch = mpatches.Patch(color=c_train, alpha=0.4, label=f\"Train ({len(tr)} rows)\")\n",
    "    v_patch = mpatches.Patch(color=c_val, alpha=0.4, label=f\"Val ({len(va)} rows)\")\n",
    "    te_patch = mpatches.Patch(color=c_test, alpha=0.4, label=f\"Test ({len(te)} rows)\")\n",
    "    g_patch = mpatches.Patch(color=c_gap, alpha=0.3, label='Gap (20%)')\n",
    "    ax.legend(handles=[t_patch, v_patch, te_patch, g_patch], loc='upper right', fontsize=7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(label, fontsize=10, loc='left')\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.suptitle('Split-Gap \\u2014 Simulating Disconnected Periods (2022 + 2025)', fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8846f",
   "metadata": {},
   "source": [
    "## TFT — Sliding Window Experiments\n",
    "\n",
    "For each dataset strategy, train TFT on chunk *i*, validate on chunk *i+1*, test on chunk *i+2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "all_cv_details = []\n",
    "\n",
    "def run_tft_experiment(exp_name, exp_type, ds_name, train_data, val_data, test_data,\n",
    "                       train_label, val_label, test_label):\n",
    "    nf_train = prepare_nf_data(train_data)\n",
    "    nf_val = prepare_nf_data(val_data)\n",
    "    nf_test = prepare_nf_data(test_data)\n",
    "\n",
    "    common_ids = (set(nf_train['unique_id'].unique()) &\n",
    "                  set(nf_val['unique_id'].unique()) &\n",
    "                  set(nf_test['unique_id'].unique()))\n",
    "    if len(common_ids) == 0:\n",
    "        print(f\"  {exp_name}: no common series, skipping\")\n",
    "        return\n",
    "\n",
    "    nf_train = nf_train[nf_train['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_val = nf_val[nf_val['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "\n",
    "    nf_all = pd.concat([nf_train, nf_val, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    nf_all = nf_all.drop_duplicates(subset=['unique_id', 'ds'], keep='last').reset_index(drop=True)\n",
    "\n",
    "    test_horizon = nf_test.groupby('unique_id').size().min()\n",
    "    horizon = min(HORIZON, test_horizon)\n",
    "\n",
    "    print(f\"\\n  {exp_name} | h={horizon} | series={len(common_ids)}\")\n",
    "    print(f\"    Train: {train_label} ({len(train_data)} rows)\")\n",
    "    print(f\"    Val:   {val_label} | Test: {test_label}\")\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        model = TFT(\n",
    "            h=horizon, input_size=INPUT_SIZE, hidden_size=64, n_head=4,\n",
    "            max_steps=MAX_STEPS, early_stop_patience_steps=-1,\n",
    "            learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE,\n",
    "            scaler_type='robust', random_seed=42,\n",
    "            accelerator=ACCELERATOR, devices=DEVICES,\n",
    "            loss=MAE(), hist_exog_list=ALL_FEATURES, val_check_steps=50,\n",
    "        )\n",
    "        nf = NeuralForecast(models=[model], freq='h')\n",
    "        cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "        cv_results['pred'] = np.clip(cv_results[pred_col].values, 0, None)\n",
    "        cv_results['hour'] = cv_results['ds'].dt.hour\n",
    "        cv_results['is_night'] = ((cv_results['hour'] >= NIGHT_START) | (cv_results['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "        m_full = calc_metrics(cv_results['y'].values, cv_results['pred'].values, cv_results['y'].max())\n",
    "        day_mask = cv_results['is_night'] == 0\n",
    "        m_day = calc_metrics(cv_results.loc[day_mask, 'y'].values,\n",
    "                             cv_results.loc[day_mask, 'pred'].values,\n",
    "                             cv_results.loc[day_mask, 'y'].max()) if day_mask.sum() > 10 else m_full\n",
    "\n",
    "        beach_results = []\n",
    "        for b in cv_results['unique_id'].unique():\n",
    "            bmask = (cv_results['unique_id'] == b) & day_mask\n",
    "            if bmask.sum() < 3:\n",
    "                continue\n",
    "            yt = cv_results.loc[bmask, 'y'].values\n",
    "            yp = cv_results.loc[bmask, 'pred'].values\n",
    "            bm = calc_metrics(yt, yp, yt.max())\n",
    "            bm['beach'] = b\n",
    "            bm['max_count'] = yt.max()\n",
    "            beach_results.append(bm)\n",
    "\n",
    "        beach_df = pd.DataFrame(beach_results)\n",
    "        avg_rel_day = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "\n",
    "        all_results.append({\n",
    "            'Model': 'NF_TFT', 'Dataset': ds_name, 'Experiment': exp_type,\n",
    "            'Window': exp_name,\n",
    "            'MAE': m_full['MAE'], 'RMSE': m_full['RMSE'], 'R2': m_full['R2'],\n",
    "            'AvgRelMAE': m_full['RelMAE'],\n",
    "            'MAE_day': m_day['MAE'], 'R2_day': m_day['R2'], 'AvgRelMAE_day': avg_rel_day,\n",
    "            'Time': elapsed,\n",
    "            'train_period': train_label, 'val_period': val_label, 'test_period': test_label,\n",
    "            'train_rows': len(train_data),\n",
    "        })\n",
    "        all_cv_details.append({\n",
    "            'ds_name': ds_name, 'exp_type': exp_type, 'window': exp_name,\n",
    "            'model': 'NF_TFT',\n",
    "            'merged': cv_results[['unique_id', 'ds', 'y', 'pred', 'hour', 'is_night']].copy(),\n",
    "            'pred_col': 'pred', 'beach_df': beach_df,\n",
    "        })\n",
    "        print(f\"    {elapsed:.0f}s | MAE={m_full['MAE']:.1f} (day={m_day['MAE']:.1f}) | \"\n",
    "              f\"RelMAE_day={avg_rel_day:.1f}% | R2={m_full['R2']:.3f} (day={m_day['R2']:.3f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ============================\n",
    "# RUN ALL EXPERIMENT TYPES\n",
    "# ============================\n",
    "for ds_name in ['Full24h', 'Night0', 'NightQ1', 'NightMin']:\n",
    "    chunks = all_chunks[ds_name]\n",
    "\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# SIMPLE SLIDING \\u2014 {ds_name}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    for w in simple_windows_all[ds_name]:\n",
    "        run_tft_experiment(\n",
    "            exp_name=w['name'], exp_type=\"Simple\", ds_name=ds_name,\n",
    "            train_data=w['train_data'], val_data=w['val_data'], test_data=w['test_data'],\n",
    "            train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# EXPANDING ROLLING \\u2014 {ds_name}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    for w in rolling_windows[ds_name]:\n",
    "        run_tft_experiment(\n",
    "            exp_name=w['name'], exp_type=\"Expanding\", ds_name=ds_name,\n",
    "            train_data=w['train_data'], val_data=w['val_data'], test_data=w['test_data'],\n",
    "            train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# SPLIT-GAP \\u2014 {ds_name}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    sg = split_gap_windows[ds_name]\n",
    "    for key, label in [('early', 'SG_Early'), ('late', 'SG_Late'), ('combined', 'SG_Combined')]:\n",
    "        exp = sg[key]\n",
    "        run_tft_experiment(\n",
    "            exp_name=label, exp_type=\"SplitGap\", ds_name=ds_name,\n",
    "            train_data=exp['train_data'], val_data=exp['val_data'], test_data=exp['test_data'],\n",
    "            train_label=exp['train_label'], val_label=exp['val_label'], test_label=exp['test_label'],\n",
    "        )\n",
    "\n",
    "# ============================\n",
    "# DAYTIME (sequential index, no night data)\n",
    "# ============================\n",
    "def run_tft_experiment_seq(exp_name, exp_type, ds_name, train_data, val_data, test_data,\n",
    "                           train_label, val_label, test_label):\n",
    "    \"\"\"Same as run_tft_experiment but uses sequential integer index for daytime-only data.\"\"\"\n",
    "    nf_train = prepare_nf_data_sequential(train_data)\n",
    "    nf_val = prepare_nf_data_sequential(val_data)\n",
    "    nf_test = prepare_nf_data_sequential(test_data)\n",
    "\n",
    "    common_ids = (set(nf_train['unique_id'].unique()) &\n",
    "                  set(nf_val['unique_id'].unique()) &\n",
    "                  set(nf_test['unique_id'].unique()))\n",
    "    if len(common_ids) == 0:\n",
    "        print(f\"  {exp_name}: no common series, skipping\")\n",
    "        return\n",
    "\n",
    "    nf_train = nf_train[nf_train['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_val = nf_val[nf_val['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "\n",
    "    # Rebuild sequential ds across train+val+test per unique_id\n",
    "    parts = []\n",
    "    for uid in sorted(common_ids):\n",
    "        tr = nf_train[nf_train['unique_id'] == uid].copy()\n",
    "        va = nf_val[nf_val['unique_id'] == uid].copy()\n",
    "        te = nf_test[nf_test['unique_id'] == uid].copy()\n",
    "        combined = pd.concat([tr, va, te]).reset_index(drop=True)\n",
    "        combined['ds'] = range(len(combined))\n",
    "        parts.append(combined)\n",
    "    nf_all = pd.concat(parts).reset_index(drop=True)\n",
    "\n",
    "    test_len = nf_test.groupby('unique_id').size().min()\n",
    "    horizon = min(HORIZON, test_len)\n",
    "\n",
    "    print(f\"\\n  {exp_name} | h={horizon} | series={len(common_ids)} | sequential ds\")\n",
    "    print(f\"    Train: {train_label} ({len(train_data)} rows)\")\n",
    "    print(f\"    Val:   {val_label} | Test: {test_label}\")\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        model = TFT(\n",
    "            h=horizon, input_size=INPUT_SIZE, hidden_size=64, n_head=4,\n",
    "            max_steps=MAX_STEPS, early_stop_patience_steps=-1,\n",
    "            learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE,\n",
    "            scaler_type='robust', random_seed=42,\n",
    "            accelerator=ACCELERATOR, devices=DEVICES,\n",
    "            loss=MAE(), hist_exog_list=ALL_FEATURES, val_check_steps=50,\n",
    "        )\n",
    "        nf = NeuralForecast(models=[model], freq=1)\n",
    "        cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "        cv_results['pred'] = np.clip(cv_results[pred_col].values, 0, None)\n",
    "\n",
    "        m = calc_metrics(cv_results['y'].values, cv_results['pred'].values, cv_results['y'].max())\n",
    "\n",
    "        beach_results = []\n",
    "        for b in cv_results['unique_id'].unique():\n",
    "            bmask = cv_results['unique_id'] == b\n",
    "            if bmask.sum() < 3:\n",
    "                continue\n",
    "            yt = cv_results.loc[bmask, 'y'].values\n",
    "            yp = cv_results.loc[bmask, 'pred'].values\n",
    "            bm = calc_metrics(yt, yp, yt.max())\n",
    "            bm['beach'] = b\n",
    "            bm['max_count'] = yt.max()\n",
    "            beach_results.append(bm)\n",
    "\n",
    "        beach_df = pd.DataFrame(beach_results)\n",
    "        avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "\n",
    "        all_results.append({\n",
    "            'Model': 'NF_TFT', 'Dataset': ds_name, 'Experiment': exp_type,\n",
    "            'Window': exp_name,\n",
    "            'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "            'AvgRelMAE': m['RelMAE'],\n",
    "            'MAE_day': m['MAE'], 'R2_day': m['R2'], 'AvgRelMAE_day': avg_rel,\n",
    "            'Time': elapsed,\n",
    "            'train_period': train_label, 'val_period': val_label, 'test_period': test_label,\n",
    "            'train_rows': len(train_data),\n",
    "        })\n",
    "        all_cv_details.append({\n",
    "            'ds_name': ds_name, 'exp_type': exp_type, 'window': exp_name,\n",
    "            'model': 'NF_TFT',\n",
    "            'merged': cv_results[['unique_id', 'ds', 'y', 'pred']].copy(),\n",
    "            'pred_col': 'pred', 'beach_df': beach_df,\n",
    "        })\n",
    "        print(f\"    {elapsed:.0f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Build Daytime chunks + windows\n",
    "ds_day = datasets['Daytime']\n",
    "chunks_day, _ = create_temporal_chunks(ds_day, N_CHUNKS)\n",
    "all_chunks['Daytime'] = chunks_day\n",
    "\n",
    "# Simple sliding for Daytime\n",
    "simple_windows_all['Daytime'] = []\n",
    "for i in range(len(chunks_day) - 3):\n",
    "    train_data = pd.concat([chunks_day[i]['data'], chunks_day[i+1]['data']])\n",
    "    simple_windows_all['Daytime'].append({\n",
    "        'name': f\"SW{i+1}\",\n",
    "        'train_chunks': [chunks_day[i], chunks_day[i+1]],\n",
    "        'train_data': train_data,\n",
    "        'val_data': chunks_day[i+2]['data'],\n",
    "        'test_data': chunks_day[i+3]['data'],\n",
    "        'train_label': f\"{chunks_day[i]['label'].split(':')[0]}+{chunks_day[i+1]['label'].split(':')[0]} (2ch)\",\n",
    "        'val_label': chunks_day[i+2]['label'],\n",
    "        'test_label': chunks_day[i+3]['label'],\n",
    "    })\n",
    "\n",
    "# Rolling for Daytime\n",
    "rolling_windows['Daytime'] = []\n",
    "for test_idx in range(2, N_CHUNKS):\n",
    "    val_idx = test_idx - 1\n",
    "    train_chunks = chunks_day[:val_idx]\n",
    "    rolling_windows['Daytime'].append({\n",
    "        'name': f\"W{test_idx-1}\",\n",
    "        'train_chunks': train_chunks,\n",
    "        'train_data': pd.concat([c['data'] for c in train_chunks]),\n",
    "        'val_data': chunks_day[val_idx]['data'],\n",
    "        'test_data': chunks_day[test_idx]['data'],\n",
    "        'train_label': f\"{train_chunks[0]['label'].split(':')[0]}\\u2192{train_chunks[-1]['label'].split(':')[0]} ({len(train_chunks)}ch)\",\n",
    "        'val_label': chunks_day[val_idx]['label'],\n",
    "        'test_label': chunks_day[test_idx]['label'],\n",
    "    })\n",
    "\n",
    "# Split-gap for Daytime\n",
    "ds_day_sorted = ds_day.sort_values('datetime').reset_index(drop=True)\n",
    "n_day = len(ds_day_sorted)\n",
    "early_d = ds_day_sorted.iloc[:int(n_day * 0.4)]\n",
    "late_d = ds_day_sorted.iloc[int(n_day * 0.6):]\n",
    "gap_s = ds_day_sorted.iloc[int(n_day * 0.4)]['datetime']\n",
    "gap_e = ds_day_sorted.iloc[int(n_day * 0.6)]['datetime']\n",
    "\n",
    "def _split(prd, nm):\n",
    "    np_ = len(prd)\n",
    "    tr = prd.iloc[:int(np_ * 0.6)]\n",
    "    va = prd.iloc[int(np_ * 0.6):int(np_ * 0.8)]\n",
    "    te = prd.iloc[int(np_ * 0.8):]\n",
    "    return {'name': nm, 'train_data': tr, 'val_data': va, 'test_data': te,\n",
    "            'train_label': f\"{nm} train\", 'val_label': f\"{nm} val\", 'test_label': f\"{nm} test\"}\n",
    "\n",
    "sg_e = _split(early_d, \"Early\")\n",
    "sg_l = _split(late_d, \"Late\")\n",
    "sg_c = {'name': \"Combined\", 'train_data': pd.concat([sg_e['train_data'], sg_l['train_data']]),\n",
    "         'val_data': sg_l['val_data'], 'test_data': sg_l['test_data'],\n",
    "         'train_label': \"Early+Late train\", 'val_label': sg_l['val_label'], 'test_label': sg_l['test_label']}\n",
    "\n",
    "split_gap_windows['Daytime'] = {'early': sg_e, 'late': sg_l, 'combined': sg_c,\n",
    "                                 'gap_start': gap_s, 'gap_end': gap_e}\n",
    "\n",
    "# Run Daytime experiments with sequential index\n",
    "print(f\"\\n{'#' * 70}\")\n",
    "print(f\"# NF_TFT DAYTIME (sequential index) \\u2014 Simple Sliding\")\n",
    "print(f\"{'#' * 70}\")\n",
    "for w in simple_windows_all['Daytime']:\n",
    "    run_tft_experiment_seq(\n",
    "        exp_name=w['name'], exp_type=\"Simple\", ds_name=\"Daytime\",\n",
    "        train_data=w['train_data'], val_data=w['val_data'], test_data=w['test_data'],\n",
    "        train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'#' * 70}\")\n",
    "print(f\"# NF_TFT DAYTIME (sequential index) \\u2014 Expanding Rolling\")\n",
    "print(f\"{'#' * 70}\")\n",
    "for w in rolling_windows['Daytime']:\n",
    "    run_tft_experiment_seq(\n",
    "        exp_name=w['name'], exp_type=\"Expanding\", ds_name=\"Daytime\",\n",
    "        train_data=w['train_data'], val_data=w['val_data'], test_data=w['test_data'],\n",
    "        train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'#' * 70}\")\n",
    "print(f\"# NF_TFT DAYTIME (sequential index) \\u2014 Split-Gap\")\n",
    "print(f\"{'#' * 70}\")\n",
    "sg = split_gap_windows['Daytime']\n",
    "for key, label in [('early', 'SG_Early'), ('late', 'SG_Late'), ('combined', 'SG_Combined')]:\n",
    "    exp = sg[key]\n",
    "    run_tft_experiment_seq(\n",
    "        exp_name=label, exp_type=\"SplitGap\", ds_name=\"Daytime\",\n",
    "        train_data=exp['train_data'], val_data=exp['val_data'], test_data=exp['test_data'],\n",
    "        train_label=exp['train_label'], val_label=exp['val_label'], test_label=exp['test_label'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193d890",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e40336",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'tft_chunk_results.csv', index=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ALL RESULTS (NeuralForecast TFT)\")\n",
    "print(\"=\" * 70)\n",
    "cols = ['Model', 'Dataset', 'Experiment', 'Window', 'train_rows', 'MAE', 'MAE_day', 'R2', 'R2_day', 'AvgRelMAE_day', 'Time']\n",
    "cols = [c for c in cols if c in results_df.columns]\n",
    "print(results_df[cols].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be05822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_type in results_df['Experiment'].unique():\n",
    "    sub = results_df[results_df['Experiment'] == exp_type]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"{exp_type} — AvgRelMAE (%)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    pivot = sub.pivot_table(index='Dataset', columns='Window', values='AvgRelMAE')\n",
    "    print(pivot.round(1).to_string())\n",
    "\n",
    "    print(f\"\\n{exp_type} — R²\")\n",
    "    pivot_r2 = sub.pivot_table(index='Dataset', columns='Window', values='R2')\n",
    "    print(pivot_r2.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3b513",
   "metadata": {},
   "source": [
    "### Experiment Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17960dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    metric_col = 'AvgRelMAE_day' if 'AvgRelMAE_day' in results_df.columns else 'AvgRelMAE'\n",
    "    r2_col = 'R2_day' if 'R2_day' in results_df.columns else 'R2'\n",
    "\n",
    "    exp_types = results_df['Experiment'].unique()\n",
    "    n_exp = len(exp_types)\n",
    "    fig, axes = plt.subplots(n_exp, 2, figsize=(16, 5 * n_exp))\n",
    "    if n_exp == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    for row, exp_type in enumerate(exp_types):\n",
    "        sub = results_df[results_df['Experiment'] == exp_type]\n",
    "        pivot = sub.pivot_table(index='Dataset', columns='Window', values=metric_col)\n",
    "        pivot.plot(kind='bar', ax=axes[row, 0], width=0.7, colormap='Set2')\n",
    "        axes[row, 0].set_ylabel('AvgRelMAE_day (%)')\n",
    "        axes[row, 0].set_title(f'{exp_type} \\u2014 RelMAE daytime (lower is better)')\n",
    "        axes[row, 0].tick_params(axis='x', rotation=30)\n",
    "        axes[row, 0].legend(title='Window', fontsize=7)\n",
    "        pivot_r2 = sub.pivot_table(index='Dataset', columns='Window', values=r2_col)\n",
    "        pivot_r2.plot(kind='bar', ax=axes[row, 1], width=0.7, colormap='Set2')\n",
    "        axes[row, 1].set_ylabel('R\\u00b2 (day)')\n",
    "        axes[row, 1].set_title(f'{exp_type} \\u2014 R\\u00b2 daytime (higher is better)')\n",
    "        axes[row, 1].tick_params(axis='x', rotation=30)\n",
    "        axes[row, 1].legend(title='Window', fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'tft_all_experiments.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ranking = results_df.groupby(['Dataset', 'Experiment'])[metric_col].mean().unstack()\n",
    "    ranking = ranking.loc[ranking.mean(axis=1).sort_values().index]\n",
    "    ranking.plot(kind='barh', ax=ax, width=0.7)\n",
    "    ax.set_xlabel('Mean AvgRelMAE_day (%)')\n",
    "    ax.set_title('Dataset Strategy Ranking by Experiment Type (Daytime Only)')\n",
    "    ax.legend(title='Experiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'tft_dataset_ranking.png', dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196207d",
   "metadata": {},
   "source": [
    "### Predictions vs Actual — All NF Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_cv_details) > 0:\n",
    "    for detail_idx, detail in enumerate(all_cv_details):\n",
    "        if detail_idx >= len(results_df):\n",
    "            break\n",
    "        row = results_df.iloc[detail_idx]\n",
    "        merged = detail['merged']\n",
    "\n",
    "        beaches = merged.groupby('unique_id').size().nlargest(4).index.tolist()\n",
    "        n_show = len(beaches)\n",
    "        fig, axes = plt.subplots(n_show, 1, figsize=(16, 3 * n_show), sharex=False)\n",
    "        if n_show == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, beach in zip(axes, beaches):\n",
    "            sub = merged[merged['unique_id'] == beach].sort_values('ds')\n",
    "            x_axis = sub['ds']\n",
    "            ax.plot(x_axis, sub['y'], label='Actual', color='steelblue', linewidth=1)\n",
    "            ax.plot(x_axis, sub['pred'], label='Predicted', color='coral', linewidth=1, alpha=0.8)\n",
    "            ax.fill_between(x_axis, sub['y'], sub['pred'], alpha=0.1, color='red')\n",
    "\n",
    "            if 'is_night' in sub.columns:\n",
    "                night = sub[sub['is_night'] == 1]\n",
    "                if len(night) > 0 and pd.api.types.is_datetime64_any_dtype(x_axis):\n",
    "                    for _, r in night.iterrows():\n",
    "                        ax.axvspan(r['ds'] - pd.Timedelta(minutes=30),\n",
    "                                   r['ds'] + pd.Timedelta(minutes=30), alpha=0.06, color='navy', zorder=0)\n",
    "                eval_sub = sub[sub['is_night'] == 0]\n",
    "            else:\n",
    "                eval_sub = sub\n",
    "\n",
    "            mae_v = np.abs(eval_sub['y'] - eval_sub['pred']).mean() if len(eval_sub) > 0 else 0\n",
    "            r2_v = r2_score(eval_sub['y'], eval_sub['pred']) if len(eval_sub) > 3 else 0\n",
    "            ax.set_title(f'{beach[:30]} | MAE={mae_v:.1f} | R\\u00b2={r2_v:.3f}', fontsize=9, loc='left')\n",
    "            ax.legend(loc='upper right', fontsize=7)\n",
    "            ax.set_ylabel('Count')\n",
    "\n",
    "        axes[-1].set_xlabel('Time')\n",
    "        rel_day = row.get('AvgRelMAE_day', row.get('AvgRelMAE', 0))\n",
    "        plt.suptitle(f\"NF_TFT | {row['Dataset']} | {row['Experiment']} {row['Window']} | \"\n",
    "                     f\"RelMAE={rel_day:.1f}%\", fontsize=11, y=1.01)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / f\"nf_pred_{row['Dataset']}_{row['Experiment']}_{row['Window']}.png\", dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"\\nPlotted {len(all_cv_details)} NF experiment predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ac3c9",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ac1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_cv_details) > 0:\n",
    "    metric_col = 'AvgRelMAE_day' if 'AvgRelMAE_day' in results_df.columns else 'AvgRelMAE'\n",
    "    best_idx = results_df[metric_col].idxmin()\n",
    "    best_detail = all_cv_details[best_idx]\n",
    "    merged = best_detail['merged']\n",
    "\n",
    "    if 'hour' not in merged.columns and pd.api.types.is_datetime64_any_dtype(merged['ds']):\n",
    "        merged['hour'] = merged['ds'].dt.hour\n",
    "    if 'is_night' not in merged.columns and 'hour' in merged.columns:\n",
    "        merged['is_night'] = ((merged['hour'] >= NIGHT_START) | (merged['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "    merged['abs_err'] = np.abs(merged['y'] - merged['pred'])\n",
    "    errors = merged['y'].values - merged['pred'].values\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes[0,0].hist(errors, bins=80, color='steelblue', alpha=0.7, edgecolor='white')\n",
    "    axes[0,0].axvline(0, color='red', linestyle='--')\n",
    "    axes[0,0].axvline(errors.mean(), color='orange', linestyle='--', label=f'Bias={errors.mean():.1f}')\n",
    "    axes[0,0].set_xlabel('Error')\n",
    "    axes[0,0].set_title('Error Distribution')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "    axes[0,1].scatter(merged['y'], merged['pred'], s=2, alpha=0.3, color='steelblue')\n",
    "    mx = max(merged['y'].max(), merged['pred'].max())\n",
    "    axes[0,1].plot([0, mx], [0, mx], 'r--', linewidth=1)\n",
    "    axes[0,1].set_xlabel('Actual')\n",
    "    axes[0,1].set_ylabel('Predicted')\n",
    "    axes[0,1].set_title('Actual vs Predicted')\n",
    "\n",
    "    hourly = merged.groupby('hour')['abs_err'].mean()\n",
    "    colors = ['navy' if (h >= NIGHT_START or h <= NIGHT_END) else 'steelblue' for h in hourly.index]\n",
    "    axes[1,0].bar(hourly.index, hourly.values, color=colors, alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Hour')\n",
    "    axes[1,0].set_ylabel('MAE')\n",
    "    axes[1,0].set_title('MAE by Hour (dark=night)')\n",
    "\n",
    "    day_err = merged[merged['is_night'] == 0]['abs_err']\n",
    "    night_err = merged[merged['is_night'] == 1]['abs_err']\n",
    "    bp = axes[1,1].boxplot([day_err, night_err], labels=['Day', 'Night'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('steelblue')\n",
    "    bp['boxes'][1].set_facecolor('navy')\n",
    "    for box in bp['boxes']:\n",
    "        box.set_alpha(0.5)\n",
    "    axes[1,1].set_ylabel('Absolute Error')\n",
    "    axes[1,1].set_title(f'Day MAE={day_err.mean():.1f} vs Night MAE={night_err.mean():.1f}')\n",
    "\n",
    "    plt.suptitle('Error Analysis \\u2014 Best NF_TFT Window', fontsize=13, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'tft_error_analysis.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Day MAE: {day_err.mean():.1f} | Night MAE: {night_err.mean():.1f}\")\n",
    "    print(f\"Night is {night_err.mean()/day_err.mean():.1f}x {'lower' if night_err.mean() < day_err.mean() else 'higher'} than day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141346be",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY \\u2014 NeuralForecast TFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metric_col = 'AvgRelMAE_day' if 'AvgRelMAE_day' in results_df.columns else 'AvgRelMAE'\n",
    "r2_col = 'R2_day' if 'R2_day' in results_df.columns else 'R2'\n",
    "\n",
    "print(\"\\n--- Best per Experiment Type (daytime metrics) ---\")\n",
    "for exp_type in results_df['Experiment'].unique():\n",
    "    sub = results_df[results_df['Experiment'] == exp_type]\n",
    "    best = sub.loc[sub[metric_col].idxmin()]\n",
    "    print(f\"  {exp_type:12s} | {best['Dataset']:10s} | {best['Window']} \"\n",
    "          f\"| RelMAE_day={best[metric_col]:.1f}% | R2_day={best[r2_col]:.3f}\")\n",
    "\n",
    "print(\"\\n--- Best per Dataset ---\")\n",
    "for ds in results_df['Dataset'].unique():\n",
    "    sub = results_df[results_df['Dataset'] == ds]\n",
    "    best = sub.loc[sub[metric_col].idxmin()]\n",
    "    print(f\"  {ds:10s} | {best['Experiment']:12s} {best['Window']} \"\n",
    "          f\"| RelMAE_day={best[metric_col]:.1f}% | R2_day={best[r2_col]:.3f}\")\n",
    "\n",
    "print(\"\\n--- Stability (CV% across windows) ---\")\n",
    "for exp_type in results_df['Experiment'].unique():\n",
    "    sub = results_df[results_df['Experiment'] == exp_type]\n",
    "    stab = sub.groupby('Dataset')[metric_col].agg(['mean', 'std'])\n",
    "    stab['cv%'] = (stab['std'] / stab['mean'] * 100).round(1)\n",
    "    print(f\"\\n  {exp_type}:\")\n",
    "    print(stab.sort_values('mean').round(1).to_string())\n",
    "\n",
    "overall = results_df.loc[results_df[metric_col].idxmin()]\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"OVERALL BEST NF_TFT: {overall['Experiment']} | {overall['Dataset']} | {overall['Window']}\")\n",
    "print(f\"  RelMAE_day={overall[metric_col]:.1f}% | R2_day={overall[r2_col]:.3f} | MAE={overall['MAE']:.1f}\")\n",
    "print(f\"{'=' * 70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800a7c3",
   "metadata": {},
   "source": [
    "---\n",
    "## PyTorch-Forecasting TFT (Daytime Only)\n",
    "\n",
    "Uses `TimeSeriesDataSet` with integer `time_idx` to handle daytime-only data without gaps.\n",
    "This avoids the night bias problem entirely — no fake interpolation, no night=0 tricks.\n",
    "\n",
    "Same experiment structure: Simple Sliding, Expanding Rolling, Split-Gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"pytorch-forecasting\", \"pytorch_optimizer\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE as PF_MAE, QuantileLoss\n",
    "\n",
    "print(\"pytorch-forecasting imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0de53b",
   "metadata": {},
   "source": [
    "### Prepare Daytime Dataset for PyTorch-Forecasting\n",
    "\n",
    "Filter to daytime hours only, then assign a sequential `time_idx` per beach.\n",
    "TFT sees a continuous sequence — no gaps, no night bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daytime only — no night hours at all\n",
    "df_day = df[df['is_night'] == 0].copy()\n",
    "df_day = df_day.sort_values(['beach_folder', 'datetime']).reset_index(drop=True)\n",
    "df_day['time_idx'] = df_day.groupby('beach_folder').cumcount()\n",
    "df_day['group_id'] = df_day['beach_folder'].astype(str)\n",
    "\n",
    "# Keep original datetime for later analysis\n",
    "df_day['orig_datetime'] = df_day['datetime']\n",
    "\n",
    "print(f\"Daytime dataset: {len(df_day)} rows, {df_day['group_id'].nunique()} beaches\")\n",
    "print(f\"Hours per beach: {df_day.groupby('group_id').size().describe()[['min', 'max', 'mean']].to_dict()}\")\n",
    "print(f\"Time idx range: 0 to {df_day['time_idx'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da442be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for pytorch-forecasting TFT\n",
    "PF_MAX_ENCODER = 48\n",
    "PF_MAX_PREDICTION = 24  # predict next 24 daytime hours\n",
    "PF_BATCH_SIZE = 64\n",
    "PF_MAX_EPOCHS = 50\n",
    "PF_HIDDEN_SIZE = 32\n",
    "PF_LR = 0.03\n",
    "\n",
    "PF_TIME_VARYING_KNOWN = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer']\n",
    "PF_TIME_VARYING_UNKNOWN = ['count'] + WEATHER_COLS\n",
    "\n",
    "pf_results = []\n",
    "pf_cv_details = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pf_experiment(exp_name, exp_type, ds_name, train_df, val_df, test_df,\n",
    "                      train_label, val_label, test_label):\n",
    "    common_ids = (set(train_df['group_id'].unique()) &\n",
    "                  set(val_df['group_id'].unique()) &\n",
    "                  set(test_df['group_id'].unique()))\n",
    "    if len(common_ids) == 0:\n",
    "        print(f\"  {exp_name}: no common series, skipping\")\n",
    "        return\n",
    "\n",
    "    train_df = train_df[train_df['group_id'].isin(common_ids)].copy()\n",
    "    val_df = val_df[val_df['group_id'].isin(common_ids)].copy()\n",
    "    test_df = test_df[test_df['group_id'].isin(common_ids)].copy()\n",
    "\n",
    "    combined = pd.concat([train_df, val_df, test_df]).sort_values(['group_id', 'datetime']).reset_index(drop=True)\n",
    "    combined['time_idx'] = combined.groupby('group_id').cumcount()\n",
    "\n",
    "    train_cutoff = combined[combined['datetime'] <= train_df['datetime'].max()]['time_idx'].max()\n",
    "    val_cutoff = combined[combined['datetime'] <= val_df['datetime'].max()]['time_idx'].max()\n",
    "\n",
    "    known_reals = [c for c in PF_TIME_VARYING_KNOWN if c in combined.columns]\n",
    "    unknown_reals = [c for c in PF_TIME_VARYING_UNKNOWN if c in combined.columns]\n",
    "\n",
    "    print(f\"\\n  {exp_name} | series={len(common_ids)} | encoder={PF_MAX_ENCODER} | horizon={PF_MAX_PREDICTION}\")\n",
    "    print(f\"    Train: {train_label} | Val: {val_label} | Test: {test_label}\")\n",
    "\n",
    "    try:\n",
    "        training_ds = TimeSeriesDataSet(\n",
    "            combined[combined['time_idx'] <= val_cutoff],\n",
    "            time_idx='time_idx',\n",
    "            target='count',\n",
    "            group_ids=['group_id'],\n",
    "            min_encoder_length=PF_MAX_ENCODER // 2,\n",
    "            max_encoder_length=PF_MAX_ENCODER,\n",
    "            min_prediction_length=1,\n",
    "            max_prediction_length=PF_MAX_PREDICTION,\n",
    "            time_varying_known_reals=['time_idx'] + known_reals,\n",
    "            time_varying_unknown_reals=unknown_reals,\n",
    "            target_normalizer=GroupNormalizer(groups=['group_id'], transformation='softplus'),\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "\n",
    "        validation_ds = TimeSeriesDataSet.from_dataset(\n",
    "            training_ds, combined[combined['time_idx'] <= val_cutoff],\n",
    "            predict=True, stop_randomization=True\n",
    "        )\n",
    "        test_ds = TimeSeriesDataSet.from_dataset(\n",
    "            training_ds, combined, predict=True, stop_randomization=True\n",
    "        )\n",
    "\n",
    "        train_dl = training_ds.to_dataloader(train=True, batch_size=PF_BATCH_SIZE, num_workers=0)\n",
    "        val_dl = validation_ds.to_dataloader(train=False, batch_size=PF_BATCH_SIZE * 4, num_workers=0)\n",
    "        test_dl = test_ds.to_dataloader(train=False, batch_size=PF_BATCH_SIZE * 4, num_workers=0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min')\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=PF_MAX_EPOCHS,\n",
    "            accelerator='auto',\n",
    "            devices=DEVICES,\n",
    "            gradient_clip_val=0.1,\n",
    "            callbacks=[early_stop],\n",
    "            enable_model_summary=False,\n",
    "            enable_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        tft = TemporalFusionTransformer.from_dataset(\n",
    "            training_ds,\n",
    "            learning_rate=PF_LR,\n",
    "            hidden_size=PF_HIDDEN_SIZE,\n",
    "            attention_head_size=2,\n",
    "            dropout=0.1,\n",
    "            hidden_continuous_size=PF_HIDDEN_SIZE,\n",
    "            loss=QuantileLoss(),\n",
    "            optimizer='adam',\n",
    "            reduce_on_plateau_patience=4,\n",
    "        )\n",
    "\n",
    "        trainer.fit(tft, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        raw_preds = tft.predict(test_dl, mode=\"raw\", return_x=True,\n",
    "                                trainer_kwargs=dict(accelerator='cpu'))\n",
    "        predictions = tft.predict(test_dl, return_y=True, trainer_kwargs=dict(accelerator='cpu'))\n",
    "\n",
    "        y_pred_raw = predictions.output.cpu().numpy()\n",
    "        if y_pred_raw.ndim == 3:\n",
    "            y_pred_raw = y_pred_raw.mean(axis=-1)\n",
    "        y_pred = np.clip(y_pred_raw.flatten(), 0, None)\n",
    "        y_true = predictions.y[0].cpu().numpy().flatten()\n",
    "\n",
    "        m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "\n",
    "        beach_results = []\n",
    "        beach_ids = sorted(common_ids)\n",
    "        n_per = len(y_true) // len(beach_ids) if len(beach_ids) > 0 else len(y_true)\n",
    "        for i, b in enumerate(beach_ids):\n",
    "            start = i * n_per\n",
    "            end = start + n_per\n",
    "            if end > len(y_true):\n",
    "                break\n",
    "            yt = y_true[start:end]\n",
    "            yp = y_pred[start:end]\n",
    "            if len(yt) < 3:\n",
    "                continue\n",
    "            bm = calc_metrics(yt, yp, yt.max())\n",
    "            bm['beach'] = b\n",
    "            bm['max_count'] = yt.max()\n",
    "            beach_results.append(bm)\n",
    "\n",
    "        beach_df = pd.DataFrame(beach_results)\n",
    "        avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else m['RelMAE']\n",
    "\n",
    "        pf_results.append({\n",
    "            'Model': 'PF_TFT', 'Dataset': ds_name, 'Experiment': exp_type,\n",
    "            'Window': exp_name,\n",
    "            'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "            'AvgRelMAE_day': avg_rel,\n",
    "            'Time': elapsed,\n",
    "            'train_period': train_label, 'val_period': val_label, 'test_period': test_label,\n",
    "            'train_rows': len(train_df),\n",
    "        })\n",
    "        pf_cv_details.append({\n",
    "            'ds_name': ds_name, 'exp_type': exp_type, 'window': exp_name,\n",
    "            'model': 'PF_TFT', 'y_true': y_true, 'y_pred': y_pred,\n",
    "            'beach_df': beach_df,\n",
    "            'raw_predictions': raw_preds,\n",
    "            'tft_model': tft,\n",
    "            'test_dl': test_dl,\n",
    "        })\n",
    "        print(f\"    {elapsed:.0f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026d7a6",
   "metadata": {},
   "source": [
    "### Run PyTorch-Forecasting TFT Experiments\n",
    "\n",
    "Uses the **Daytime** dataset only (no night hours). Same 3 experiment types as NeuralForecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build experiments for daytime dataset\n",
    "ds_name = 'Daytime'\n",
    "ds_day = datasets['Daytime']\n",
    "\n",
    "chunks_day, _ = create_temporal_chunks(ds_day, N_CHUNKS)\n",
    "all_chunks['Daytime'] = chunks_day\n",
    "\n",
    "# Simple sliding\n",
    "sw_day = []\n",
    "for i in range(len(chunks_day) - 3):\n",
    "    train_data = pd.concat([chunks_day[i]['data'], chunks_day[i+1]['data']])\n",
    "    sw_day.append({\n",
    "        'name': f\"SW{i+1}\",\n",
    "        'train_data': train_data,\n",
    "        'val_data': chunks_day[i+2]['data'],\n",
    "        'test_data': chunks_day[i+3]['data'],\n",
    "        'train_label': f\"{chunks_day[i]['label'].split(':')[0]}+{chunks_day[i+1]['label'].split(':')[0]}\",\n",
    "        'val_label': chunks_day[i+2]['label'],\n",
    "        'test_label': chunks_day[i+3]['label'],\n",
    "    })\n",
    "\n",
    "# Expanding rolling\n",
    "ew_day = []\n",
    "for test_idx in range(2, N_CHUNKS):\n",
    "    val_idx = test_idx - 1\n",
    "    train_chunks = chunks_day[:val_idx]\n",
    "    ew_day.append({\n",
    "        'name': f\"W{len(ew_day)+1}\",\n",
    "        'train_data': pd.concat([c['data'] for c in train_chunks]),\n",
    "        'val_data': chunks_day[val_idx]['data'],\n",
    "        'test_data': chunks_day[test_idx]['data'],\n",
    "        'train_label': f\"{train_chunks[0]['label'].split(':')[0]}\\u2192{train_chunks[-1]['label'].split(':')[0]} ({len(train_chunks)}ch)\",\n",
    "        'val_label': chunks_day[val_idx]['label'],\n",
    "        'test_label': chunks_day[test_idx]['label'],\n",
    "    })\n",
    "\n",
    "# Split-gap\n",
    "n = len(ds_day)\n",
    "ds_day_sorted = ds_day.sort_values('datetime').reset_index(drop=True)\n",
    "early = ds_day_sorted.iloc[:int(n * 0.4)]\n",
    "late = ds_day_sorted.iloc[int(n * 0.6):]\n",
    "gap_start = ds_day_sorted.iloc[int(n * 0.4)]['datetime']\n",
    "gap_end = ds_day_sorted.iloc[int(n * 0.6)]['datetime']\n",
    "\n",
    "def split_period_day(period_df, name):\n",
    "    np_ = len(period_df)\n",
    "    tr = period_df.iloc[:int(np_ * 0.6)]\n",
    "    va = period_df.iloc[int(np_ * 0.6):int(np_ * 0.8)]\n",
    "    te = period_df.iloc[int(np_ * 0.8):]\n",
    "    return {\n",
    "        'name': name,\n",
    "        'train_data': tr, 'val_data': va, 'test_data': te,\n",
    "        'train_label': f\"{name} train ({tr['datetime'].min().strftime('%b %d')}\\u2192{tr['datetime'].max().strftime('%b %d')})\",\n",
    "        'val_label': f\"{name} val\",\n",
    "        'test_label': f\"{name} test\",\n",
    "    }\n",
    "\n",
    "sg_early = split_period_day(early, \"Early\")\n",
    "sg_late = split_period_day(late, \"Late\")\n",
    "sg_combined = {\n",
    "    'name': \"Combined\",\n",
    "    'train_data': pd.concat([sg_early['train_data'], sg_late['train_data']]),\n",
    "    'val_data': sg_late['val_data'],\n",
    "    'test_data': sg_late['test_data'],\n",
    "    'train_label': f\"Early+Late train\",\n",
    "    'val_label': sg_late['val_label'],\n",
    "    'test_label': sg_late['test_label'],\n",
    "}\n",
    "\n",
    "sg_day = [sg_early, sg_late, sg_combined]\n",
    "\n",
    "print(f\"Daytime experiments:\")\n",
    "print(f\"  Simple sliding: {len(sw_day)} windows\")\n",
    "print(f\"  Expanding: {len(ew_day)} windows\")\n",
    "print(f\"  Split-gap: 3 (Early, Late, Combined)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare daytime df with time_idx for pytorch-forecasting\n",
    "def prepare_pf_data(data):\n",
    "    d = data.copy()\n",
    "    d = d.sort_values(['beach_folder', 'datetime']).reset_index(drop=True)\n",
    "    d['group_id'] = d['beach_folder'].astype(str)\n",
    "    return d\n",
    "\n",
    "# Run simple sliding\n",
    "print(\"=\" * 70)\n",
    "print(\"PYTORCH-FORECASTING TFT \\u2014 SIMPLE SLIDING (Daytime)\")\n",
    "print(\"=\" * 70)\n",
    "for w in sw_day:\n",
    "    run_pf_experiment(\n",
    "        exp_name=w['name'], exp_type=\"Simple\", ds_name=\"Daytime\",\n",
    "        train_df=prepare_pf_data(w['train_data']),\n",
    "        val_df=prepare_pf_data(w['val_data']),\n",
    "        test_df=prepare_pf_data(w['test_data']),\n",
    "        train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "    )\n",
    "\n",
    "# Run expanding rolling\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PYTORCH-FORECASTING TFT \\u2014 EXPANDING ROLLING (Daytime)\")\n",
    "print(\"=\" * 70)\n",
    "for w in ew_day:\n",
    "    run_pf_experiment(\n",
    "        exp_name=w['name'], exp_type=\"Expanding\", ds_name=\"Daytime\",\n",
    "        train_df=prepare_pf_data(w['train_data']),\n",
    "        val_df=prepare_pf_data(w['val_data']),\n",
    "        test_df=prepare_pf_data(w['test_data']),\n",
    "        train_label=w['train_label'], val_label=w['val_label'], test_label=w['test_label'],\n",
    "    )\n",
    "\n",
    "# Run split-gap\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PYTORCH-FORECASTING TFT \\u2014 SPLIT-GAP (Daytime)\")\n",
    "print(\"=\" * 70)\n",
    "for exp in sg_day:\n",
    "    run_pf_experiment(\n",
    "        exp_name=f\"SG_{exp['name']}\", exp_type=\"SplitGap\", ds_name=\"Daytime\",\n",
    "        train_df=prepare_pf_data(exp['train_data']),\n",
    "        val_df=prepare_pf_data(exp['val_data']),\n",
    "        test_df=prepare_pf_data(exp['test_data']),\n",
    "        train_label=exp['train_label'], val_label=exp['val_label'], test_label=exp['test_label'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107cc30",
   "metadata": {},
   "source": [
    "### PyTorch-Forecasting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_results_df = pd.DataFrame(pf_results)\n",
    "\n",
    "if len(pf_results_df) > 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PYTORCH-FORECASTING TFT RESULTS (Daytime Only)\")\n",
    "    print(\"=\" * 70)\n",
    "    cols = ['Model', 'Experiment', 'Window', 'train_rows', 'MAE', 'R2', 'AvgRelMAE_day', 'Time']\n",
    "    cols = [c for c in cols if c in pf_results_df.columns]\n",
    "    print(pf_results_df[cols].to_string(index=False))\n",
    "    pf_results_df.to_csv(save_dir / 'pf_tft_results.csv', index=False)\n",
    "\n",
    "# Predictions vs Actual for ALL PF experiments\n",
    "if len(pf_cv_details) > 0:\n",
    "    for detail_idx, detail in enumerate(pf_cv_details):\n",
    "        if detail_idx >= len(pf_results_df):\n",
    "            break\n",
    "        row = pf_results_df.iloc[detail_idx]\n",
    "\n",
    "        # 1) Built-in plot_prediction (encoder + decoder + attention)\n",
    "        if 'raw_predictions' in detail and detail['raw_predictions'] is not None:\n",
    "            try:\n",
    "                raw = detail['raw_predictions']\n",
    "                tft_model = detail['tft_model']\n",
    "                n_plots = min(4, raw.output['prediction'].shape[0])\n",
    "                fig, axes = plt.subplots(n_plots, 1, figsize=(16, 3 * n_plots))\n",
    "                if n_plots == 1:\n",
    "                    axes = [axes]\n",
    "                for idx in range(n_plots):\n",
    "                    tft_model.plot_prediction(raw.x, raw.output, idx=idx, ax=axes[idx],\n",
    "                                              add_loss_to_title=True)\n",
    "                plt.suptitle(f\"PF_TFT built-in | {row['Experiment']} {row['Window']} | \"\n",
    "                             f\"RelMAE={row['AvgRelMAE_day']:.1f}%\", fontsize=11, y=1.01)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(save_dir / f\"pf_builtin_{row['Experiment']}_{row['Window']}.png\", dpi=100)\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"  plot_prediction failed: {e}\")\n",
    "\n",
    "        # 2) Per-beach actual vs predicted (same style as NF)\n",
    "        y_true = detail['y_true']\n",
    "        y_pred = detail['y_pred']\n",
    "        beach_df = detail.get('beach_df', pd.DataFrame())\n",
    "\n",
    "        if len(beach_df) > 0:\n",
    "            top_beaches = beach_df.nlargest(4, 'max_count')\n",
    "            n_show = len(top_beaches)\n",
    "            fig, axes = plt.subplots(n_show, 1, figsize=(16, 3 * n_show))\n",
    "            if n_show == 1:\n",
    "                axes = [axes]\n",
    "            beach_ids = sorted(detail.get('beach_df', pd.DataFrame()).get('beach', []))\n",
    "            n_per = len(y_true) // len(beach_ids) if len(beach_ids) > 0 else len(y_true)\n",
    "\n",
    "            for ax_i, (_, brow) in enumerate(top_beaches.iterrows()):\n",
    "                if ax_i >= n_show:\n",
    "                    break\n",
    "                b = brow['beach']\n",
    "                b_idx = beach_ids.index(b) if b in beach_ids else ax_i\n",
    "                start = b_idx * n_per\n",
    "                end = start + n_per\n",
    "                yt = y_true[start:end]\n",
    "                yp = y_pred[start:end]\n",
    "\n",
    "                ax = axes[ax_i]\n",
    "                ax.plot(yt, label='Actual', color='steelblue', linewidth=1)\n",
    "                ax.plot(yp, label='Predicted', color='coral', linewidth=1, alpha=0.8)\n",
    "                ax.fill_between(range(len(yt)), yt, yp, alpha=0.1, color='red')\n",
    "                ax.set_title(f\"{b[:30]} | MAE={brow['MAE']:.1f} | R\\u00b2={brow['R2']:.3f}\", fontsize=9, loc='left')\n",
    "                ax.legend(loc='upper right', fontsize=7)\n",
    "                ax.set_ylabel('Count')\n",
    "\n",
    "            axes[-1].set_xlabel('Daytime Step')\n",
    "            plt.suptitle(f\"PF_TFT | {row['Experiment']} {row['Window']} | \"\n",
    "                         f\"RelMAE={row['AvgRelMAE_day']:.1f}%\", fontsize=11, y=1.01)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_dir / f\"pf_pred_{row['Experiment']}_{row['Window']}.png\", dpi=100)\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(16, 4))\n",
    "            ax.plot(y_true, label='Actual', color='steelblue', linewidth=0.8)\n",
    "            ax.plot(y_pred, label='Predicted', color='coral', linewidth=0.8, alpha=0.8)\n",
    "            ax.legend()\n",
    "            ax.set_title(f\"PF_TFT | {row['Experiment']} {row['Window']} | \"\n",
    "                         f\"MAE={row['MAE']:.1f} | R\\u00b2={row['R2']:.3f}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_dir / f\"pf_pred_{row['Experiment']}_{row['Window']}.png\", dpi=100)\n",
    "            plt.show()\n",
    "\n",
    "    print(f\"\\nPlotted {len(pf_cv_details)} PF experiment predictions\")\n",
    "\n",
    "    # Error analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    all_true = np.concatenate([d['y_true'] for d in pf_cv_details])\n",
    "    all_pred = np.concatenate([d['y_pred'] for d in pf_cv_details])\n",
    "\n",
    "    axes[0].scatter(all_true, all_pred, s=2, alpha=0.3, color='coral')\n",
    "    mx = max(all_true.max(), all_pred.max())\n",
    "    axes[0].plot([0, mx], [0, mx], 'r--', linewidth=1)\n",
    "    axes[0].set_xlabel('Actual')\n",
    "    axes[0].set_ylabel('Predicted')\n",
    "    axes[0].set_title('PF_TFT: Actual vs Predicted (all experiments)')\n",
    "\n",
    "    errors = all_true - all_pred\n",
    "    axes[1].hist(errors, bins=80, color='coral', alpha=0.7, edgecolor='white')\n",
    "    axes[1].axvline(0, color='red', linestyle='--')\n",
    "    axes[1].axvline(errors.mean(), color='orange', linestyle='--', label=f'Bias={errors.mean():.1f}')\n",
    "    axes[1].set_xlabel('Error')\n",
    "    axes[1].set_title('PF_TFT: Error Distribution')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'pf_error_analysis.png', dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe52b52",
   "metadata": {},
   "source": [
    "---\n",
    "## NeuralForecast vs PyTorch-Forecasting Comparison\n",
    "\n",
    "Compare both TFT implementations side by side. NF_TFT uses 24h data with daytime-only metrics.\n",
    "PF_TFT uses daytime-only data natively (no night hours at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results\n",
    "combined_results = pd.concat([results_df, pf_results_df], ignore_index=True) if len(pf_results_df) > 0 else results_df.copy()\n",
    "combined_results.to_csv(save_dir / 'combined_results.csv', index=False)\n",
    "\n",
    "metric_col = 'AvgRelMAE_day'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NF_TFT vs PF_TFT \\u2014 COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp_type in combined_results['Experiment'].unique():\n",
    "    sub = combined_results[combined_results['Experiment'] == exp_type]\n",
    "    print(f\"\\n--- {exp_type} ---\")\n",
    "    pivot = sub.pivot_table(index='Window', columns='Model', values=metric_col)\n",
    "    print(pivot.round(1).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pf_results_df) > 0 and len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Bar chart: best per experiment type\n",
    "    best_per_exp = combined_results.groupby(['Experiment', 'Model'])[metric_col].min().unstack()\n",
    "    best_per_exp.plot(kind='bar', ax=axes[0], width=0.6, color=['steelblue', 'coral'])\n",
    "    axes[0].set_ylabel('Best AvgRelMAE_day (%)')\n",
    "    axes[0].set_title('Best RelMAE by Experiment Type')\n",
    "    axes[0].tick_params(axis='x', rotation=30)\n",
    "    axes[0].legend(title='Model')\n",
    "\n",
    "    # R2 comparison\n",
    "    r2_col = 'R2_day' if 'R2_day' in combined_results.columns else 'R2'\n",
    "    best_r2 = combined_results.groupby(['Experiment', 'Model'])[r2_col].max().unstack()\n",
    "    best_r2.plot(kind='bar', ax=axes[1], width=0.6, color=['steelblue', 'coral'])\n",
    "    axes[1].set_ylabel('Best R\\u00b2 (day)')\n",
    "    axes[1].set_title('Best R\\u00b2 by Experiment Type')\n",
    "    axes[1].tick_params(axis='x', rotation=30)\n",
    "    axes[1].legend(title='Model')\n",
    "\n",
    "    # Training time\n",
    "    time_avg = combined_results.groupby('Model')['Time'].mean()\n",
    "    axes[2].bar(time_avg.index, time_avg.values, color=['steelblue', 'coral'], alpha=0.7)\n",
    "    axes[2].set_ylabel('Avg Time (s)')\n",
    "    axes[2].set_title('Average Training Time')\n",
    "\n",
    "    plt.suptitle('NeuralForecast TFT vs PyTorch-Forecasting TFT', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'nf_vs_pf_comparison.png', dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5261da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final overall summary\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL OVERALL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in combined_results['Model'].unique():\n",
    "    sub = combined_results[combined_results['Model'] == model_name]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best = sub.loc[sub[metric_col].idxmin()]\n",
    "    r2_col = 'R2_day' if 'R2_day' in sub.columns else 'R2'\n",
    "    print(f\"\\n  {model_name}:\")\n",
    "    print(f\"    Best: {best['Experiment']} | {best['Dataset']} | {best['Window']}\")\n",
    "    print(f\"    RelMAE_day={best[metric_col]:.1f}% | R2_day={best.get(r2_col, best['R2']):.3f} | MAE={best['MAE']:.1f}\")\n",
    "    print(f\"    Train: {best['train_period']}\")\n",
    "\n",
    "overall = combined_results.loc[combined_results[metric_col].idxmin()]\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"OVERALL WINNER: {overall['Model']} | {overall['Experiment']} | {overall['Dataset']} | {overall['Window']}\")\n",
    "print(f\"  RelMAE_day={overall[metric_col]:.1f}% | MAE={overall['MAE']:.1f}\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "print(\"\\nKey insight: PF_TFT uses daytime-only data (no night bias).\")\n",
    "print(\"NF_TFT trains on 24h data but is evaluated on daytime hours only.\")\n",
    "print(\"If PF_TFT wins \\u2192 night data hurts more than it helps.\")\n",
    "print(\"If NF_TFT wins \\u2192 night context provides useful temporal patterns.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
