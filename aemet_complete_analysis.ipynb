{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEMET Data Analysis: Availability, Correlation & Maps\n",
    "\n",
    "Complete analysis of AEMET weather data for Balearic Islands:\n",
    "1. Variable availability\n",
    "2. Station-level availability\n",
    "3. Correlation analysis \n",
    "4. Geographic maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T09:23:56.590987Z",
     "start_time": "2026-01-26T09:23:56.588308Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import folium\n",
    "import contextily as ctx\n",
    "\n",
    "print('All imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:20.262539Z",
     "start_time": "2026-01-22T14:13:20.258570Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "AEMET_DIR = PROJECT_ROOT / 'aemet/2022'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'results'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BBOX = {'lat_min': 38.5, 'lat_max': 40.2, 'lon_min': 1.0, 'lon_max': 4.5}\n",
    "\n",
    "# Exclude variables with no/sparse data (<10%)\n",
    "EXCLUDE_VARS = [\n",
    "    'psoltp', 'pliqt', 'vis', 'tss5cm', 'tss20cm',\n",
    "    'stdvvu', 'geo850', 'geo700', 'stddvu', 'dvu',\n",
    "    'dmaxu', 'vmaxu', 'vvu', 'pacutp', 'nieve', 'geo925'\n",
    "]\n",
    "\n",
    "AEMET_VARS = {\n",
    "    'ta': 'Temperature', 'tamax': 'Temp Max', 'tamin': 'Temp Min',\n",
    "    'tpr': 'Dew Point', 'ts': 'Ground Temp',\n",
    "    'hr': 'Humidity', 'pres': 'Pressure', 'pres_nmar': 'Sea Lv Press',\n",
    "    'prec': 'Precipitation',\n",
    "    'vv': 'Wind Speed', 'vmax': 'Wind Gust', 'dv': 'Wind Dir', 'dmax': 'Gust Dir',\n",
    "    'stdvv': 'Wind StdDev', 'stddv': 'Dir StdDev', 'rviento': 'Wind Run',\n",
    "    'inso': 'Insolation',\n",
    "}\n",
    "\n",
    "# Variable labels for plotting\n",
    "VAR_LABELS = {\n",
    "    'ta': 'Temperature (¬∞C)', 'tamax': 'Temp Max (¬∞C)', 'tamin': 'Temp Min (¬∞C)',\n",
    "    'tpr': 'Dew Point (¬∞C)', 'ts': 'Ground Temp (¬∞C)',\n",
    "    'hr': 'Humidity (%)', 'pres': 'Pressure (hPa)', 'pres_nmar': 'Sea Lv Press (hPa)',\n",
    "    'prec': 'Precipitation (mm)',\n",
    "    'vv': 'Wind Speed (m/s)', 'vmax': 'Wind Gust (m/s)', 'dv': 'Wind Dir (¬∞)', 'dmax': 'Gust Dir (¬∞)',\n",
    "    'stdvv': 'Wind StdDev', 'stddv': 'Dir StdDev', 'rviento': 'Wind Run',\n",
    "    'inso': 'Insolation (h)',\n",
    "}\n",
    "\n",
    "# Island boundaries\n",
    "ISLANDS = {\n",
    "    'Mallorca': {'lon_min': 2.3, 'lon_max': 3.5, 'lat_min': 39.2, 'lat_max': 39.95},\n",
    "    'Menorca': {'lon_min': 3.8, 'lon_max': 4.35, 'lat_min': 39.8, 'lat_max': 40.1},\n",
    "    'Ibiza': {'lon_min': 1.15, 'lon_max': 1.65, 'lat_min': 38.85, 'lat_max': 39.15},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:46.575536Z",
     "start_time": "2026-01-22T14:13:20.357596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "json_files = sorted(glob.glob(str(AEMET_DIR / '*.json')))\n",
    "all_records = []\n",
    "for f in json_files:\n",
    "    try:\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            data = json.loads(file.read())\n",
    "            if 'datos' in data:\n",
    "                all_records.extend(data['datos'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "df['fint'] = pd.to_datetime(df['fint'])\n",
    "\n",
    "# Filter Balearic\n",
    "df_bal = df[\n",
    "    (df['lat'] >= BBOX['lat_min']) & (df['lat'] <= BBOX['lat_max']) &\n",
    "    (df['lon'] >= BBOX['lon_min']) & (df['lon'] <= BBOX['lon_max'])\n",
    "].copy()\n",
    "\n",
    "print(f'Total records: {len(df_bal):,}')\n",
    "print(f'Stations: {df_bal[\"idema\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variable Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:47.083333Z",
     "start_time": "2026-01-22T14:13:47.043490Z"
    }
   },
   "outputs": [],
   "source": [
    "available_vars = [v for v in AEMET_VARS.keys() if v in df_bal.columns and v not in EXCLUDE_VARS]\n",
    "\n",
    "availability = []\n",
    "for var in available_vars:\n",
    "    total = len(df_bal)\n",
    "    valid = df_bal[var].notna().sum()\n",
    "    pct = (valid / total * 100)\n",
    "    availability.append({'var': var, 'label': AEMET_VARS[var], 'valid_count': valid,\n",
    "                         'total_count': total, 'availability_pct': round(pct, 1)})\n",
    "\n",
    "avail_df = pd.DataFrame(availability).sort_values('availability_pct', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print('VARIABLE AVAILABILITY')\n",
    "print('=' * 65)\n",
    "for _, row in avail_df.iterrows():\n",
    "    status = 'GOOD' if row['availability_pct'] >= 80 else 'PARTIAL' if row['availability_pct'] >= 40 else 'SPARSE'\n",
    "    print(f'{row[\"var\"]:<10} {row[\"label\"]:<15} {row[\"availability_pct\"]:>6.1f}%  [{status}]')\n",
    "\n",
    "print(f'\\nExcluded (no data): {EXCLUDE_VARS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:47.621279Z",
     "start_time": "2026-01-22T14:13:47.181575Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#2ecc71' if p >= 80 else '#f39c12' if p >= 40 else '#e74c3c' for p in avail_df['availability_pct']]\n",
    "bars = ax.barh(range(len(avail_df)), avail_df['availability_pct'], color=colors, edgecolor='black')\n",
    "ax.set_yticks(range(len(avail_df)))\n",
    "ax.set_yticklabels([f\"{r['var']} ({r['label']})\" for _, r in avail_df.iterrows()])\n",
    "ax.axvline(80, color='green', ls='--', alpha=0.7, label='Good (80%)')\n",
    "ax.axvline(40, color='orange', ls='--', alpha=0.7, label='Partial (40%)')\n",
    "ax.set_xlim(0, 105)\n",
    "ax.set_xlabel('Availability %')\n",
    "ax.set_title('Variable Availability', fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "for i, pct in enumerate(avail_df['availability_pct']):\n",
    "    ax.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'variable_availability.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Availability per Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:48.023259Z",
     "start_time": "2026-01-22T14:13:47.641835Z"
    }
   },
   "outputs": [],
   "source": [
    "stations = df_bal.groupby('idema').agg({\n",
    "    'lat': 'first', 'lon': 'first', 'alt': 'first', 'ubi': 'first', 'fint': 'count'\n",
    "}).reset_index()\n",
    "stations.columns = ['idema', 'lat', 'lon', 'alt', 'name', 'n_records']\n",
    "\n",
    "for var in available_vars:\n",
    "    counts = df_bal.groupby('idema')[var].apply(lambda x: x.notna().sum()).reset_index()\n",
    "    counts.columns = ['idema', f'{var}_count']\n",
    "    stations = stations.merge(counts, on='idema', how='left')\n",
    "    stations[f'{var}_pct'] = (stations[f'{var}_count'] / stations['n_records'] * 100).round(1)\n",
    "\n",
    "pct_cols = [f'{v}_pct' for v in available_vars]\n",
    "stations['overall_pct'] = stations[pct_cols].mean(axis=1).round(1)\n",
    "\n",
    "print(f'Stations: {len(stations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:49.631997Z",
     "start_time": "2026-01-22T14:13:48.030386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "heatmap_df = stations.sort_values('overall_pct', ascending=False).copy()\n",
    "heatmap_df['label'] = heatmap_df['name'].str[:22] + ' (' + heatmap_df['alt'].astype(int).astype(str) + 'm)'\n",
    "heatmap_data = heatmap_df.set_index('label')[pct_cols]\n",
    "heatmap_data.columns = [AEMET_VARS[c.replace('_pct', '')] for c in heatmap_data.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, max(10, len(heatmap_data) * 0.35)))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn', vmin=0, vmax=100, ax=ax,\n",
    "            cbar_kws={'label': 'Availability %'}, annot_kws={'size': 8})\n",
    "ax.set_title('Availability by Station and Variable (%)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'station_variable_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis \n",
    "\n",
    "**Note:** High correlation shown for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:50.203185Z",
     "start_time": "2026-01-22T14:13:49.659279Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_bal[available_vars].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, ax=ax, annot_kws={'size': 9})\n",
    "ax.set_title('Variable Correlation Matrix', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'correlation_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:50.222426Z",
     "start_time": "2026-01-22T14:13:50.217123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find and show highly correlated pairs (informative only)\n",
    "print('HIGHLY CORRELATED PAIRS (|r| >= 0.9)')\n",
    "print('=' * 80)\n",
    "print('Note: For information only. Decide based on your model needs.')\n",
    "print('-' * 80)\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i, var1 in enumerate(available_vars):\n",
    "    for j, var2 in enumerate(available_vars):\n",
    "        if i < j:\n",
    "            corr = corr_matrix.loc[var1, var2]\n",
    "            if abs(corr) >= 0.9:\n",
    "                a1 = avail_df[avail_df['var'] == var1]['availability_pct'].values[0]\n",
    "                a2 = avail_df[avail_df['var'] == var2]['availability_pct'].values[0]\n",
    "                high_corr_pairs.append({'var1': var1, 'var2': var2, 'corr': round(corr, 3),\n",
    "                                       'avail1': a1, 'avail2': a2})\n",
    "                print(f'{var1:<10} ~ {var2:<10}  r = {corr:>6.3f}  (avail: {a1:.0f}% vs {a2:.0f}%)')\n",
    "\n",
    "if not high_corr_pairs:\n",
    "    print('No pairs with |r| >= 0.9 found')\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:50.301711Z",
     "start_time": "2026-01-22T14:13:50.296351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary table: Variable, Availability, Correlations\n",
    "print('\\nVARIABLE SUMMARY')\n",
    "print('=' * 90)\n",
    "print(f'{\"Variable\":<10} {\"Description\":<15} {\"Availability\":>12} {\"Correlated with (r>=0.9)\":<40}')\n",
    "print('-' * 90)\n",
    "\n",
    "for _, row in avail_df.iterrows():\n",
    "    var = row['var']\n",
    "    corr_with = []\n",
    "    for _, p in high_corr_df.iterrows() if len(high_corr_df) > 0 else []:\n",
    "        if p['var1'] == var:\n",
    "            corr_with.append(f\"{p['var2']}({p['corr']:.2f})\")\n",
    "        elif p['var2'] == var:\n",
    "            corr_with.append(f\"{p['var1']}({p['corr']:.2f})\")\n",
    "    corr_str = ', '.join(corr_with) if corr_with else '-'\n",
    "    print(f'{var:<10} {row[\"label\"]:<15} {row[\"availability_pct\"]:>11.1f}%  {corr_str:<40}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geographic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:50.455167Z",
     "start_time": "2026-01-22T14:13:50.377128Z"
    }
   },
   "outputs": [],
   "source": [
    "### Interactive Map - Data Availability\n",
    "m_avail = folium.Map(location=[stations['lat'].mean(), stations['lon'].mean()], zoom_start=8, tiles=None)\n",
    "\n",
    "folium.TileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}',\n",
    "                 attr='Esri', name='Topographic').add_to(m_avail)\n",
    "folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m_avail)\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    pct = row['overall_pct']\n",
    "    color = '#2ecc71' if pct >= 80 else '#f39c12' if pct >= 50 else '#e74c3c'\n",
    "    popup = f'<b>{row[\"name\"]}</b><br>ID: {row[\"idema\"]}<br>Alt: {row[\"alt\"]:.0f}m<br>Avail: {pct:.0f}%'\n",
    "    folium.CircleMarker([row['lat'], row['lon']], radius=10, color='black', weight=2,\n",
    "                        fill=True, fillColor=color, fillOpacity=0.85,\n",
    "                        popup=folium.Popup(popup, max_width=250)).add_to(m_avail)\n",
    "\n",
    "folium.LayerControl().add_to(m_avail)\n",
    "legend = '''<div style=\"position:fixed;bottom:50px;left:50px;z-index:1000;background:white;padding:10px;border:2px solid #333;border-radius:5px\">\n",
    "<b>Availability</b><br>\n",
    "<i style=\"background:#2ecc71;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> ‚â•80%<br>\n",
    "<i style=\"background:#f39c12;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> 50-80%<br>\n",
    "<i style=\"background:#e74c3c;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> <50%</div>'''\n",
    "m_avail.get_root().html.add_child(folium.Element(legend))\n",
    "m_avail.save(str(OUTPUT_DIR / 'map_availability.html'))\n",
    "print('Saved: map_availability.html')\n",
    "m_avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:51.240370Z",
     "start_time": "2026-01-22T14:13:50.574453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Interactive Maps - Insolation and Pressure Availability\n",
    "\n",
    "# Calculate availability per station for specific variables\n",
    "def calc_var_availability(df, stations_df, var):\n",
    "    avail = []\n",
    "    for _, st in stations_df.iterrows():\n",
    "        st_data = df[df['idema'] == st['idema']]\n",
    "        total = len(st_data)\n",
    "        valid = st_data[var].notna().sum() if var in st_data.columns else 0\n",
    "        pct = (valid / total * 100) if total > 0 else 0\n",
    "        avail.append({\n",
    "            'idema': st['idema'],\n",
    "            'name': st['name'],\n",
    "            'lat': st['lat'],\n",
    "            'lon': st['lon'],\n",
    "            'alt': st['alt'],\n",
    "            f'{var}_pct': pct,\n",
    "            f'{var}_count': valid,\n",
    "            'total': total\n",
    "        })\n",
    "    return pd.DataFrame(avail)\n",
    "\n",
    "# Calculate availability for inso and pres\n",
    "inso_avail = calc_var_availability(df_bal, stations, 'inso')\n",
    "pres_avail = calc_var_availability(df_bal, stations, 'pres')\n",
    "\n",
    "print('VARIABLE AVAILABILITY BY STATION')\n",
    "print('=' * 70)\n",
    "print(f'{\"Variable\":<15} {\"Stations with data\":<20} {\"Mean availability\":<20}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Insolation\":<15} {(inso_avail[\"inso_pct\"] > 0).sum():<20} {inso_avail[\"inso_pct\"].mean():.1f}%')\n",
    "print(f'{\"Pressure\":<15} {(pres_avail[\"pres_pct\"] > 0).sum():<20} {pres_avail[\"pres_pct\"].mean():.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:51.288642Z",
     "start_time": "2026-01-22T14:13:51.244353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map 1: Insolation (inso) availability\n",
    "m_inso = folium.Map(location=[stations['lat'].mean(), stations['lon'].mean()], zoom_start=8, tiles=None)\n",
    "\n",
    "folium.TileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}',\n",
    "                 attr='Esri', name='Topographic').add_to(m_inso)\n",
    "folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m_inso)\n",
    "\n",
    "for _, row in inso_avail.iterrows():\n",
    "    pct = row['inso_pct']\n",
    "    \n",
    "    if pct >= 80:\n",
    "        color = '#2ecc71'  # Green\n",
    "    elif pct >= 50:\n",
    "        color = '#f39c12'  # Orange\n",
    "    elif pct > 0:\n",
    "        color = '#e74c3c'  # Red\n",
    "    else:\n",
    "        color = '#95a5a6'  # Gray - no data\n",
    "    \n",
    "    popup = f'''\n",
    "    <b>{row[\"name\"]}</b><br>\n",
    "    ID: {row[\"idema\"]}<br>\n",
    "    Alt: {row[\"alt\"]:.0f}m<br>\n",
    "    <hr>\n",
    "    <b>Insolation (inso)</b><br>\n",
    "    Availability: {pct:.1f}%<br>\n",
    "    Records: {row[\"inso_count\"]:,.0f} / {row[\"total\"]:,.0f}\n",
    "    '''\n",
    "    \n",
    "    # Size based on availability\n",
    "    radius = 12 if pct >= 50 else 8 if pct > 0 else 6\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        [row['lat'], row['lon']], \n",
    "        radius=radius, \n",
    "        color='black', \n",
    "        weight=2,\n",
    "        fill=True, \n",
    "        fillColor=color, \n",
    "        fillOpacity=0.85,\n",
    "        popup=folium.Popup(popup, max_width=250)\n",
    "    ).add_to(m_inso)\n",
    "\n",
    "folium.LayerControl().add_to(m_inso)\n",
    "\n",
    "legend_inso = '''\n",
    "<div style=\"position:fixed;bottom:50px;left:50px;z-index:1000;background:white;padding:10px;border:2px solid #333;border-radius:5px\">\n",
    "<b>Insolation (inso)</b><br>\n",
    "<i style=\"background:#2ecc71;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> ‚â•80%<br>\n",
    "<i style=\"background:#f39c12;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> 50-80%<br>\n",
    "<i style=\"background:#e74c3c;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> 1-50%<br>\n",
    "<i style=\"background:#95a5a6;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> No data\n",
    "</div>\n",
    "'''\n",
    "m_inso.get_root().html.add_child(folium.Element(legend_inso))\n",
    "\n",
    "m_inso.save(str(OUTPUT_DIR / 'map_insolation_availability.html'))\n",
    "print('‚úÖ Saved: map_insolation_availability.html')\n",
    "m_inso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:51.376841Z",
     "start_time": "2026-01-22T14:13:51.321137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map 2: Pressure (pres) availability\n",
    "m_pres = folium.Map(location=[stations['lat'].mean(), stations['lon'].mean()], zoom_start=8, tiles=None)\n",
    "\n",
    "folium.TileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}',\n",
    "                 attr='Esri', name='Topographic').add_to(m_pres)\n",
    "folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m_pres)\n",
    "\n",
    "for _, row in pres_avail.iterrows():\n",
    "    pct = row['pres_pct']\n",
    "    \n",
    "    if pct >= 80:\n",
    "        color = '#3498db'  # Blue\n",
    "    elif pct >= 50:\n",
    "        color = '#9b59b6'  # Purple\n",
    "    elif pct > 0:\n",
    "        color = '#e74c3c'  # Red\n",
    "    else:\n",
    "        color = '#95a5a6'  # Gray - no data\n",
    "    \n",
    "    popup = f'''\n",
    "    <b>{row[\"name\"]}</b><br>\n",
    "    ID: {row[\"idema\"]}<br>\n",
    "    Alt: {row[\"alt\"]:.0f}m<br>\n",
    "    <hr>\n",
    "    <b>Pressure (pres)</b><br>\n",
    "    Availability: {pct:.1f}%<br>\n",
    "    Records: {row[\"pres_count\"]:,.0f} / {row[\"total\"]:,.0f}\n",
    "    '''\n",
    "    \n",
    "    radius = 12 if pct >= 50 else 8 if pct > 0 else 6\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        [row['lat'], row['lon']], \n",
    "        radius=radius, \n",
    "        color='black', \n",
    "        weight=2,\n",
    "        fill=True, \n",
    "        fillColor=color, \n",
    "        fillOpacity=0.85,\n",
    "        popup=folium.Popup(popup, max_width=250)\n",
    "    ).add_to(m_pres)\n",
    "\n",
    "folium.LayerControl().add_to(m_pres)\n",
    "\n",
    "legend_pres = '''\n",
    "<div style=\"position:fixed;bottom:50px;left:50px;z-index:1000;background:white;padding:10px;border:2px solid #333;border-radius:5px\">\n",
    "<b>Pressure (pres)</b><br>\n",
    "<i style=\"background:#3498db;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> ‚â•80%<br>\n",
    "<i style=\"background:#9b59b6;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> 50-80%<br>\n",
    "<i style=\"background:#e74c3c;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> 1-50%<br>\n",
    "<i style=\"background:#95a5a6;width:12px;height:12px;display:inline-block;border-radius:50%\"></i> No data\n",
    "</div>\n",
    "'''\n",
    "m_pres.get_root().html.add_child(folium.Element(legend_pres))\n",
    "\n",
    "m_pres.save(str(OUTPUT_DIR / 'map_pressure_availability.html'))\n",
    "print('‚úÖ Saved: map_pressure_availability.html')\n",
    "m_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:51.403211Z",
     "start_time": "2026-01-22T14:13:51.397846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary table: Stations with insolation and pressure data\n",
    "print('\\nSTATIONS WITH INSOLATION DATA')\n",
    "print('=' * 70)\n",
    "inso_stations = inso_avail[inso_avail['inso_pct'] > 0].sort_values('inso_pct', ascending=False)\n",
    "print(f'{\"Station\":<35} {\"Alt (m)\":<10} {\"Avail %\":<10} {\"Records\":<15}')\n",
    "print('-' * 70)\n",
    "for _, row in inso_stations.iterrows():\n",
    "    print(f'{row[\"name\"][:33]:<35} {row[\"alt\"]:<10.0f} {row[\"inso_pct\"]:<10.1f} {row[\"inso_count\"]:,.0f}')\n",
    "\n",
    "print(f'\\nTotal stations with inso: {len(inso_stations)}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STATIONS WITH PRESSURE DATA')\n",
    "print('=' * 70)\n",
    "pres_stations = pres_avail[pres_avail['pres_pct'] > 0].sort_values('pres_pct', ascending=False)\n",
    "print(f'{\"Station\":<35} {\"Alt (m)\":<10} {\"Avail %\":<10} {\"Records\":<15}')\n",
    "print('-' * 70)\n",
    "for _, row in pres_stations.iterrows():\n",
    "    print(f'{row[\"name\"][:33]:<35} {row[\"alt\"]:<10.0f} {row[\"pres_pct\"]:<10.1f} {row[\"pres_count\"]:,.0f}')\n",
    "\n",
    "print(f'\\nTotal stations with pres: {len(pres_stations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:13:51.436151Z",
     "start_time": "2026-01-22T14:13:51.406221Z"
    }
   },
   "outputs": [],
   "source": [
    "# STATION-TO-STATION VARIANCE ANALYSIS - PER ISLAND\n",
    "# Compare stations WITHIN each island only (not between islands)\n",
    "\n",
    "print('STATION-TO-STATION VARIANCE ANALYSIS - PER ISLAND')\n",
    "print('=' * 80)\n",
    "print('Comparing measurements between stations WITHIN the same island')\n",
    "print('=' * 80)\n",
    "\n",
    "from itertools import combinations\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "# Variables to analyze\n",
    "vars_to_analyze = ['ta', 'hr', 'prec', 'vv', 'pres', 'inso']\n",
    "vars_to_analyze = [v for v in vars_to_analyze if v in df_bal.columns]\n",
    "\n",
    "VAR_LABELS = {\n",
    "    'ta': 'Temperature (¬∞C)', 'hr': 'Humidity (%)', 'prec': 'Precipitation (mm)',\n",
    "    'vv': 'Wind Speed (m/s)', 'pres': 'Pressure (hPa)', 'inso': 'Insolation (h)'\n",
    "}\n",
    "\n",
    "ISLANDS = {\n",
    "    'Mallorca': {'lon_min': 2.3, 'lon_max': 3.5, 'lat_min': 39.2, 'lat_max': 39.95},\n",
    "    'Menorca': {'lon_min': 3.8, 'lon_max': 4.35, 'lat_min': 39.8, 'lat_max': 40.1},\n",
    "    'Ibiza': {'lon_min': 1.15, 'lon_max': 1.65, 'lat_min': 38.85, 'lat_max': 39.15},\n",
    "}\n",
    "\n",
    "# Prepare data with hourly resolution\n",
    "df_bal['hour'] = df_bal['fint'].dt.floor('h')\n",
    "\n",
    "# Get station info with island\n",
    "station_info = df_bal.groupby('idema').agg({\n",
    "    'lat': 'first',\n",
    "    'lon': 'first', \n",
    "    'ubi': 'first',\n",
    "    'alt': 'first'\n",
    "}).reset_index()\n",
    "station_info.rename(columns={'ubi': 'name'}, inplace=True)\n",
    "\n",
    "# Assign island to each station\n",
    "def detect_island(row):\n",
    "    for island, bbox in ISLANDS.items():\n",
    "        if bbox['lon_min'] <= row['lon'] <= bbox['lon_max'] and bbox['lat_min'] <= row['lat'] <= bbox['lat_max']:\n",
    "            return island\n",
    "    return 'Unknown'\n",
    "\n",
    "station_info['island'] = station_info.apply(detect_island, axis=1)\n",
    "\n",
    "print(f'\\nStations per island:')\n",
    "for island in ISLANDS.keys():\n",
    "    n = len(station_info[station_info['island'] == island])\n",
    "    print(f'  {island}: {n} stations')\n",
    "\n",
    "print(f'\\nVariables: {vars_to_analyze}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:01.585962Z",
     "start_time": "2026-01-22T14:13:51.440299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate pairwise differences for stations WITHIN each island\n",
    "print('\\nCalculating pairwise station differences PER ISLAND...')\n",
    "\n",
    "pairwise_results = []\n",
    "\n",
    "for island in ISLANDS.keys():\n",
    "    island_stations = station_info[station_info['island'] == island]['idema'].tolist()\n",
    "    \n",
    "    if len(island_stations) < 2:\n",
    "        print(f'  {island}: Not enough stations ({len(island_stations)})')\n",
    "        continue\n",
    "    \n",
    "    station_pairs = list(combinations(island_stations, 2))\n",
    "    print(f'  {island}: {len(island_stations)} stations ‚Üí {len(station_pairs)} pairs')\n",
    "    \n",
    "    for st1, st2 in station_pairs:\n",
    "        # Get station info\n",
    "        info1 = station_info[station_info['idema'] == st1].iloc[0]\n",
    "        info2 = station_info[station_info['idema'] == st2].iloc[0]\n",
    "        \n",
    "        # Calculate distance\n",
    "        dist = haversine((info1['lat'], info1['lon']), (info2['lat'], info2['lon']), unit=Unit.KILOMETERS)\n",
    "        alt_diff = abs(info1['alt'] - info2['alt'])\n",
    "        \n",
    "        # Get data for both stations\n",
    "        data1 = df_bal[df_bal['idema'] == st1][['hour'] + vars_to_analyze].groupby('hour').mean()\n",
    "        data2 = df_bal[df_bal['idema'] == st2][['hour'] + vars_to_analyze].groupby('hour').mean()\n",
    "        \n",
    "        # Merge on hour (same timestamp)\n",
    "        merged = data1.join(data2, lsuffix='_1', rsuffix='_2', how='inner')\n",
    "        \n",
    "        if len(merged) < 10:\n",
    "            continue\n",
    "        \n",
    "        pair_result = {\n",
    "            'island': island,\n",
    "            'station1': info1['name'],\n",
    "            'station2': info2['name'],\n",
    "            'idema1': st1,\n",
    "            'idema2': st2,\n",
    "            'distance_km': dist,\n",
    "            'alt_diff_m': alt_diff,\n",
    "            'n_common_hours': len(merged)\n",
    "        }\n",
    "        \n",
    "        # Calculate statistics for each variable\n",
    "        for var in vars_to_analyze:\n",
    "            col1, col2 = f'{var}_1', f'{var}_2'\n",
    "            if col1 in merged.columns and col2 in merged.columns:\n",
    "                mask = merged[[col1, col2]].notna().all(axis=1)\n",
    "                if mask.sum() < 10:\n",
    "                    continue\n",
    "                \n",
    "                v1 = merged.loc[mask, col1].values\n",
    "                v2 = merged.loc[mask, col2].values\n",
    "                \n",
    "                diff = v1 - v2\n",
    "                \n",
    "                pair_result[f'{var}_mean_diff'] = diff.mean()\n",
    "                pair_result[f'{var}_std_diff'] = diff.std()\n",
    "                pair_result[f'{var}_mae'] = np.abs(diff).mean()\n",
    "                pair_result[f'{var}_max_diff'] = np.abs(diff).max()\n",
    "                pair_result[f'{var}_correlation'] = np.corrcoef(v1, v2)[0, 1]\n",
    "                pair_result[f'{var}_n_obs'] = mask.sum()\n",
    "        \n",
    "        pairwise_results.append(pair_result)\n",
    "\n",
    "pairs_df = pd.DataFrame(pairwise_results)\n",
    "print(f'\\nTotal valid pairs: {len(pairs_df)}')\n",
    "print(pairs_df.groupby('island').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:01.635072Z",
     "start_time": "2026-01-22T14:14:01.626850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics by variable AND island\n",
    "print('\\n' + '=' * 100)\n",
    "print('PAIRWISE DIFFERENCE SUMMARY BY ISLAND')\n",
    "print('=' * 100)\n",
    "\n",
    "island_var_summary = []\n",
    "\n",
    "for island in ISLANDS.keys():\n",
    "    island_pairs = pairs_df[pairs_df['island'] == island]\n",
    "    \n",
    "    if len(island_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n{island.upper()} ({len(island_pairs)} pairs)')\n",
    "    print('-' * 90)\n",
    "    print(f'{\"Variable\":<20} {\"Mean Diff\":<12} {\"Std Diff\":<12} {\"MAE\":<12} {\"Max Diff\":<12} {\"Corr\":<10}')\n",
    "    print('-' * 90)\n",
    "    \n",
    "    for var in vars_to_analyze:\n",
    "        cols = [f'{var}_mean_diff', f'{var}_std_diff', f'{var}_mae', f'{var}_max_diff', f'{var}_correlation']\n",
    "        if not all(c in island_pairs.columns for c in cols):\n",
    "            continue\n",
    "        \n",
    "        valid = island_pairs[island_pairs[f'{var}_mae'].notna()]\n",
    "        if len(valid) == 0:\n",
    "            continue\n",
    "        \n",
    "        mean_diff = valid[f'{var}_mean_diff'].mean()\n",
    "        std_diff = valid[f'{var}_std_diff'].mean()\n",
    "        mae = valid[f'{var}_mae'].mean()\n",
    "        max_diff = valid[f'{var}_max_diff'].mean()\n",
    "        mean_corr = valid[f'{var}_correlation'].mean()\n",
    "        \n",
    "        island_var_summary.append({\n",
    "            'island': island,\n",
    "            'variable': var,\n",
    "            'label': VAR_LABELS.get(var, var),\n",
    "            'n_pairs': len(valid),\n",
    "            'mean_diff': mean_diff,\n",
    "            'std_diff': std_diff,\n",
    "            'mae': mae,\n",
    "            'max_diff': max_diff,\n",
    "            'correlation': mean_corr\n",
    "        })\n",
    "        \n",
    "        print(f'{VAR_LABELS.get(var, var):<20} {mean_diff:>+11.3f} {std_diff:>11.3f} {mae:>11.3f} {max_diff:>11.3f} {mean_corr:>9.3f}')\n",
    "\n",
    "island_summary_df = pd.DataFrame(island_var_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:03.274696Z",
     "start_time": "2026-01-22T14:14:01.638408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize MAE distribution per island\n",
    "fig, axes = plt.subplots(len(ISLANDS), len(vars_to_analyze[:6]), figsize=(18, 4 * len(ISLANDS)))\n",
    "\n",
    "for i, island in enumerate(ISLANDS.keys()):\n",
    "    island_pairs = pairs_df[pairs_df['island'] == island]\n",
    "    \n",
    "    for j, var in enumerate(vars_to_analyze[:6]):\n",
    "        ax = axes[i, j] if len(ISLANDS) > 1 else axes[j]\n",
    "        \n",
    "        mae_col = f'{var}_mae'\n",
    "        if mae_col not in island_pairs.columns or island_pairs[mae_col].isna().all():\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{island}\\n{VAR_LABELS.get(var, var)[:15]}', fontsize=10)\n",
    "            continue\n",
    "        \n",
    "        mae_values = island_pairs[mae_col].dropna()\n",
    "        \n",
    "        if len(mae_values) > 0:\n",
    "            ax.hist(mae_values, bins=min(20, len(mae_values)), color='steelblue', alpha=0.7, edgecolor='black')\n",
    "            ax.axvline(x=mae_values.mean(), color='red', linestyle='--', linewidth=2)\n",
    "            ax.set_xlabel('MAE')\n",
    "            ax.set_ylabel('Pairs')\n",
    "            ax.set_title(f'{island}\\n{VAR_LABELS.get(var, var)[:15]}\\nMAE={mae_values.mean():.2f}', fontsize=10)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{island}\\n{VAR_LABELS.get(var, var)[:15]}', fontsize=10)\n",
    "\n",
    "plt.suptitle('MAE Distribution Between Station Pairs (Within Island)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'pairwise_mae_per_island.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:04.884030Z",
     "start_time": "2026-01-22T14:14:03.325102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation vs Distance per island\n",
    "fig, axes = plt.subplots(len(ISLANDS), len(vars_to_analyze[:6]), figsize=(18, 4 * len(ISLANDS)))\n",
    "\n",
    "for i, island in enumerate(ISLANDS.keys()):\n",
    "    island_pairs = pairs_df[pairs_df['island'] == island]\n",
    "    \n",
    "    for j, var in enumerate(vars_to_analyze[:6]):\n",
    "        ax = axes[i, j] if len(ISLANDS) > 1 else axes[j]\n",
    "        \n",
    "        corr_col = f'{var}_correlation'\n",
    "        if corr_col not in island_pairs.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        \n",
    "        valid = island_pairs[['distance_km', corr_col]].dropna()\n",
    "        \n",
    "        if len(valid) < 3:\n",
    "            ax.text(0.5, 0.5, f'Not enough data\\n({len(valid)} pairs)', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{island} - {VAR_LABELS.get(var, var)[:12]}', fontsize=10)\n",
    "            continue\n",
    "        \n",
    "        ax.scatter(valid['distance_km'], valid[corr_col], alpha=0.6, s=40, c='steelblue')\n",
    "        \n",
    "        # Trend line\n",
    "        if len(valid) > 3:\n",
    "            z = np.polyfit(valid['distance_km'], valid[corr_col], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(valid['distance_km'].min(), valid['distance_km'].max(), 50)\n",
    "            ax.plot(x_line, p(x_line), 'r-', linewidth=2)\n",
    "            ax.text(0.95, 0.05, f'slope={z[0]:.4f}', transform=ax.transAxes, \n",
    "                   ha='right', fontsize=8, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.axhline(y=0.8, color='green', linestyle=':', alpha=0.5)\n",
    "        ax.set_xlabel('Distance (km)')\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.set_title(f'{island} - {VAR_LABELS.get(var, var)[:12]}', fontsize=10)\n",
    "        ax.set_ylim(-0.2, 1.05)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Station Correlation vs Distance (Within Island)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'correlation_vs_distance_per_island.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.260528Z",
     "start_time": "2026-01-22T14:14:04.930713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare islands side by side\n",
    "print('\\n' + '=' * 100)\n",
    "print('ISLAND COMPARISON: SPATIAL VARIANCE')\n",
    "print('=' * 100)\n",
    "\n",
    "# Pivot table for comparison\n",
    "if len(island_summary_df) > 0:\n",
    "    for metric in ['mae', 'correlation']:\n",
    "        print(f'\\n{metric.upper()}:')\n",
    "        pivot = island_summary_df.pivot(index='label', columns='island', values=metric)\n",
    "        print(pivot.round(3).to_string())\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# MAE comparison\n",
    "ax1 = axes[0]\n",
    "if len(island_summary_df) > 0:\n",
    "    pivot_mae = island_summary_df.pivot(index='label', columns='island', values='mae')\n",
    "    pivot_mae.plot(kind='bar', ax=ax1, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Mean Absolute Error')\n",
    "    ax1.set_title('MAE Between Stations by Island', fontweight='bold')\n",
    "    ax1.legend(title='Island')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Correlation comparison\n",
    "ax2 = axes[1]\n",
    "if len(island_summary_df) > 0:\n",
    "    pivot_corr = island_summary_df.pivot(index='label', columns='island', values='correlation')\n",
    "    pivot_corr.plot(kind='bar', ax=ax2, alpha=0.8, edgecolor='black')\n",
    "    ax2.set_ylabel('Mean Correlation')\n",
    "    ax2.set_title('Station Correlation by Island', fontweight='bold')\n",
    "    ax2.legend(title='Island')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'island_variance_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPATIAL vs TEMPORAL VARIABILITY ANALYSIS (AEMET Stations)\n",
    "# Analyze if sunshine and pressure vary more by LOCATION or by TIME\n",
    "\n",
    "print(\"SPATIAL vs TEMPORAL VARIABILITY ANALYSIS (AEMET)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Question: Do insolation and pressure vary more by LOCATION or by TIME?\")\n",
    "print(\"If temporal >> spatial ‚Üí fewer stations needed\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "variables = ['inso', 'pres']\n",
    "var_labels = {'inso': 'Insolation (h)', 'pres': 'Pressure (hPa)'}\n",
    "\n",
    "# Check which variables are available\n",
    "variables = [v for v in variables if v in df_bal.columns]\n",
    "if not variables:\n",
    "    print(\"‚ö†Ô∏è Required variables not found in df_bal\")\n",
    "    print(f\"Available columns: {df_bal.columns.tolist()}\")\n",
    "else:\n",
    "    # Add hour column if not present\n",
    "    if 'hour' not in df_bal.columns:\n",
    "        df_bal['hour'] = df_bal['fint'].dt.floor('h')\n",
    "    \n",
    "    # Assign island to stations if not done\n",
    "    if 'island' not in stations.columns:\n",
    "        def detect_island(row):\n",
    "            for island, bbox in ISLANDS.items():\n",
    "                if bbox['lon_min'] <= row['lon'] <= bbox['lon_max'] and bbox['lat_min'] <= row['lat'] <= bbox['lat_max']:\n",
    "                    return island\n",
    "            return 'Unknown'\n",
    "        stations['island'] = stations.apply(detect_island, axis=1)\n",
    "    \n",
    "    # Merge island info to df_bal\n",
    "    if 'island' not in df_bal.columns:\n",
    "        df_bal = df_bal.merge(stations[['idema', 'island']], on='idema', how='left')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for island in ISLANDS.keys():\n",
    "        island_stations = stations[stations['island'] == island]['idema'].tolist()\n",
    "        island_data = df_bal[df_bal['idema'].isin(island_stations)]\n",
    "        n_stations = len(island_stations)\n",
    "        \n",
    "        if n_stations < 2:\n",
    "            print(f\"‚ö†Ô∏è  {island}: Only {n_stations} station(s) - skipping\")\n",
    "            continue\n",
    "        \n",
    "        for var in variables:\n",
    "            if var not in island_data.columns:\n",
    "                continue\n",
    "            \n",
    "            # Only use data where variable is available\n",
    "            var_data = island_data[island_data[var].notna()]\n",
    "            \n",
    "            if len(var_data) < 100:\n",
    "                print(f\"‚ö†Ô∏è  {island}/{var}: Only {len(var_data)} records - skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Spatial std: variation across stations at same time\n",
    "            spatial_std = var_data.groupby('hour')[var].std().mean()\n",
    "            \n",
    "            # Temporal std: variation over time at same station\n",
    "            temporal_std = var_data.groupby('idema')[var].std().mean()\n",
    "            \n",
    "            # Spatial range: max - min across stations at same time\n",
    "            spatial_range = var_data.groupby('hour')[var].apply(lambda x: x.max() - x.min()).mean()\n",
    "            \n",
    "            # Number of stations with this variable\n",
    "            n_with_var = var_data['idema'].nunique()\n",
    "            \n",
    "            ratio = temporal_std / spatial_std if spatial_std > 0 else np.inf\n",
    "            \n",
    "            results.append({\n",
    "                'island': island,\n",
    "                'variable': var_labels.get(var, var),\n",
    "                'var_code': var,\n",
    "                'n_stations': n_with_var,\n",
    "                'spatial_std': spatial_std,\n",
    "                'spatial_range': spatial_range,\n",
    "                'temporal_std': temporal_std,\n",
    "                'ratio': ratio\n",
    "            })\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"‚ö†Ô∏è No valid results - check data availability for inso and pres\")\n",
    "    else:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\n{'Island':<12} {'Variable':<15} {'Stations':>8} {'Spatial œÉ':>10} {'Range':>10} {'Temporal œÉ':>11} {'Ratio':>8}\")\n",
    "        print(\"-\" * 80)\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['island']:<12} {row['variable']:<15} {row['n_stations']:>8} {row['spatial_std']:>10.2f} \"\n",
    "                  f\"{row['spatial_range']:>10.2f} {row['temporal_std']:>11.2f} {row['ratio']:>7.1f}x\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\\nINTERPRETATION:\")\n",
    "        print(\"  Ratio > 5  ‚Üí ONE station sufficient (time dominates)\")\n",
    "        print(\"  Ratio 2-5  ‚Üí FEW stations enough\")\n",
    "        print(\"  Ratio < 2  ‚Üí MULTIPLE stations needed (location matters)\")\n",
    "        \n",
    "        # Visualization\n",
    "        available_vars = results_df['var_code'].unique()\n",
    "        n_vars = len(available_vars)\n",
    "        n_islands = len(ISLANDS)\n",
    "        \n",
    "        if n_vars > 0:\n",
    "            fig, axes = plt.subplots(n_vars, n_islands, figsize=(14, 4 * n_vars))\n",
    "            if n_vars == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for col, island in enumerate(ISLANDS.keys()):\n",
    "                island_stations = stations[stations['island'] == island]['idema'].tolist()\n",
    "                island_data = df_bal[df_bal['idema'].isin(island_stations)]\n",
    "                \n",
    "                for row, var in enumerate(available_vars):\n",
    "                    ax = axes[row, col] if n_vars > 1 else axes[col]\n",
    "                    label = var_labels.get(var, var)\n",
    "                    \n",
    "                    if var not in island_data.columns:\n",
    "                        ax.text(0.5, 0.5, f'No {var} data', ha='center', va='center', transform=ax.transAxes)\n",
    "                        continue\n",
    "                    \n",
    "                    var_data = island_data[island_data[var].notna()]\n",
    "                    \n",
    "                    if len(var_data) < 10:\n",
    "                        ax.text(0.5, 0.5, f'Insufficient {var} data', ha='center', va='center', transform=ax.transAxes)\n",
    "                        continue\n",
    "                    \n",
    "                    hourly_stats = var_data.groupby('hour')[var].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "                    hourly_stats = hourly_stats.sort_values('hour').reset_index(drop=True)\n",
    "                    \n",
    "                    ax.fill_between(range(len(hourly_stats)), hourly_stats['min'], hourly_stats['max'], \n",
    "                                    alpha=0.3, color='blue', label='Spatial range')\n",
    "                    ax.plot(hourly_stats['mean'], 'b-', linewidth=1.5, label='Island mean')\n",
    "                    \n",
    "                    ax.set_title(f\"{island}\\n{label} ({var_data['idema'].nunique()} stations)\", fontsize=10, fontweight='bold')\n",
    "                    ax.set_xlabel('Hour index')\n",
    "                    \n",
    "                    if col == 0:\n",
    "                        ax.set_ylabel(label.split('(')[0].strip())\n",
    "                    if row == 0 and col == n_islands - 1:\n",
    "                        ax.legend(loc='upper right', fontsize=8)\n",
    "                    \n",
    "                    ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle('AEMET: Temporal Evolution with Spatial Range\\n(narrow bands = low spatial variance = fewer stations needed)', \n",
    "                         fontsize=12, fontweight='bold', y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUTPUT_DIR / 'aemet_spatial_temporal_variability.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "        # Conclusions\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üìã CONCLUSIONS (AEMET Data)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for var in available_vars:\n",
    "            label = var_labels.get(var, var)\n",
    "            var_results = results_df[results_df['var_code'] == var]\n",
    "            \n",
    "            if len(var_results) == 0:\n",
    "                continue\n",
    "                \n",
    "            avg_ratio = var_results['ratio'].mean()\n",
    "            avg_stations = var_results['n_stations'].mean()\n",
    "            \n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"  Average stations with data: {avg_stations:.0f}\")\n",
    "            print(f\"  Average Temporal/Spatial ratio: {avg_ratio:.1f}x\")\n",
    "            \n",
    "            if avg_ratio > 5:\n",
    "                print(\"  ‚úÖ HIGHLY STABLE SPATIALLY ‚Üí 1 station per island sufficient\")\n",
    "            elif avg_ratio > 2:\n",
    "                print(\"  ‚ö†Ô∏è MODERATELY STABLE ‚Üí 2-3 stations per island recommended\")\n",
    "            else:\n",
    "                print(\"  ‚ùå SIGNIFICANT SPATIAL VARIATION ‚Üí Multiple stations needed (4+)\")\n",
    "        \n",
    "        # Station coverage summary\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"STATION COVERAGE:\")\n",
    "        for var in available_vars:\n",
    "            label = var_labels.get(var, var)\n",
    "            stations_with_var = df_bal[df_bal[var].notna()]['idema'].nunique()\n",
    "            total_stations = df_bal['idema'].nunique()\n",
    "            print(f\"  {label}: {stations_with_var}/{total_stations} stations have data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.267031Z",
     "start_time": "2026-01-22T14:14:05.264159Z"
    }
   },
   "outputs": [],
   "source": [
    "print('STATIONS BY AVAILABILITY')\n",
    "print('=' * 70)\n",
    "for _, r in stations.sort_values('overall_pct', ascending=False).iterrows():\n",
    "    print(f'{r[\"idema\"]:<8} {r[\"name\"]:<40} {r[\"alt\"]:>5.0f}m  {r[\"overall_pct\"]:>5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.302819Z",
     "start_time": "2026-01-22T14:14:05.269448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare each high altitude station vs island average\n",
    "HIGH_ALT_THRESHOLD = 200  # meters\n",
    "\n",
    "numeric_vars = ['ta', 'tamax', 'tamin', 'hr', 'pres', 'vv', 'vmax', 'prec']\n",
    "numeric_vars = [v for v in numeric_vars if v in df_bal.columns]\n",
    "high_stations_df = stations[stations['alt'] >= HIGH_ALT_THRESHOLD].copy()\n",
    "island_avg = df_bal[numeric_vars].mean()\n",
    "\n",
    "\n",
    "print('HIGH ALTITUDE STATIONS vs ISLAND AVERAGE')\n",
    "print('=' * 120)\n",
    "\n",
    "for _, station in high_stations_df.sort_values('alt', ascending=False).iterrows():\n",
    "    station_data = df_bal[df_bal['idema'] == station['idema']]\n",
    "    station_avg = station_data[numeric_vars].mean()\n",
    "    \n",
    "    print(f'\\n{station[\"name\"]} ({station[\"idema\"]}) - {station[\"alt\"]:.0f}m')\n",
    "    print('-' * 80)\n",
    "    print(f'{\"Variable\":<12} {\"Station\":>12} {\"Island Avg\":>12} {\"Diff\":>12} {\"% Diff\":>10}')\n",
    "    print('-' * 80)\n",
    "    \n",
    "    for var in numeric_vars:\n",
    "        st_val = station_avg[var]\n",
    "        isl_val = island_avg[var]\n",
    "        diff = st_val - isl_val\n",
    "        pct = (diff / isl_val * 100) if isl_val != 0 else 0\n",
    "        print(f'{var:<12} {st_val:>12.2f} {isl_val:>12.2f} {diff:>+12.2f} {pct:>+9.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.309102Z",
     "start_time": "2026-01-22T14:14:05.305896Z"
    }
   },
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('FINAL SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'''\n",
    "AVAILABLE VARIABLES: {len(available_vars)}\n",
    "{available_vars}\n",
    "\n",
    "EXCLUDED (no data): {len(EXCLUDE_VARS)}\n",
    "{EXCLUDE_VARS}\n",
    "\n",
    "HIGHLY CORRELATED PAIRS: {len(high_corr_pairs)}\n",
    "(Not removed - for your consideration when building models)\n",
    "\n",
    "STATIONS: {len(stations)}\n",
    "Altitude range: {stations[\"alt\"].min():.0f}m - {stations[\"alt\"].max():.0f}m\n",
    "\n",
    "Best stations (>45% overall availability):\n",
    "''')\n",
    "for _, r in stations[stations['overall_pct'] >= 45].sort_values('overall_pct', ascending=False).iterrows():\n",
    "    print(f'  {r[\"idema\"]}: {r[\"name\"]} ({r[\"alt\"]:.0f}m) - {r[\"overall_pct\"]:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.326328Z",
     "start_time": "2026-01-22T14:14:05.311842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map of stations with low availability (<45%)\n",
    "low_avail = stations[stations['overall_pct'] < 45].copy()\n",
    "\n",
    "m_low = folium.Map(location=[stations['lat'].mean(), stations['lon'].mean()], zoom_start=8, tiles=None)\n",
    "\n",
    "folium.TileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}',\n",
    "                 attr='Esri', name='Topographic').add_to(m_low)\n",
    "\n",
    "for _, row in low_avail.iterrows():\n",
    "    pct = row['overall_pct']\n",
    "    color = '#f39c12' if pct >= 30 else '#e74c3c'\n",
    "    popup = f'<b>{row[\"name\"]}</b><br>ID: {row[\"idema\"]}<br>Alt: {row[\"alt\"]:.0f}m<br>Avail: {pct:.1f}%'\n",
    "    folium.CircleMarker([row['lat'], row['lon']], radius=10, color='black', weight=2,\n",
    "                        fill=True, fillColor=color, fillOpacity=0.85,\n",
    "                        popup=folium.Popup(popup, max_width=250)).add_to(m_low)\n",
    "\n",
    "folium.LayerControl().add_to(m_low)\n",
    "m_low.save(str(OUTPUT_DIR / 'map_low_availability.html'))\n",
    "print(f'Stations with <50% availability: {len(low_avail)}')\n",
    "m_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.336015Z",
     "start_time": "2026-01-22T14:14:05.330169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export\n",
    "avail_df.to_csv(OUTPUT_DIR / 'variable_availability.csv', index=False)\n",
    "stations.to_csv(OUTPUT_DIR / 'station_availability.csv', index=False)\n",
    "if len(high_corr_df) > 0:\n",
    "    high_corr_df.to_csv(OUTPUT_DIR / 'high_correlations.csv', index=False)\n",
    "\n",
    "print('Exported to results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:05.341011Z",
     "start_time": "2026-01-22T14:14:05.338940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter to remove stations above 300 meters\n",
    "\n",
    "ALT_THRESHOLD = 70\n",
    "coastal_stations = stations[stations['alt'] < ALT_THRESHOLD].copy()\n",
    "stations = coastal_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:06.842487Z",
     "start_time": "2026-01-22T14:14:05.343538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "heatmap_df = coastal_stations.sort_values('overall_pct', ascending=False).copy()\n",
    "heatmap_df['label'] = heatmap_df['name'].str[:22] + ' (' + heatmap_df['alt'].astype(int).astype(str) + 'm)'\n",
    "heatmap_data = heatmap_df.set_index('label')[pct_cols]\n",
    "heatmap_data.columns = [AEMET_VARS[c.replace('_pct', '')] for c in heatmap_data.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, max(10, len(heatmap_data) * 0.35)))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn', vmin=0, vmax=100, ax=ax,\n",
    "            cbar_kws={'label': 'Availability %'}, annot_kws={'size': 8})\n",
    "ax.set_title('Availability by Station and Variable (%)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'station_variable_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:07.586109Z",
     "start_time": "2026-01-22T14:14:06.897225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get coastal station IDs\n",
    "coastal_ids = stations['idema'].tolist()\n",
    "\n",
    "# Filter df_bal to only coastal stations\n",
    "df_coastal = df_bal[df_bal['idema'].isin(coastal_ids)].copy()\n",
    "\n",
    "# Get available variables\n",
    "available_vars = [v for v in ['ta', 'tamax', 'tamin', 'hr', 'prec', 'vv', 'vmax', 'dv', 'dmax', 'pres', 'inso'] \n",
    "                  if v in df_coastal.columns]\n",
    "\n",
    "print(f'\\nVariables for correlation: {available_vars}')\n",
    "\n",
    "# Correlation matrix - ALL stations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Left: All stations\n",
    "corr_all = df_bal[available_vars].corr()\n",
    "mask_all = np.triu(np.ones_like(corr_all, dtype=bool), k=1)\n",
    "sns.heatmap(corr_all, mask=mask_all, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, ax=axes[0], annot_kws={'size': 8})\n",
    "axes[0].set_title(f'All Stations (n={df_bal[\"idema\"].nunique()})', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Right: Coastal stations only\n",
    "corr_coastal = df_coastal[available_vars].corr()\n",
    "mask_coastal = np.triu(np.ones_like(corr_coastal, dtype=bool), k=1)\n",
    "sns.heatmap(corr_coastal, mask=mask_coastal, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, ax=axes[1], annot_kws={'size': 8})\n",
    "axes[1].set_title(f'Coastal Stations Only (alt < {ALT_THRESHOLD}m, n={len(coastal_ids)})', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.suptitle('Variable Correlation: All Stations vs Coastal Stations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'correlation_matrix_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Show correlation differences\n",
    "print('\\nCORRELATION DIFFERENCES (Coastal - All)')\n",
    "print('=' * 60)\n",
    "corr_diff = corr_coastal - corr_all\n",
    "print('Significant changes (|diff| > 0.05):')\n",
    "for i in range(len(available_vars)):\n",
    "    for j in range(i+1, len(available_vars)):\n",
    "        diff = corr_diff.iloc[i, j]\n",
    "        if abs(diff) > 0.05:\n",
    "            print(f'  {available_vars[i]:<6} - {available_vars[j]:<6}: {diff:+.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:07.676520Z",
     "start_time": "2026-01-22T14:14:07.590876Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import date\n",
    "\n",
    "ISLANDS = {\n",
    "    'Mallorca': {'lon_min': 2.3, 'lon_max': 3.5, 'lat_min': 39.2, 'lat_max': 39.95},\n",
    "    'Menorca': {'lon_min': 3.8, 'lon_max': 4.35, 'lat_min': 39.8, 'lat_max': 40.1},\n",
    "    'Ibiza': {'lon_min': 1.15, 'lon_max': 1.65, 'lat_min': 38.85, 'lat_max': 39.15},\n",
    "}\n",
    "\n",
    "interp_vars = ['prec', 'tamax', 'ta', 'tamin', 'hr', 'vv', 'dv', 'vmax', 'dmax']\n",
    "interp_vars = [v for v in interp_vars if v in df_bal.columns]\n",
    "\n",
    "VAR_LABELS = {\n",
    "    'prec': 'Precipitation (mm)', 'tamax': 'Temp Max (¬∞C)', 'ta': 'Temperature (¬∞C)', \n",
    "    'tamin': 'Temp Min (¬∞C)', 'hr': 'Humidity (%)', 'vv': 'Wind Speed (m/s)',\n",
    "    'dv': 'Wind Dir (¬∞)', 'vmax': 'Wind Gust (m/s)', 'dmax': 'Gust Dir (¬∞)'\n",
    "}\n",
    "\n",
    "VAR_SCALES = {\n",
    "    'prec': {'vmin': 0, 'vmax': 20, 'cmap': 'Blues'},\n",
    "    'tamax': {'vmin': 0, 'vmax': 40, 'cmap': 'RdYlBu_r'},\n",
    "    'ta': {'vmin': 0, 'vmax': 40, 'cmap': 'RdYlBu_r'},\n",
    "    'tamin': {'vmin': 0, 'vmax': 40, 'cmap': 'RdYlBu_r'},\n",
    "    'hr': {'vmin': 30, 'vmax': 100, 'cmap': 'BuGn'},\n",
    "    'vv': {'vmin': 0, 'vmax': 15, 'cmap': 'YlOrRd'},\n",
    "    'dv': {'vmin': 0, 'vmax': 360, 'cmap': 'hsv'},\n",
    "    'vmax': {'vmin': 0, 'vmax': 25, 'cmap': 'YlOrRd'},\n",
    "    'dmax': {'vmin': 0, 'vmax': 360, 'cmap': 'hsv'},\n",
    "}\n",
    "\n",
    "def detect_island(row):\n",
    "    lat, lon = row['lat'], row['lon']\n",
    "    for island, bbox in ISLANDS.items():\n",
    "        if bbox['lon_min'] <= lon <= bbox['lon_max'] and bbox['lat_min'] <= lat <= bbox['lat_max']:\n",
    "            return island\n",
    "    return 'Unknown'\n",
    "\n",
    "coastal_stations['island'] = coastal_stations.apply(detect_island, axis=1)\n",
    "dates = sorted(df_bal['fint'].dt.date.unique())\n",
    "\n",
    "date_dropdown = widgets.Dropdown(options=dates, description='Date:', value=date(2022, 10, 26))\n",
    "var_dropdown = widgets.Dropdown(options=interp_vars, description='Variable:', value='ta')\n",
    "island_dropdown = widgets.Dropdown(options=list(ISLANDS.keys()), description='Island:', value='Mallorca')\n",
    "btn = widgets.Button(description='Show Interpolation')\n",
    "output = widgets.Output()\n",
    "\n",
    "def plot_interpolation(b):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        sel_date = date_dropdown.value\n",
    "        sel_var = var_dropdown.value\n",
    "        sel_island = island_dropdown.value\n",
    "        bbox = ISLANDS[sel_island]\n",
    "        scale = VAR_SCALES[sel_var]\n",
    "        \n",
    "        day_data = df_bal[df_bal['fint'].dt.date == sel_date]\n",
    "        island_stations = coastal_stations[coastal_stations['island'] == sel_island]\n",
    "        day_island = day_data[day_data['idema'].isin(island_stations['idema'])]\n",
    "        \n",
    "        station_vals = day_island.groupby('idema')[sel_var].mean().reset_index()\n",
    "        station_vals = station_vals.merge(island_stations[['idema', 'lon', 'lat', 'name']], on='idema')\n",
    "        station_vals = station_vals.dropna(subset=[sel_var])\n",
    "        \n",
    "        if len(station_vals) < 3:\n",
    "            print(f'Not enough stations ({len(station_vals)})')\n",
    "            return\n",
    "        \n",
    "        points = station_vals[['lon', 'lat']].values\n",
    "        values = station_vals[sel_var].values\n",
    "        \n",
    "        grid_lon = np.linspace(bbox['lon_min'], bbox['lon_max'], 50)\n",
    "        grid_lat = np.linspace(bbox['lat_min'], bbox['lat_max'], 50)\n",
    "        grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n",
    "        \n",
    "        linear_interp = LinearNDInterpolator(points, values)\n",
    "        grid_values = linear_interp(grid_lon_2d, grid_lat_2d)\n",
    "        \n",
    "        levels = np.linspace(scale['vmin'], scale['vmax'], 40)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        im = ax.contourf(grid_lon_2d, grid_lat_2d, grid_values, levels=levels,\n",
    "                        cmap=scale['cmap'], vmin=scale['vmin'], vmax=scale['vmax'], \n",
    "                        alpha=0.8, extend='both')\n",
    "        plt.colorbar(im, ax=ax, label=VAR_LABELS[sel_var])\n",
    "        ax.scatter(points[:, 0], points[:, 1], c='black', s=60, zorder=5, \n",
    "                  edgecolors='white', linewidth=1.5)\n",
    "        for _, row in station_vals.iterrows():\n",
    "            ax.annotate(f'{row[sel_var]:.1f}', (row['lon'], row['lat']), fontsize=8,\n",
    "                       xytext=(3, 3), textcoords='offset points',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        ax.set_xlim(bbox['lon_min'], bbox['lon_max'])\n",
    "        ax.set_ylim(bbox['lat_min'], bbox['lat_max'])\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title(f'{sel_island} - {VAR_LABELS[sel_var]} - {sel_date}', fontweight='bold', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        coverage = (~np.isnan(grid_values)).mean() * 100\n",
    "        print(f'\\nLinear Interpolation Stats:')\n",
    "        print(f'  Coverage: {coverage:.1f}%')\n",
    "        print(f'  Mean: {np.nanmean(grid_values):.2f}')\n",
    "        print(f'  Min: {np.nanmin(grid_values):.2f}')\n",
    "        print(f'  Max: {np.nanmax(grid_values):.2f}')\n",
    "\n",
    "btn.on_click(plot_interpolation)\n",
    "display(widgets.HBox([date_dropdown, var_dropdown, island_dropdown, btn]))\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_weather_at_location(df_bal, stations, target_date, target_lat, target_lon, variables=None):\n",
    "    \"\"\"\n",
    "    Get interpolated weather data for a specific date and location.\n",
    "    \n",
    "    Parameters:\n",
    "        df_bal: DataFrame with weather observations\n",
    "        stations: DataFrame with station metadata (idema, lat, lon)\n",
    "        target_date: date object or string 'YYYY-MM-DD'\n",
    "        target_lat: latitude of target location\n",
    "        target_lon: longitude of target location\n",
    "        variables: list of variables to interpolate (default: all available)\n",
    "    \n",
    "    Returns:\n",
    "        dict with interpolated values for each variable\n",
    "    \"\"\"\n",
    "    if isinstance(target_date, str):\n",
    "        target_date = pd.to_datetime(target_date).date()\n",
    "    \n",
    "    if variables is None:\n",
    "        variables = ['ta', 'tamax', 'tamin', 'hr', 'prec', 'vv', 'vmax', 'dv', 'pres']\n",
    "    variables = [v for v in variables if v in df_bal.columns]\n",
    "    \n",
    "    day_data = df_bal[df_bal['fint'].dt.date == target_date]\n",
    "    \n",
    "    if len(day_data) == 0:\n",
    "        return {'error': f'No data for {target_date}'}\n",
    "    \n",
    "    results = {'date': target_date, 'lat': target_lat, 'lon': target_lon}\n",
    "    \n",
    "    for var in variables:\n",
    "        station_vals = day_data.groupby('idema')[var].mean().reset_index()\n",
    "        station_vals = station_vals.merge(stations[['idema', 'lon', 'lat']], on='idema')\n",
    "        station_vals = station_vals.dropna(subset=[var])\n",
    "        \n",
    "        if len(station_vals) < 3:\n",
    "            results[var] = None\n",
    "            continue\n",
    "        \n",
    "        points = station_vals[['lon', 'lat']].values\n",
    "        values = station_vals[var].values\n",
    "        \n",
    "        linear_interp = LinearNDInterpolator(points, values)\n",
    "        nearest_interp = NearestNDInterpolator(points, values)\n",
    "        \n",
    "        value = linear_interp(target_lon, target_lat)\n",
    "        if np.isnan(value):\n",
    "            value = nearest_interp(target_lon, target_lat)\n",
    "            results[f'{var}_method'] = 'nearest'\n",
    "        else:\n",
    "            results[f'{var}_method'] = 'linear'\n",
    "        \n",
    "        results[var] = float(value)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "target_date = '2022-10-26'\n",
    "target_lat = 39.5695  \n",
    "target_lon = 2.7396\n",
    "\n",
    "result = get_weather_at_location(df_bal, stations, target_date, target_lat, target_lon)\n",
    "\n",
    "print(f\"Weather at ({target_lat}, {target_lon}) on {target_date}:\")\n",
    "print(\"=\" * 50)\n",
    "for key, val in result.items():\n",
    "    if key in ['date', 'lat', 'lon'] or '_method' in key:\n",
    "        continue\n",
    "    method = result.get(f'{key}_method', '')\n",
    "    if val is not None:\n",
    "        print(f\"  {key:>8}: {val:>8.2f}  ({method})\")\n",
    "    else:\n",
    "        print(f\"  {key:>8}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:14:08.485832Z",
     "start_time": "2026-01-22T14:14:07.783764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Highest STD analysis - by variable, day, and station\n",
    "print('SPATIAL VARIABILITY ANALYSIS (Standard Deviation)')\n",
    "print('=' * 80)\n",
    "\n",
    "df_bal['date'] = df_bal['fint'].dt.date\n",
    "analysis_vars = ['prec', 'tamax', 'ta', 'tamin', 'hr', 'vv', 'vmax']\n",
    "analysis_vars = [v for v in analysis_vars if v in df_bal.columns]\n",
    "\n",
    "# 1. Overall STD per variable\n",
    "print('\\n1. OVERALL STD BY VARIABLE')\n",
    "print('-' * 40)\n",
    "for var in analysis_vars:\n",
    "    std_val = df_bal[var].std()\n",
    "    print(f'{var:<10} {VAR_LABELS.get(var, var):<25} STD: {std_val:>8.2f}')\n",
    "\n",
    "# 2. Day with highest spatial STD per variable\n",
    "print('\\n2. DAY WITH HIGHEST SPATIAL VARIABILITY (per variable)')\n",
    "print('-' * 80)\n",
    "print(f'{\"Variable\":<10} {\"Date\":<12} {\"STD\":>8} {\"Mean\":>8} {\"Min\":>8} {\"Max\":>8}')\n",
    "print('-' * 80)\n",
    "\n",
    "high_std_days = {}\n",
    "for var in analysis_vars:\n",
    "    daily_std = df_bal.groupby('date')[var].std()\n",
    "    max_std_date = daily_std.idxmax()\n",
    "    max_std = daily_std.max()\n",
    "    day_data = df_bal[df_bal['date'] == max_std_date][var]\n",
    "    high_std_days[var] = {'date': max_std_date, 'std': max_std}\n",
    "    print(f'{var:<10} {str(max_std_date):<12} {max_std:>8.2f} {day_data.mean():>8.2f} {day_data.min():>8.2f} {day_data.max():>8.2f}')\n",
    "\n",
    "# 3. Station with highest STD per variable\n",
    "print('\\n3. STATION WITH HIGHEST TEMPORAL VARIABILITY (per variable)')\n",
    "print('-' * 80)\n",
    "print(f'{\"Variable\":<10} {\"Station\":<30} {\"STD\":>8} {\"Mean\":>8}')\n",
    "print('-' * 80)\n",
    "\n",
    "for var in analysis_vars:\n",
    "    station_std = df_bal.groupby('idema')[var].std().dropna()\n",
    "    if len(station_std) == 0:\n",
    "        print(f'{var:<10} {\"No data\":<30}')\n",
    "        continue\n",
    "    max_std_station = station_std.idxmax()\n",
    "    max_std = station_std.max()\n",
    "    station_match = stations[stations['idema'] == max_std_station]\n",
    "    if len(station_match) > 0:\n",
    "        station_name = station_match['name'].values[0]\n",
    "    else:\n",
    "        station_name = max_std_station\n",
    "    station_mean = df_bal[df_bal['idema'] == max_std_station][var].mean()\n",
    "    print(f'{var:<10} {str(station_name)[:28]:<30} {max_std:>8.2f} {station_mean:>8.2f}')\n",
    "\n",
    "# 4. Heatmap: Daily STD per variable\n",
    "print('\\n4. DAILY SPATIAL STD HEATMAP')\n",
    "daily_std_df = df_bal.groupby('date')[analysis_vars].std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(daily_std_df.T, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'STD'})\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Variable')\n",
    "ax.set_title('Daily Spatial Standard Deviation by Variable', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'daily_std_heatmap.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 5. Top 10 days with highest overall variability\n",
    "print('\\n5. TOP 10 DAYS WITH HIGHEST OVERALL VARIABILITY')\n",
    "print('-' * 60)\n",
    "daily_std_df['total_std'] = daily_std_df[['ta', 'hr', 'vv']].mean(axis=1)\n",
    "top_days = daily_std_df.nlargest(10, 'total_std')\n",
    "print(f'{\"Date\":<15} {\"ta STD\":>10} {\"hr STD\":>10} {\"vv STD\":>10}')\n",
    "print('-' * 60)\n",
    "for date, row in top_days.iterrows():\n",
    "    print(f'{str(date):<15} {row[\"ta\"]:>10.2f} {row[\"hr\"]:>10.2f} {row[\"vv\"]:>10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import Delaunay, ConvexHull\n",
    "# from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator, RBFInterpolator\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# class HullMultipointInterpolator:\n",
    "#     \"\"\"Linear inside hull + IDW on boundary samples outside.\"\"\"\n",
    "    \n",
    "#     def __init__(self, n_boundary_samples=50, k_neighbors=10, power=2):\n",
    "#         self.n_boundary_samples = n_boundary_samples\n",
    "#         self.k_neighbors = k_neighbors\n",
    "#         self.power = power\n",
    "        \n",
    "#     def _sample_hull_boundary(self, points, n_samples):\n",
    "#         \"\"\"Sample points along hull boundary using scipy ConvexHull.\"\"\"\n",
    "#         hull = ConvexHull(points)\n",
    "#         hull_vertices = points[hull.vertices]\n",
    "#         hull_closed = np.vstack([hull_vertices, hull_vertices[0]])\n",
    "        \n",
    "#         edges = np.diff(hull_closed, axis=0)\n",
    "#         edge_lengths = np.linalg.norm(edges, axis=1)\n",
    "#         total_length = edge_lengths.sum()\n",
    "        \n",
    "#         sample_distances = np.linspace(0, total_length, n_samples, endpoint=False)\n",
    "        \n",
    "#         boundary_pts = []\n",
    "#         cumulative = 0\n",
    "#         edge_idx = 0\n",
    "        \n",
    "#         for d in sample_distances:\n",
    "#             while edge_idx < len(edge_lengths) - 1 and cumulative + edge_lengths[edge_idx] < d:\n",
    "#                 cumulative += edge_lengths[edge_idx]\n",
    "#                 edge_idx += 1\n",
    "            \n",
    "#             t = (d - cumulative) / edge_lengths[edge_idx] if edge_lengths[edge_idx] > 0 else 0\n",
    "#             t = np.clip(t, 0, 1)\n",
    "#             pt = hull_closed[edge_idx] + t * edges[edge_idx]\n",
    "#             boundary_pts.append(pt)\n",
    "        \n",
    "#         return np.array(boundary_pts), hull_closed\n",
    "        \n",
    "#     def fit(self, points, values):\n",
    "#         self.points = np.asarray(points)\n",
    "#         self.values = np.asarray(values)\n",
    "        \n",
    "#         self.linear = LinearNDInterpolator(self.points, self.values)\n",
    "#         self.delaunay = Delaunay(self.points)\n",
    "#         self.boundary_pts, self.hull_closed = self._sample_hull_boundary(self.points, self.n_boundary_samples)\n",
    "#         self.boundary_vals = self.linear(self.boundary_pts)\n",
    "        \n",
    "#         valid = ~np.isnan(self.boundary_vals)\n",
    "#         self.boundary_pts = self.boundary_pts[valid]\n",
    "#         self.boundary_vals = self.boundary_vals[valid]\n",
    "        \n",
    "#         self.knn = KNeighborsRegressor(\n",
    "#             n_neighbors=min(self.k_neighbors, len(self.boundary_pts)),\n",
    "#             weights='distance', p=self.power\n",
    "#         )\n",
    "#         self.knn.fit(self.boundary_pts, self.boundary_vals)\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, targets):\n",
    "#         targets = np.asarray(targets)\n",
    "#         if targets.ndim == 1:\n",
    "#             targets = targets.reshape(1, -1)\n",
    "        \n",
    "#         result = np.zeros(len(targets))\n",
    "#         inside_mask = self.delaunay.find_simplex(targets) >= 0\n",
    "        \n",
    "#         if inside_mask.any():\n",
    "#             result[inside_mask] = self.linear(targets[inside_mask])\n",
    "#         if (~inside_mask).any():\n",
    "#             result[~inside_mask] = self.knn.predict(targets[~inside_mask])\n",
    "        \n",
    "#         return result\n",
    "\n",
    "# # ============================================================\n",
    "# # INTERPOLATION METHODS\n",
    "# # ============================================================\n",
    "# def find_nearest_point_on_hull(hull_points, target_point):\n",
    "#     \"\"\"Find nearest point on convex hull boundary.\"\"\"\n",
    "#     hull = ConvexHull(hull_points)\n",
    "#     min_dist = np.inf\n",
    "#     nearest_point = None\n",
    "    \n",
    "#     for simplex in hull.simplices:\n",
    "#         p1, p2 = hull_points[simplex[0]], hull_points[simplex[1]]\n",
    "#         edge_vec = p2 - p1\n",
    "#         edge_len_sq = np.dot(edge_vec, edge_vec)\n",
    "        \n",
    "#         if edge_len_sq == 0:\n",
    "#             nearest_on_edge = p1\n",
    "#         else:\n",
    "#             t = max(0, min(1, np.dot(target_point - p1, edge_vec) / edge_len_sq))\n",
    "#             nearest_on_edge = p1 + t * edge_vec\n",
    "        \n",
    "#         dist = np.linalg.norm(target_point - nearest_on_edge)\n",
    "#         if dist < min_dist:\n",
    "#             min_dist = dist\n",
    "#             nearest_point = nearest_on_edge\n",
    "    \n",
    "#     return nearest_point, min_dist\n",
    "\n",
    "# def interpolate_with_method(points, values, grid_pts, method):\n",
    "#     \"\"\"Apply selected interpolation method.\"\"\"\n",
    "    \n",
    "#     if method == 'linear':\n",
    "#         interp = LinearNDInterpolator(points, values)\n",
    "#         return interp(grid_pts), 'Linear (NaN outside hull)'\n",
    "    \n",
    "#     elif method == 'nearest':\n",
    "#         interp = NearestNDInterpolator(points, values)\n",
    "#         return interp(grid_pts), 'Nearest Neighbor'\n",
    "    \n",
    "#     elif method == 'linear_nearest':\n",
    "#         linear = LinearNDInterpolator(points, values)\n",
    "#         nearest = NearestNDInterpolator(points, values)\n",
    "#         linear_vals = linear(grid_pts)\n",
    "#         nearest_vals = nearest(grid_pts)\n",
    "#         return np.where(np.isnan(linear_vals), nearest_vals, linear_vals), 'Linear + Nearest Station'\n",
    "    \n",
    "#     elif method == 'linear_hull':\n",
    "#         linear = LinearNDInterpolator(points, values)\n",
    "#         linear_vals = linear(grid_pts)\n",
    "#         try:\n",
    "#             hull_check = Delaunay(points)\n",
    "#         except:\n",
    "#             nearest = NearestNDInterpolator(points, values)\n",
    "#             return np.where(np.isnan(linear_vals), nearest(grid_pts), linear_vals), 'Linear + Nearest'\n",
    "        \n",
    "#         result = linear_vals.copy()\n",
    "#         for i, (pt, val) in enumerate(zip(grid_pts, linear_vals)):\n",
    "#             if np.isnan(val):\n",
    "#                 nearest_on_hull, _ = find_nearest_point_on_hull(points, pt)\n",
    "#                 result[i] = linear(nearest_on_hull.reshape(1, -1))[0]\n",
    "#                 if np.isnan(result[i]):\n",
    "#                     nearest = NearestNDInterpolator(points, values)\n",
    "#                     result[i] = nearest(pt.reshape(1, -1))[0]\n",
    "#         return result, 'Linear + Hull Boundary (basic)'\n",
    "    \n",
    "#     elif method == 'hull_gradient':\n",
    "#         linear = LinearNDInterpolator(points, values)\n",
    "#         try:\n",
    "#             hull_check = Delaunay(points)\n",
    "#         except:\n",
    "#             nearest = NearestNDInterpolator(points, values)\n",
    "#             return nearest(grid_pts), 'Nearest (hull failed)'\n",
    "        \n",
    "#         result = linear(grid_pts)\n",
    "#         epsilon = 0.01\n",
    "        \n",
    "#         for i, (pt, val) in enumerate(zip(grid_pts, result)):\n",
    "#             if np.isnan(val):\n",
    "#                 nearest_on_hull, dist = find_nearest_point_on_hull(points, pt)\n",
    "#                 hull_val = linear(nearest_on_hull.reshape(1, -1))[0]\n",
    "                \n",
    "#                 if np.isnan(hull_val):\n",
    "#                     nearest = NearestNDInterpolator(points, values)\n",
    "#                     result[i] = nearest(pt.reshape(1, -1))[0]\n",
    "#                     continue\n",
    "                \n",
    "#                 direction = pt - nearest_on_hull\n",
    "#                 dir_norm = np.linalg.norm(direction)\n",
    "                \n",
    "#                 if dir_norm > 0:\n",
    "#                     direction = direction / dir_norm\n",
    "#                     inside_pt = nearest_on_hull - epsilon * direction\n",
    "#                     inside_val = linear(inside_pt.reshape(1, -1))[0]\n",
    "                    \n",
    "#                     if not np.isnan(inside_val):\n",
    "#                         gradient = (hull_val - inside_val) / epsilon\n",
    "#                         decay = np.exp(-dist * 2)\n",
    "#                         result[i] = hull_val + gradient * dist * decay\n",
    "#                     else:\n",
    "#                         result[i] = hull_val\n",
    "#                 else:\n",
    "#                     result[i] = hull_val\n",
    "        \n",
    "#         return result, 'Linear + Hull Gradient'\n",
    "    \n",
    "#     elif method == 'hull_decay':\n",
    "#         linear = LinearNDInterpolator(points, values)\n",
    "#         try:\n",
    "#             hull_check = Delaunay(points)\n",
    "#         except:\n",
    "#             nearest = NearestNDInterpolator(points, values)\n",
    "#             return nearest(grid_pts), 'Nearest (hull failed)'\n",
    "        \n",
    "#         result = linear(grid_pts)\n",
    "#         regional_mean = np.mean(values)\n",
    "#         decay_rate = 3.0\n",
    "        \n",
    "#         for i, (pt, val) in enumerate(zip(grid_pts, result)):\n",
    "#             if np.isnan(val):\n",
    "#                 nearest_on_hull, dist = find_nearest_point_on_hull(points, pt)\n",
    "#                 hull_val = linear(nearest_on_hull.reshape(1, -1))[0]\n",
    "                \n",
    "#                 if np.isnan(hull_val):\n",
    "#                     nearest = NearestNDInterpolator(points, values)\n",
    "#                     hull_val = nearest(pt.reshape(1, -1))[0]\n",
    "                \n",
    "#                 weight = np.exp(-decay_rate * dist)\n",
    "#                 result[i] = weight * hull_val + (1 - weight) * regional_mean\n",
    "        \n",
    "#         return result, 'Linear + Hull Decay to Mean'\n",
    "    \n",
    "#     elif method == 'hull_idw_blend':\n",
    "#         linear = LinearNDInterpolator(points, values)\n",
    "#         try:\n",
    "#             hull_check = Delaunay(points)\n",
    "#         except:\n",
    "#             return idw_interpolate(points, values, grid_pts, 2), 'IDW (hull failed)'\n",
    "        \n",
    "#         result = linear(grid_pts)\n",
    "        \n",
    "#         distances = cdist(grid_pts, points)\n",
    "#         distances = np.maximum(distances, 1e-10)\n",
    "#         weights = 1 / distances**2\n",
    "#         weights_norm = weights / weights.sum(axis=1, keepdims=True)\n",
    "#         idw_vals = (weights_norm * values).sum(axis=1)\n",
    "        \n",
    "#         blend_distance = 0.1\n",
    "        \n",
    "#         for i, (pt, val) in enumerate(zip(grid_pts, result)):\n",
    "#             if np.isnan(val):\n",
    "#                 nearest_on_hull, dist = find_nearest_point_on_hull(points, pt)\n",
    "#                 hull_val = linear(nearest_on_hull.reshape(1, -1))[0]\n",
    "                \n",
    "#                 if np.isnan(hull_val):\n",
    "#                     result[i] = idw_vals[i]\n",
    "#                     continue\n",
    "                \n",
    "#                 blend = 1 - np.exp(-dist / blend_distance)\n",
    "#                 result[i] = (1 - blend) * hull_val + blend * idw_vals[i]\n",
    "        \n",
    "#         return result, 'Linear + Hull-IDW Blend'\n",
    "    \n",
    "#     elif method == 'hull_multipoint':\n",
    "#         # Use the class-based implementation\n",
    "#         try:\n",
    "#             interp = HullMultipointInterpolator(n_boundary_samples=50, k_neighbors=10)\n",
    "#             interp.fit(points, values)\n",
    "#             return interp.predict(grid_pts), 'Linear + Hull Multipoint (sklearn)'\n",
    "#         except:\n",
    "#             distances = cdist(grid_pts, points)\n",
    "#             distances = np.maximum(distances, 1e-10)\n",
    "#             weights = 1 / distances**2\n",
    "#             weights_norm = weights / weights.sum(axis=1, keepdims=True)\n",
    "#             return (weights_norm * values).sum(axis=1), 'IDW (hull_multipoint failed)'\n",
    "    \n",
    "#     elif method == 'idw2':\n",
    "#         distances = cdist(grid_pts, points)\n",
    "#         distances = np.maximum(distances, 1e-10)\n",
    "#         weights = 1 / distances**2\n",
    "#         weights_norm = weights / weights.sum(axis=1, keepdims=True)\n",
    "#         return (weights_norm * values).sum(axis=1), 'IDW (power=2)'\n",
    "    \n",
    "#     elif method == 'idw3':\n",
    "#         distances = cdist(grid_pts, points)\n",
    "#         distances = np.maximum(distances, 1e-10)\n",
    "#         weights = 1 / distances**3\n",
    "#         weights_norm = weights / weights.sum(axis=1, keepdims=True)\n",
    "#         return (weights_norm * values).sum(axis=1), 'IDW (power=3)'\n",
    "    \n",
    "#     elif method == 'rbf_smooth':\n",
    "#         try:\n",
    "#             rbf = RBFInterpolator(points, values, kernel='thin_plate_spline', smoothing=0.5)\n",
    "#             result = rbf(grid_pts)\n",
    "#             val_min, val_max = values.min(), values.max()\n",
    "#             val_range = val_max - val_min\n",
    "#             result = np.clip(result, val_min - 0.2 * val_range, val_max + 0.2 * val_range)\n",
    "#             return result, 'RBF Smooth (clipped)'\n",
    "#         except:\n",
    "#             return interpolate_with_method(points, values, grid_pts, 'idw2')\n",
    "    \n",
    "#     elif method == 'rbf_tps':\n",
    "#         try:\n",
    "#             rbf = RBFInterpolator(points, values, kernel='thin_plate_spline')\n",
    "#             return rbf(grid_pts), 'RBF (thin plate spline)'\n",
    "#         except:\n",
    "#             return interpolate_with_method(points, values, grid_pts, 'idw2')\n",
    "    \n",
    "#     elif method == 'knn5':\n",
    "#         k = min(5, len(points))\n",
    "#         knn = KNeighborsRegressor(n_neighbors=k, weights='distance')\n",
    "#         knn.fit(points, values)\n",
    "#         return knn.predict(grid_pts), 'KNN (k=5, sklearn)'\n",
    "    \n",
    "#     return LinearNDInterpolator(points, values)(grid_pts), 'Linear'\n",
    "\n",
    "# # ============================================================\n",
    "# # WIDGET INTERFACE\n",
    "# # ============================================================\n",
    "# available_dates = sorted(df_bal['fint'].dt.date.unique())\n",
    "# available_vars = [v for v in AEMET_VARS.keys() if v in df_bal.columns]\n",
    "# island_options = ['All'] + list(ISLANDS.keys())\n",
    "\n",
    "# method_options = [\n",
    "#     ('Linear (NaN outside)', 'linear'),\n",
    "#     ('Nearest Neighbor', 'nearest'),\n",
    "#     ('Linear + Nearest Station', 'linear_nearest'),\n",
    "#     ('Linear + Hull Boundary (basic)', 'linear_hull'),\n",
    "#     ('‚îÄ‚îÄ‚îÄ IMPROVED HULL METHODS ‚îÄ‚îÄ‚îÄ', 'separator'),\n",
    "#     ('Linear + Hull Gradient', 'hull_gradient'),\n",
    "#     ('Linear + Hull Decay to Mean', 'hull_decay'),\n",
    "#     ('Linear + Hull-IDW Blend', 'hull_idw_blend'),\n",
    "#     ('Linear + Hull Multipoint (sklearn)', 'hull_multipoint'),\n",
    "#     ('‚îÄ‚îÄ‚îÄ OTHER METHODS ‚îÄ‚îÄ‚îÄ', 'separator2'),\n",
    "#     ('IDW (power=2)', 'idw2'),\n",
    "#     ('IDW (power=3)', 'idw3'),\n",
    "#     ('RBF Smooth (clipped)', 'rbf_smooth'),\n",
    "#     ('RBF (thin plate spline)', 'rbf_tps'),\n",
    "#     ('KNN (k=5, sklearn)', 'knn5'),\n",
    "# ]\n",
    "\n",
    "# from datetime import date\n",
    "# date_dropdown = widgets.Dropdown(options=available_dates, value=date(2022, 10, 26), description='Date:')\n",
    "# var_dropdown = widgets.Dropdown(options=[(AEMET_VARS[v], v) for v in available_vars], value='hr', description='Variable:')\n",
    "# island_dropdown = widgets.Dropdown(options=island_options, value='Mallorca', description='Island:')\n",
    "# method_dropdown = widgets.Dropdown(options=method_options, value='hull_multipoint', description='Method:')\n",
    "# show_hull = widgets.Checkbox(value=True, description='Show Hull')\n",
    "# show_labels = widgets.Checkbox(value=True, description='Show Labels')\n",
    "# show_boundary = widgets.Checkbox(value=False, description='Show Boundary Samples')\n",
    "# show_btn = widgets.Button(description='Show Interpolation', button_style='primary')\n",
    "# compare_btn = widgets.Button(description='Compare Hull Methods', button_style='info')\n",
    "# output = widgets.Output()\n",
    "\n",
    "# def get_station_data(sel_date, sel_var, sel_island):\n",
    "#     day_data = df_bal[df_bal['fint'].dt.date == sel_date]\n",
    "#     station_data = day_data.groupby(['idema', 'lat', 'lon']).agg({sel_var: 'mean'}).reset_index().dropna()\n",
    "    \n",
    "#     if sel_island and sel_island != 'All':\n",
    "#         bbox = ISLANDS[sel_island]\n",
    "#         station_data = station_data[\n",
    "#             (station_data['lon'] >= bbox['lon_min'] - 0.2) & \n",
    "#             (station_data['lon'] <= bbox['lon_max'] + 0.2) &\n",
    "#             (station_data['lat'] >= bbox['lat_min'] - 0.2) & \n",
    "#             (station_data['lat'] <= bbox['lat_max'] + 0.2)\n",
    "#         ]\n",
    "#     return station_data\n",
    "\n",
    "# def show_interpolation(b):\n",
    "#     with output:\n",
    "#         clear_output(wait=True)\n",
    "        \n",
    "#         sel_date = date_dropdown.value\n",
    "#         sel_var = var_dropdown.value\n",
    "#         sel_island = island_dropdown.value if island_dropdown.value != 'All' else None\n",
    "#         sel_method = method_dropdown.value\n",
    "        \n",
    "#         if 'separator' in sel_method:\n",
    "#             print(\"Please select an actual method\")\n",
    "#             return\n",
    "        \n",
    "#         station_data = get_station_data(sel_date, sel_var, sel_island)\n",
    "        \n",
    "#         if len(station_data) < 2:\n",
    "#             print(f\"Not enough stations\")\n",
    "#             return\n",
    "        \n",
    "#         points = station_data[['lon', 'lat']].values\n",
    "#         values = station_data[sel_var].values\n",
    "        \n",
    "#         bbox = ISLANDS[sel_island] if sel_island else BBOX\n",
    "#         grid_lon = np.linspace(bbox['lon_min'], bbox['lon_max'], 60)\n",
    "#         grid_lat = np.linspace(bbox['lat_min'], bbox['lat_max'], 60)\n",
    "#         grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n",
    "#         grid_pts = np.column_stack([grid_lon_2d.ravel(), grid_lat_2d.ravel()])\n",
    "        \n",
    "#         grid_values, method_label = interpolate_with_method(points, values, grid_pts, sel_method)\n",
    "#         grid_values = grid_values.reshape(grid_lon_2d.shape)\n",
    "        \n",
    "#         fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#         im = ax.contourf(grid_lon_2d, grid_lat_2d, grid_values, levels=20, cmap='RdYlBu_r', extend='both')\n",
    "#         ax.scatter(points[:, 0], points[:, 1], c='black', s=80, marker='^', edgecolors='white', linewidth=1.5, zorder=5)\n",
    "        \n",
    "#         if show_hull.value:\n",
    "#             try:\n",
    "#                 hull = ConvexHull(points)\n",
    "#                 hull_pts = points[hull.vertices]\n",
    "#                 hull_closed = np.vstack([hull_pts, hull_pts[0]])\n",
    "#                 ax.plot(hull_closed[:, 0], hull_closed[:, 1], 'k--', linewidth=2, alpha=0.7)\n",
    "#             except:\n",
    "#                 pass\n",
    "        \n",
    "#         # Show boundary samples for hull_multipoint\n",
    "#         if show_boundary.value and sel_method == 'hull_multipoint':\n",
    "#             try:\n",
    "#                 interp = HullMultipointInterpolator(n_boundary_samples=50, k_neighbors=10)\n",
    "#                 interp.fit(points, values)\n",
    "#                 ax.scatter(interp.boundary_pts[:, 0], interp.boundary_pts[:, 1], \n",
    "#                           c='lime', s=20, alpha=0.8, zorder=4, label='Boundary Samples')\n",
    "#                 ax.legend(loc='upper right')\n",
    "#             except:\n",
    "#                 pass\n",
    "        \n",
    "#         if show_labels.value:\n",
    "#             for _, row in station_data.iterrows():\n",
    "#                 ax.annotate(f'{row[sel_var]:.1f}', (row['lon'], row['lat']), \n",
    "#                            fontsize=8, ha='center', va='bottom', xytext=(0, 5), \n",
    "#                            textcoords='offset points',\n",
    "#                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "#         plt.colorbar(im, ax=ax, label=VAR_LABELS.get(sel_var, sel_var))\n",
    "#         ax.set_xlabel('Longitude')\n",
    "#         ax.set_ylabel('Latitude')\n",
    "#         ax.set_title(f'{VAR_LABELS.get(sel_var, sel_var)} - {sel_date}\\n{sel_island or \"All\"} | {method_label}', fontweight='bold')\n",
    "#         ax.grid(True, alpha=0.3)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(f\"Stations: {len(station_data)} | Range: {values.min():.1f} - {values.max():.1f}\")\n",
    "\n",
    "# def compare_hull_methods(b):\n",
    "#     with output:\n",
    "#         clear_output(wait=True)\n",
    "        \n",
    "#         sel_date = date_dropdown.value\n",
    "#         sel_var = var_dropdown.value\n",
    "#         sel_island = island_dropdown.value if island_dropdown.value != 'All' else None\n",
    "        \n",
    "#         station_data = get_station_data(sel_date, sel_var, sel_island)\n",
    "        \n",
    "#         if len(station_data) < 2:\n",
    "#             print(f\"Not enough stations\")\n",
    "#             return\n",
    "        \n",
    "#         points = station_data[['lon', 'lat']].values\n",
    "#         values = station_data[sel_var].values\n",
    "        \n",
    "#         bbox = ISLANDS[sel_island] if sel_island else BBOX\n",
    "#         grid_lon = np.linspace(bbox['lon_min'], bbox['lon_max'], 50)\n",
    "#         grid_lat = np.linspace(bbox['lat_min'], bbox['lat_max'], 50)\n",
    "#         grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n",
    "#         grid_pts = np.column_stack([grid_lon_2d.ravel(), grid_lat_2d.ravel()])\n",
    "        \n",
    "#         methods = ['linear_nearest', 'linear_hull', 'hull_gradient', 'hull_decay', 'hull_idw_blend', 'hull_multipoint']\n",
    "        \n",
    "#         fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "#         axes = axes.flatten()\n",
    "        \n",
    "#         for idx, method in enumerate(methods):\n",
    "#             ax = axes[idx]\n",
    "#             grid_values, method_label = interpolate_with_method(points, values, grid_pts, method)\n",
    "#             grid_values = grid_values.reshape(grid_lon_2d.shape)\n",
    "            \n",
    "#             im = ax.contourf(grid_lon_2d, grid_lat_2d, grid_values, levels=15, cmap='RdYlBu_r', extend='both')\n",
    "#             ax.scatter(points[:, 0], points[:, 1], c='black', s=40, marker='^', edgecolors='white')\n",
    "            \n",
    "#             try:\n",
    "#                 hull = ConvexHull(points)\n",
    "#                 hull_pts = points[hull.vertices]\n",
    "#                 hull_closed = np.vstack([hull_pts, hull_pts[0]])\n",
    "#                 ax.plot(hull_closed[:, 0], hull_closed[:, 1], 'k--', linewidth=1.5, alpha=0.6)\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             ax.set_title(method_label, fontweight='bold', fontsize=9)\n",
    "#             plt.colorbar(im, ax=ax)\n",
    "        \n",
    "#         plt.suptitle(f'{VAR_LABELS.get(sel_var, sel_var)} - {sel_date} | Comparison of Interpolation Methods', \n",
    "#                      fontsize=12, fontweight='bold', y=1.02)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(OUTPUT_DIR / 'hull_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(f\"Saved to {OUTPUT_DIR}/hull_methods_comparison.png\")\n",
    "\n",
    "# show_btn.on_click(show_interpolation)\n",
    "# compare_btn.on_click(compare_hull_methods)\n",
    "\n",
    "# print(\"INTERPOLATION EXPLORER (with Hull Multipoint using sklearn)\")\n",
    "# print(\"=\" * 60)\n",
    "# display(widgets.VBox([\n",
    "#     widgets.HBox([date_dropdown, var_dropdown, island_dropdown]),\n",
    "#     widgets.HBox([method_dropdown, show_hull, show_labels, show_boundary]),\n",
    "#     widgets.HBox([show_btn, compare_btn])\n",
    "# ]))\n",
    "# display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import Delaunay, ConvexHull\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Cache\n",
    "CACHE_DIR = Path('cache/openmeteo')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Variables\n",
    "AEMET_VARS = ['ta', 'hr', 'prec', 'vv', 'dv', 'pres', 'tamin', 'tamax', 'inso', 'pres_nmar', 'ts']\n",
    "OPENMETEO_VARS = [\n",
    "    'temperature_2m', 'apparent_temperature', 'dewpoint_2m', 'relative_humidity_2m',\n",
    "    'pressure_msl', 'precipitation', 'rain', 'wind_speed_10m', 'wind_direction_10m',\n",
    "    'wind_gusts_10m', 'cloud_cover', 'cloud_cover_low', 'cloud_cover_mid', 'cloud_cover_high',\n",
    "    'visibility', 'uv_index', 'uv_index_clear_sky', 'sunshine_duration',\n",
    "    'evapotranspiration', 'vapour_pressure_deficit'\n",
    "]\n",
    "\n",
    "def _get_openmeteo(lat, lon, target_date):\n",
    "    \"\"\"Fetch Open-Meteo data with cache.\"\"\"\n",
    "    if hasattr(target_date, 'date'):\n",
    "        target_date = target_date.date()\n",
    "    \n",
    "    start = target_date - timedelta(days=1)\n",
    "    end = target_date + timedelta(days=1)\n",
    "    \n",
    "    cache_key = hashlib.md5(f\"{lat:.4f}_{lon:.4f}_{start}_{end}\".encode()).hexdigest()\n",
    "    cache_file = CACHE_DIR / f\"{cache_key}.json\"\n",
    "    \n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "                params={\n",
    "                    'latitude': lat, 'longitude': lon,\n",
    "                    'start_date': str(start), 'end_date': str(end),\n",
    "                    'hourly': ','.join(OPENMETEO_VARS),\n",
    "                    'timezone': 'Europe/Madrid'\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    if 'hourly' not in data:\n",
    "        return {}\n",
    "    \n",
    "    times = pd.to_datetime(data['hourly']['time'])\n",
    "    mask = times.date == target_date\n",
    "    \n",
    "    result = {}\n",
    "    for var in OPENMETEO_VARS:\n",
    "        if var in data['hourly']:\n",
    "            vals = np.array(data['hourly'][var])[mask]\n",
    "            vals = vals[~pd.isna(vals)]\n",
    "            if len(vals) > 0:\n",
    "                result[var] = float(np.mean(vals))\n",
    "    return result\n",
    "\n",
    "def _get_aemet(df, lat, lon, target_date, var):\n",
    "    \"\"\"Get interpolated AEMET value.\"\"\"\n",
    "    if hasattr(target_date, 'date'):\n",
    "        target_date = target_date.date()\n",
    "    \n",
    "    if var not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    day_data = df[df['fint'].dt.date == target_date]\n",
    "    stations = day_data.groupby(['idema', 'lat', 'lon']).agg({var: 'mean'}).reset_index()\n",
    "    \n",
    "    # Fill missing with spatial KNN\n",
    "    missing = stations[var].isna()\n",
    "    if missing.any() and (~missing).sum() >= 2:\n",
    "        knn = KNeighborsRegressor(n_neighbors=min(3, (~missing).sum()), weights='distance')\n",
    "        knn.fit(stations.loc[~missing, ['lon', 'lat']].values, stations.loc[~missing, var].values)\n",
    "        stations.loc[missing, var] = knn.predict(stations.loc[missing, ['lon', 'lat']].values)\n",
    "    \n",
    "    stations = stations.dropna(subset=[var])\n",
    "    if len(stations) < 3:\n",
    "        return None\n",
    "    \n",
    "    points = stations[['lon', 'lat']].values\n",
    "    values = stations[var].values\n",
    "    target = np.array([[lon, lat]])\n",
    "    \n",
    "    linear = LinearNDInterpolator(points, values)\n",
    "    delaunay = Delaunay(points)\n",
    "    \n",
    "    # Inside hull\n",
    "    if delaunay.find_simplex(target)[0] >= 0:\n",
    "        return float(linear(target)[0])\n",
    "    \n",
    "    # Outside hull: boundary KNN\n",
    "    hull = ConvexHull(points)\n",
    "    hull_closed = np.vstack([points[hull.vertices], points[hull.vertices[0]]])\n",
    "    edges = np.diff(hull_closed, axis=0)\n",
    "    lengths = np.linalg.norm(edges, axis=1)\n",
    "    total = lengths.sum()\n",
    "    \n",
    "    boundary_pts = []\n",
    "    cum, idx = 0, 0\n",
    "    for d in np.linspace(0, total, 50, endpoint=False):\n",
    "        while idx < len(lengths) - 1 and cum + lengths[idx] < d:\n",
    "            cum += lengths[idx]\n",
    "            idx += 1\n",
    "        t = (d - cum) / lengths[idx] if lengths[idx] > 0 else 0\n",
    "        boundary_pts.append(hull_closed[idx] + np.clip(t, 0, 1) * edges[idx])\n",
    "    \n",
    "    boundary_pts = np.array(boundary_pts)\n",
    "    boundary_vals = linear(boundary_pts)\n",
    "    valid = ~np.isnan(boundary_vals)\n",
    "    boundary_pts, boundary_vals = boundary_pts[valid], boundary_vals[valid]\n",
    "    \n",
    "    if len(boundary_pts) < 2:\n",
    "        return None\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=min(10, len(boundary_pts)), weights='distance')\n",
    "    knn.fit(boundary_pts, boundary_vals)\n",
    "    return float(knn.predict(target)[0])\n",
    "\n",
    "def get_all_weather(df_aemet, lat, lon, target_date):\n",
    "    \"\"\"\n",
    "    Get all weather variables from AEMET and Open-Meteo.\n",
    "    \n",
    "    Parameters:\n",
    "        df_aemet: DataFrame with AEMET data\n",
    "        lat, lon: coordinates\n",
    "        target_date: date object\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'ae_*' (AEMET) and 'om_*' (OpenMeteo) keys\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # AEMET\n",
    "    for var in AEMET_VARS:\n",
    "        val = _get_aemet(df_aemet, lat, lon, target_date, var)\n",
    "        if val is not None:\n",
    "            result[f'ae_{var}'] = val\n",
    "    \n",
    "    # OpenMeteo\n",
    "    om = _get_openmeteo(lat, lon, target_date)\n",
    "    for var, val in om.items():\n",
    "        result[f'om_{var}'] = val\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================\n",
    "# USAGE\n",
    "# ============================================================\n",
    "print(\"WEATHER MODULE LOADED\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Usage: get_all_weather(df_aemet, lat, lon, date)\")\n",
    "print()\n",
    "\n",
    "# Example\n",
    "weather = get_all_weather(df_bal, lat=39.35, lon=2.98, target_date=date(2022, 10, 26))\n",
    "print(f\"Variables retrieved: {len(weather)}\")\n",
    "print(f\"AEMET: {len([k for k in weather if k.startswith('ae_')])}\")\n",
    "print(f\"OpenMeteo: {len([k for k in weather if k.startswith('om_')])}\")\n",
    "print()\n",
    "print(\"Sample values:\")\n",
    "for k in list(weather.keys())[:5]:\n",
    "    print(f\"  {k}: {weather[k]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = get_all_weather(df_bal, lat=39.35, lon=2.98, target_date=date(2022, 10, 26))\n",
    "\n",
    "# Access values\n",
    "temp_aemet = weather.get('ae_ta')\n",
    "temp_openmeteo = weather.get('om_temperature_2m')\n",
    "uv = weather.get('om_uv_index')\n",
    "humidity = weather.get('ae_hr')\n",
    "\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
