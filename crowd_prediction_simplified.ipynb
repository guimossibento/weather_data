{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3001746d",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — 3-Dataset Comparison\n",
    "\n",
    "Compare model performance across three dataset strategies:\n",
    "1. **Daytime only** — remove night hours, sklearn models only\n",
    "2. **Full 24h** — keep all data including noisy night counts  \n",
    "3. **Night = 0** — keep 24h but replace night counts with 0\n",
    "\n",
    "Models with `stat_exog_list` (NBEATSx, NHITS, TFT, TiDE, BiTCN) can use beach metadata for zero-shot prediction on new beaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b840dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T14:11:35.472062Z",
     "start_time": "2026-02-08T14:11:35.469830Z"
    }
   },
   "outputs": [],
   "source": [
    "# === PATHS ===\n",
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/dataset_comparison\"\n",
    "\n",
    "# === SAMPLING (for quick testing) ===\n",
    "SAMPLE_FRAC = 1.0\n",
    "MAX_BEACHES = 3\n",
    "\n",
    "# === MODEL PARAMETERS ===\n",
    "MAX_STEPS = 500\n",
    "EARLY_STOP_PATIENCE = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 24\n",
    "\n",
    "# === TIME ===\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6\n",
    "\n",
    "# === FLAGS ===\n",
    "RUN_SKLEARN = True\n",
    "RUN_NEURALFORECAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f569a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator: mps\n",
      "XGB: True, LGBM: True, CatBoost: True, NF: True\n"
     ]
    }
   ],
   "source": [
    "import json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except: HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except: HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except: HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import NBEATSx, NHITS, TFT, TiDE, BiTCN\n",
    "    HAS_NF = True\n",
    "except Exception as e:\n",
    "    print(f\"NeuralForecast error: {e}\")\n",
    "    HAS_NF = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    accelerator = 'mps'\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "\n",
    "print(f\"Accelerator: {accelerator}\")\n",
    "print(f\"XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, max_count):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_mae = (mae / max_count) * 100 if max_count > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE': rel_mae}\n",
    "\n",
    "def eval_per_beach(df, y_pred, beach_col='unique_id'):\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        y_true = df.loc[mask, 'y'].values if 'y' in df.columns else df.loc[mask, 'count'].values\n",
    "        y_p = y_pred[mask.values]\n",
    "        max_count = y_true.max()\n",
    "        m = calc_metrics(y_true, y_p, max_count)\n",
    "        m['camera'] = b\n",
    "        m['max_count'] = max_count\n",
    "        m['n'] = mask.sum()\n",
    "        results.append(m)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b1a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 169241 rows, 41 beaches\n",
      "Date range: 1970-01-01 01:00:00 to 2023-01-27 08:00:00\n"
     ]
    }
   ],
   "source": [
    "def load_cache(cache_dir, model):\n",
    "    cache_path = Path(cache_dir) / model\n",
    "    records = []\n",
    "    for jf in cache_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(jf) as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    \n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {\n",
    "            'beach': r.get('beach') or r.get('beach_folder'),\n",
    "            'beach_folder': r.get('beach_folder'),\n",
    "            'datetime': r.get('datetime'),\n",
    "            'count': r.get('count')\n",
    "        }\n",
    "        for k, v in r.get('weather', {}).items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, COUNTING_MODEL)\n",
    "print(f\"Loaded: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")\n",
    "print(f\"Date range: {df_raw['datetime'].min()} to {df_raw['datetime'].max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6787a76-170c-4b3b-b136-307c21518da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               beach    beach_folder  \\\n",
      "0                                        Camp de Mar  livecampro/070   \n",
      "1                                       Port Andratx  livecampro/001   \n",
      "2                                         Cala Major  livecampro/002   \n",
      "3  Son Serra Marina (des de El Sol Sunshine Bar &...  livecampro/035   \n",
      "4              Playa de Muro (Hotel Playa Esperanza)  livecampro/024   \n",
      "\n",
      "             datetime      count  ae_ta  ae_hr  ae_prec  ae_vv  ae_dv  \\\n",
      "0 1970-01-01 01:00:00  29.308895    NaN    NaN      NaN    NaN    NaN   \n",
      "1 1970-01-01 01:00:00  13.774030    NaN    NaN      NaN    NaN    NaN   \n",
      "2 1970-01-01 01:00:00  69.105835    NaN    NaN      NaN    NaN    NaN   \n",
      "3 1970-01-01 01:00:00  40.888012    NaN    NaN      NaN    NaN    NaN   \n",
      "4 1970-01-01 01:00:00  36.653721    NaN    NaN      NaN    NaN    NaN   \n",
      "\n",
      "   ae_pres  ...  om_wind_direction_10m  om_wind_gusts_10m  om_cloud_cover  \\\n",
      "0      NaN  ...               262.0000            42.7333         70.6667   \n",
      "1      NaN  ...               262.0000            42.7333         70.6667   \n",
      "2      NaN  ...               263.0000            30.1333         65.6667   \n",
      "3      NaN  ...               251.6667            20.7333         70.3333   \n",
      "4      NaN  ...               246.6667            25.7000         76.6667   \n",
      "\n",
      "   om_cloud_cover_low  om_cloud_cover_mid  om_cloud_cover_high  \\\n",
      "0             15.3333             36.6667              62.3333   \n",
      "1             15.3333             36.6667              62.3333   \n",
      "2             14.6667             32.6667              55.6667   \n",
      "3             19.0000             20.6667              62.3333   \n",
      "4             25.6667             21.6667              68.0000   \n",
      "\n",
      "   om_sunshine_duration  om_vapour_pressure_deficit  om_direct_radiation  \\\n",
      "0                   0.0                      0.3333                  0.0   \n",
      "1                   0.0                      0.3333                  0.0   \n",
      "2                   0.0                      0.2967                  0.0   \n",
      "3                   0.0                      0.1867                  0.0   \n",
      "4                   0.0                      0.1433                  0.0   \n",
      "\n",
      "   om_shortwave_radiation  \n",
      "0                     0.0  \n",
      "1                     0.0  \n",
      "2                     0.0  \n",
      "3                     0.0  \n",
      "4                     0.0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d20f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 169241 -> 70501\n"
     ]
    }
   ],
   "source": [
    "EXCLUDE = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols']\n",
    "EXCLUDE_PREFIX = ['ibred', 'ClubNauticSoller', 'Guenthoer', 'youtube']\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw['beach_folder'].isin(EXCLUDE)]\n",
    "for p in EXCLUDE_PREFIX:\n",
    "    df_raw = df_raw[~df_raw['beach_folder'].str.startswith(p, na=False)]\n",
    "print(f\"Filtered: {before} -> {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bffe88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited to 3 beaches: 22167 rows\n",
      "Final: 22167 rows, 3 beaches\n"
     ]
    }
   ],
   "source": [
    "if SAMPLE_FRAC < 1.0:\n",
    "    df_raw = df_raw.sample(frac=SAMPLE_FRAC, random_state=42).sort_values('datetime').reset_index(drop=True)\n",
    "    print(f\"Sampled to {len(df_raw)}\")\n",
    "\n",
    "if MAX_BEACHES:\n",
    "    top = df_raw['beach'].value_counts().head(MAX_BEACHES).index.tolist()\n",
    "    df_raw = df_raw[df_raw['beach'].isin(top)].reset_index(drop=True)\n",
    "    print(f\"Limited to {MAX_BEACHES} beaches: {len(df_raw)} rows\")\n",
    "\n",
    "print(f\"Final: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea38de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 21962 rows, 3 beaches\n",
      "Features: 35\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "WEATHER_COLS = [c for c in df.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_COLS = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night']\n",
    "ALL_FEATURES = WEATHER_COLS + TEMPORAL_COLS\n",
    "\n",
    "df = df.dropna(subset=ALL_FEATURES + ['count']).reset_index(drop=True)\n",
    "good = df.groupby('beach')['count'].max()\n",
    "good = good[good > 20].index.tolist()\n",
    "df = df[df['beach'].isin(good)].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {len(df)} rows, {len(good)} beaches\")\n",
    "print(f\"Features: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7f4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Daytime:\n",
      "  Total rows:     11807\n",
      "  Beaches:        3\n",
      "  Night rows:     0 (0.0%)\n",
      "  Day rows:       11807 (100.0%)\n",
      "  Count - mean:   76.7\n",
      "  Count - median: 32.0\n",
      "  Count - std:    86.2\n",
      "  Count - min:    4.4\n",
      "  Count - max:    538.0\n",
      "  Day mean:       76.7\n",
      "  Zeros:          0 (0.0%)\n",
      "\n",
      "Full24h:\n",
      "  Total rows:     21962\n",
      "  Beaches:        3\n",
      "  Night rows:     10155 (46.2%)\n",
      "  Day rows:       11807 (53.8%)\n",
      "  Count - mean:   74.9\n",
      "  Count - median: 27.7\n",
      "  Count - std:    93.9\n",
      "  Count - min:    4.4\n",
      "  Count - max:    538.0\n",
      "  Day mean:       76.7\n",
      "  Night mean:     72.9\n",
      "  Zeros:          0 (0.0%)\n",
      "\n",
      "Night0:\n",
      "  Total rows:     21962\n",
      "  Beaches:        3\n",
      "  Night rows:     10155 (46.2%)\n",
      "  Day rows:       11807 (53.8%)\n",
      "  Count - mean:   41.2\n",
      "  Count - median: 16.3\n",
      "  Count - std:    73.9\n",
      "  Count - min:    0.0\n",
      "  Count - max:    538.0\n",
      "  Day mean:       76.7\n",
      "  Night mean:     0.0\n",
      "  Zeros:          10155 (46.2%)\n"
     ]
    }
   ],
   "source": [
    "ds_daytime = df[df['is_night'] == 0].copy().reset_index(drop=True)\n",
    "ds_full24h = df.copy()\n",
    "ds_night0 = df.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "datasets = {'Daytime': ds_daytime, 'Full24h': ds_full24h, 'Night0': ds_night0}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, d in datasets.items():\n",
    "    night_rows = d[d['is_night'] == 1] if 'is_night' in d.columns else pd.DataFrame()\n",
    "    day_rows = d[d['is_night'] == 0] if 'is_night' in d.columns else d\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total rows:     {len(d)}\")\n",
    "    print(f\"  Beaches:        {d['beach'].nunique()}\")\n",
    "    print(f\"  Night rows:     {len(night_rows)} ({len(night_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Day rows:       {len(day_rows)} ({len(day_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Count - mean:   {d['count'].mean():.1f}\")\n",
    "    print(f\"  Count - median: {d['count'].median():.1f}\")\n",
    "    print(f\"  Count - std:    {d['count'].std():.1f}\")\n",
    "    print(f\"  Count - min:    {d['count'].min():.1f}\")\n",
    "    print(f\"  Count - max:    {d['count'].max():.1f}\")\n",
    "    if len(day_rows) > 0:\n",
    "        print(f\"  Day mean:       {day_rows['count'].mean():.1f}\")\n",
    "    if len(night_rows) > 0:\n",
    "        print(f\"  Night mean:     {night_rows['count'].mean():.1f}\")\n",
    "    print(f\"  Zeros:          {(d['count'] == 0).sum()} ({(d['count'] == 0).sum()/len(d)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d8854ab-64c8-419e-aca2-1c9b09dc9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       beach                 beach_folder            datetime  \\\n",
      "0  Platja dor (Can Pastilla)        HeliosHotel/frontline 2022-07-14 11:00:00   \n",
      "1             Badia dAlcúdia  Monnaber/webcam-alcudia000M 2022-07-14 11:00:00   \n",
      "2  Platja dor (Can Pastilla)      HeliosHotel/frontline-2 2022-07-14 11:00:00   \n",
      "3             Badia dAlcúdia     skyline/mallorca-alcudia 2022-07-14 11:00:00   \n",
      "4             Port de Soller   SeeTheWorld/port_de_soller 2022-07-14 11:00:00   \n",
      "\n",
      "        count    ae_ta    ae_hr  ae_prec   ae_vv     ae_dv    ae_pres  ...  \\\n",
      "0   82.135231  30.0154  56.8880      0.0  4.4184  218.8209  1018.7906  ...   \n",
      "1   33.591919  31.2604  50.3663      0.0  2.0734   61.8588  1015.1991  ...   \n",
      "2   43.394043  30.0154  56.8880      0.0  4.4184  218.8209  1018.7906  ...   \n",
      "3   19.267483  31.7492  47.8123      0.0  2.1776   68.3430  1015.1705  ...   \n",
      "4  232.566895  28.6074  65.7055      0.0  2.5767   26.7109  1015.0849  ...   \n",
      "\n",
      "   om_sunshine_duration  om_vapour_pressure_deficit  om_direct_radiation  \\\n",
      "0                3600.0                      2.8367             625.0000   \n",
      "1                3600.0                      3.6667             626.0000   \n",
      "2                3600.0                      2.8367             625.0000   \n",
      "3                3600.0                      3.6467             626.0000   \n",
      "4                3600.0                      3.3867             631.6667   \n",
      "\n",
      "   om_shortwave_radiation  hour  day_of_week  month  is_weekend  is_summer  \\\n",
      "0                754.6667    11            3      7           0          1   \n",
      "1                757.0000    11            3      7           0          1   \n",
      "2                754.6667    11            3      7           0          1   \n",
      "3                757.0000    11            3      7           0          1   \n",
      "4                756.6667    11            3      7           0          1   \n",
      "\n",
      "   is_night  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ds_night0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547b0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daytime: train=8264, val=1771, test=1772\n",
      "Full24h: train=15373, val=3294, test=3295\n",
      "Night0: train=15373, val=3294, test=3295\n"
     ]
    }
   ],
   "source": [
    "def split_data(df, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(df)\n",
    "    t1 = int(n * train_frac)\n",
    "    t2 = int(n * (train_frac + val_frac))\n",
    "    return df.iloc[:t1], df.iloc[t1:t2], df.iloc[t2:]\n",
    "\n",
    "splits = {}\n",
    "for name, d in datasets.items():\n",
    "    train, val, test = split_data(d)\n",
    "    splits[name] = {'train': train, 'val': val, 'test': test}\n",
    "    print(f\"{name}: train={len(train)}, val={len(val)}, test={len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caed89c",
   "metadata": {},
   "source": [
    "## Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491de871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn models: ['Lasso', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']\n"
     ]
    }
   ],
   "source": [
    "def get_sklearn_models():\n",
    "    models = {\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),\n",
    "    }\n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = XGBRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbosity=0)\n",
    "    if HAS_LGBM:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    if HAS_CATBOOST:\n",
    "        models['CatBoost'] = CatBoostRegressor(n_estimators=200, max_depth=6, random_state=42, verbose=0)\n",
    "    return models\n",
    "\n",
    "print(f\"Sklearn models: {list(get_sklearn_models().keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8555298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SKLEARN - Daytime ===\n",
      "  Lasso                |   0.2s | MAE=73.4 | RelMAE=110.5%\n",
      "  RandomForest         |   1.0s | MAE=26.7 | RelMAE=37.2%\n",
      "  GradientBoosting     |   8.0s | MAE=30.9 | RelMAE=47.2%\n",
      "  XGBoost              |   0.4s | MAE=34.8 | RelMAE=51.5%\n",
      "  LightGBM             |   1.1s | MAE=25.1 | RelMAE=34.4%\n",
      "  CatBoost             |   0.4s | MAE=27.5 | RelMAE=38.3%\n",
      "\n",
      "=== SKLEARN - Full24h ===\n",
      "  Lasso                |   0.3s | MAE=45.0 | RelMAE=54.9%\n",
      "  RandomForest         |   1.7s | MAE=23.9 | RelMAE=27.0%\n",
      "  GradientBoosting     |  15.1s | MAE=31.4 | RelMAE=39.4%\n",
      "  XGBoost              |   0.5s | MAE=43.2 | RelMAE=54.5%\n",
      "  LightGBM             |   1.2s | MAE=24.8 | RelMAE=28.6%\n",
      "  CatBoost             |   0.4s | MAE=28.0 | RelMAE=32.3%\n",
      "\n",
      "=== SKLEARN - Night0 ===\n",
      "  Lasso                |   0.3s | MAE=49.0 | RelMAE=67.1%\n",
      "  RandomForest         |   1.6s | MAE=14.4 | RelMAE=19.8%\n",
      "  GradientBoosting     |  13.9s | MAE=20.1 | RelMAE=30.5%\n",
      "  XGBoost              |   0.5s | MAE=25.4 | RelMAE=38.8%\n",
      "  LightGBM             |   1.1s | MAE=14.4 | RelMAE=19.5%\n",
      "  CatBoost             |   0.4s | MAE=15.8 | RelMAE=21.7%\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    for ds_name in datasets.keys():\n",
    "        s = splits[ds_name]\n",
    "        X_train = pd.concat([s['train'], s['val']])[ALL_FEATURES]\n",
    "        y_train = pd.concat([s['train'], s['val']])['count']\n",
    "        X_test = s['test'][ALL_FEATURES]\n",
    "        y_test = s['test']['count']\n",
    "\n",
    "        print(f\"\\n=== SKLEARN - {ds_name} ===\")\n",
    "        \n",
    "        for model_name, model in get_sklearn_models().items():\n",
    "            t0 = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = np.clip(model.predict(X_test), 0, None)\n",
    "            elapsed = time.time() - t0\n",
    "\n",
    "            m = calc_metrics(y_test.values, y_pred, y_test.max())\n",
    "            beach_df = eval_per_beach(s['test'], y_pred, 'beach')\n",
    "            beach_df['model'] = model_name\n",
    "            beach_df['dataset'] = ds_name\n",
    "            all_beach_results.append(beach_df)\n",
    "\n",
    "            avg_rel = beach_df['RelMAE'].mean()\n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Dataset': ds_name, 'Type': 'Sklearn',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "            })\n",
    "            print(f\"  {model_name:20s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b896b",
   "metadata": {},
   "source": [
    "## NeuralForecast Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9701815c-3913-4ae8-94a8-0b68ddc5da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLForecast imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlforecast\", \"optuna\", \"window-ops\", \"-q\"])\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.auto import (\n",
    "    AutoMLForecast,\n",
    "    AutoLightGBM,\n",
    "    AutoXGBoost,\n",
    "    AutoCatboost,\n",
    "    AutoRidge,\n",
    ")\n",
    "\n",
    "print(\"MLForecast imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd991e49-0f1e-4277-8de4-f5741dcc0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING DATASETS WITH FILLED GAPS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing: Daytime\n",
      "======================================================================\n",
      "  Raw: train=10035, test=1772\n",
      "  After dedup: train=10035, test=1772\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 2106 rows, max_gap=23h, gaps>1h=191\n",
      "    HeliosHotel/frontline-2: 2122 rows, max_gap=23h, gaps>1h=181\n",
      "    Monnaber/webcam-alcudia000M: 2106 rows, max_gap=23h, gaps>1h=191\n",
      "\n",
      "  After fill_gaps: train=19835, test=3491\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3967 rows, max_gap=1h, gaps>1h=0, NaN=1861\n",
      "    HeliosHotel/frontline-2: 3967 rows, max_gap=1h, gaps>1h=0, NaN=1845\n",
      "    Monnaber/webcam-alcudia000M: 3967 rows, max_gap=1h, gaps>1h=0, NaN=1861\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=19835, test=3491, series=5\n",
      "\n",
      "======================================================================\n",
      "Processing: Full24h\n",
      "======================================================================\n",
      "  Raw: train=18667, test=3295\n",
      "  After dedup: train=18662, test=3295\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 3896 rows, max_gap=13h, gaps>1h=58\n",
      "    HeliosHotel/frontline-2: 3902 rows, max_gap=13h, gaps>1h=51\n",
      "    Monnaber/webcam-alcudia000M: 3863 rows, max_gap=13h, gaps>1h=93\n",
      "\n",
      "  After fill_gaps: train=19865, test=3481\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3973 rows, max_gap=1h, gaps>1h=0, NaN=77\n",
      "    HeliosHotel/frontline-2: 3973 rows, max_gap=1h, gaps>1h=0, NaN=71\n",
      "    Monnaber/webcam-alcudia000M: 3973 rows, max_gap=1h, gaps>1h=0, NaN=110\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=19865, test=3481, series=5\n",
      "\n",
      "======================================================================\n",
      "Processing: Night0\n",
      "======================================================================\n",
      "  Raw: train=18667, test=3295\n",
      "  After dedup: train=18662, test=3295\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 3896 rows, max_gap=13h, gaps>1h=58\n",
      "    HeliosHotel/frontline-2: 3902 rows, max_gap=13h, gaps>1h=51\n",
      "    Monnaber/webcam-alcudia000M: 3863 rows, max_gap=13h, gaps>1h=93\n",
      "\n",
      "  After fill_gaps: train=19865, test=3481\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3973 rows, max_gap=1h, gaps>1h=0, NaN=77\n",
      "    HeliosHotel/frontline-2: 3973 rows, max_gap=1h, gaps>1h=0, NaN=71\n",
      "    Monnaber/webcam-alcudia000M: 3973 rows, max_gap=1h, gaps>1h=0, NaN=110\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=19865, test=3481, series=5\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Daytime:\n",
      "  Train: 19835 rows\n",
      "  Test: 3491 rows\n",
      "  Series: 5\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n",
      "\n",
      "Full24h:\n",
      "  Train: 19865 rows\n",
      "  Test: 3481 rows\n",
      "  Series: 5\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n",
      "\n",
      "Night0:\n",
      "  Train: 19865 rows\n",
      "  Test: 3481 rows\n",
      "  Series: 5\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n"
     ]
    }
   ],
   "source": [
    "# === PREPARE DATASETS WITH FILLED GAPS ===\n",
    "# This cell creates clean, gap-filled datasets for all models (NeuralForecast, MLForecast, etc.)\n",
    "\n",
    "from utilsforecast.preprocessing import fill_gaps\n",
    "\n",
    "def to_nf_format(df, id_col='beach_folder'):\n",
    "    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    nf_df = df[cols].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def prepare_dataset_with_filled_gaps(train_df, test_df, freq='h'):\n",
    "    \"\"\"Prepare train/test with filled gaps and interpolation\"\"\"\n",
    "    \n",
    "    nf_train = to_nf_format(train_df)\n",
    "    nf_test = to_nf_format(test_df)\n",
    "    \n",
    "    print(f\"  Raw: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 1: Deduplicate (multiple images per hour)\n",
    "    nf_train = nf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    nf_test = nf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    print(f\"  After dedup: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 2: Check gaps before filling\n",
    "    print(f\"\\n  Gaps BEFORE fill_gaps:\")\n",
    "    for uid in nf_train['unique_id'].unique()[:3]:\n",
    "        s = nf_train[nf_train['unique_id'] == uid].sort_values('ds')\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        n_gaps_gt1 = (gaps > 1).sum()\n",
    "        max_gap = gaps.max()\n",
    "        print(f\"    {uid[:40]}: {len(s)} rows, max_gap={max_gap:.0f}h, gaps>1h={n_gaps_gt1}\")\n",
    "    \n",
    "    # Step 3: Fill gaps to create continuous hourly series\n",
    "    nf_train = fill_gaps(nf_train, freq=freq)\n",
    "    nf_test = fill_gaps(nf_test, freq=freq)\n",
    "    print(f\"\\n  After fill_gaps: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 4: Check gaps after filling\n",
    "    print(f\"\\n  Gaps AFTER fill_gaps:\")\n",
    "    for uid in nf_train['unique_id'].unique()[:3]:\n",
    "        s = nf_train[nf_train['unique_id'] == uid].sort_values('ds')\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        n_gaps_gt1 = (gaps > 1).sum()\n",
    "        max_gap = gaps.max()\n",
    "        nan_count = s['y'].isna().sum()\n",
    "        print(f\"    {uid[:40]}: {len(s)} rows, max_gap={max_gap:.0f}h, gaps>1h={n_gaps_gt1}, NaN={nan_count}\")\n",
    "    \n",
    "    # Step 5: Interpolate NaN values (linear interpolation + ffill/bfill for edges)\n",
    "    numeric_cols = nf_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c in nf_train.columns]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        nf_train[col] = nf_train.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "        nf_test[col] = nf_test.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n  After interpolation:\")\n",
    "    print(f\"    Train NaN: {nf_train['y'].isna().sum()}\")\n",
    "    print(f\"    Test NaN: {nf_test['y'].isna().sum()}\")\n",
    "    \n",
    "    # Step 6: Keep only series present in both train and test\n",
    "    common_ids = set(nf_train['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n",
    "    nf_train = nf_train[nf_train['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n  Final: train={len(nf_train)}, test={len(nf_test)}, series={len(common_ids)}\")\n",
    "    \n",
    "    return nf_train, nf_test, list(common_ids)\n",
    "\n",
    "# Process all datasets\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATASETS WITH FILLED GAPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prepared_data = {}\n",
    "\n",
    "for ds_name in ['Daytime', 'Full24h', 'Night0']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing: {ds_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    s = splits[ds_name]\n",
    "    train_val = pd.concat([s['train'], s['val']])\n",
    "    \n",
    "    nf_train, nf_test, series_ids = prepare_dataset_with_filled_gaps(train_val, s['test'])\n",
    "    \n",
    "    prepared_data[ds_name] = {\n",
    "        'train': nf_train,\n",
    "        'test': nf_test,\n",
    "        'series_ids': series_ids,\n",
    "        'n_series': len(series_ids),\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for ds_name, data in prepared_data.items():\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    print(f\"  Train: {len(data['train'])} rows\")\n",
    "    print(f\"  Test: {len(data['test'])} rows\")\n",
    "    print(f\"  Series: {data['n_series']}\")\n",
    "    print(f\"  Features: {[c for c in data['train'].columns if c not in ['unique_id', 'ds', 'y']][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90eb99b-3134-40d6-8335-67cf9b628416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NeuralForecast\n",
    "nf_train = prepared_data['Night0']['train']\n",
    "nf_test = prepared_data['Night0']['test']\n",
    "\n",
    "# For AutoMLForecast (only needs unique_id, ds, y)\n",
    "mlf_train = prepared_data['Night0']['train'][['unique_id', 'ds', 'y']]\n",
    "mlf_test = prepared_data['Night0']['test'][['unique_id', 'ds', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bf79e00-843b-4eb3-8e41-78a9c13e0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AutoMLForecast - Night0\n",
      "============================================================\n",
      "Train: 19865, Test: 3481, Series: 5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoCatboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mlf_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mlf_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Series: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlf_train[\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m auto_mlf = AutoMLForecast(\n\u001b[32m     21\u001b[39m     models={\n\u001b[32m     22\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAutoLightGBM\u001b[39m\u001b[33m'\u001b[39m: AutoLightGBM(),\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAutoCatboost\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mAutoCatboost\u001b[49m(),\n\u001b[32m     24\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAutoXGBoost\u001b[39m\u001b[33m'\u001b[39m: AutoXGBoost(),\n\u001b[32m     25\u001b[39m     },\n\u001b[32m     26\u001b[39m     freq=\u001b[33m'\u001b[39m\u001b[33mh\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m     season_length=\u001b[32m24\u001b[39m,\n\u001b[32m     28\u001b[39m     num_threads=-\u001b[32m1\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m t0 = time.time()\n\u001b[32m     32\u001b[39m auto_mlf.fit(\n\u001b[32m     33\u001b[39m     df=mlf_train,\n\u001b[32m     34\u001b[39m     n_windows=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     optimize_kwargs={\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m180\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mshow_progress_bar\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoCatboost' is not defined"
     ]
    }
   ],
   "source": [
    "def to_mlf_format(df, id_col='beach_folder'):\n",
    "    mlf_df = df[['datetime', id_col, 'count']].copy()\n",
    "    mlf_df = mlf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return mlf_df\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    for ds_name in ['Night0']:\n",
    "        s = splits[ds_name]\n",
    "        \n",
    "        train_val = pd.concat([s['train'], s['val']])\n",
    "        \n",
    "        mlf_train = mlf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "        mlf_test = mlf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"AutoMLForecast - {ds_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train: {len(mlf_train)}, Test: {len(mlf_test)}, Series: {mlf_train['unique_id'].nunique()}\")\n",
    "        \n",
    "        auto_mlf = AutoMLForecast(\n",
    "            models={\n",
    "                'AutoLightGBM': AutoLightGBM(),\n",
    "                'AutoCatboost': AutoCatboost(),\n",
    "                'AutoXGBoost': AutoXGBoost(),\n",
    "            },\n",
    "            freq='h',\n",
    "            season_length=24,\n",
    "            num_threads=-1,\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        auto_mlf.fit(\n",
    "            df=mlf_train,\n",
    "            n_windows=3,\n",
    "            h=72,\n",
    "            num_samples=15,\n",
    "            optimize_kwargs={'timeout': 180, 'show_progress_bar': False},\n",
    "        )\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"Training time: {elapsed:.1f}s\")\n",
    "        \n",
    "        preds = auto_mlf.predict(h=1)\n",
    "        print(f\"Predictions shape: {preds.shape}\")\n",
    "        print(f\"Predictions sample:\\n{preds.head()}\")\n",
    "        \n",
    "        merged = mlf_test.merge(preds, on=['unique_id', 'ds'], how='inner')\n",
    "        print(f\"Matched predictions: {len(merged)}/{len(mlf_test)}\")\n",
    "        \n",
    "        # Get actual model column names from predictions\n",
    "        model_cols = [c for c in preds.columns if c not in ['unique_id', 'ds']]\n",
    "        print(f\"Model columns: {model_cols}\")\n",
    "        \n",
    "        for model_name in model_cols:\n",
    "            y_true = merged['y'].values\n",
    "            y_pred = np.clip(merged[model_name].values, 0, None)\n",
    "            \n",
    "            m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "            \n",
    "            eval_df = merged[['unique_id', 'y']].copy()\n",
    "            eval_df['count'] = eval_df['y']\n",
    "            eval_df['beach'] = eval_df['unique_id']\n",
    "            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "            avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Dataset': ds_name, 'Type': 'AutoMLForecast',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed / len(model_cols)\n",
    "            })\n",
    "            \n",
    "            print(f\"  {model_name}: MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nBest models found:\")\n",
    "        for name, model in auto_mlf.models_.items():\n",
    "            print(f\"  {name}: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b1abb-ac2f-488a-82a5-98f840ba7701",
   "metadata": {},
   "source": [
    "\n",
    "Models with stat_exog_list: NBEATSx, NHITS, TFT, TiDE, BiTCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a626a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n",
    "    for ds_name in ['Full24h', 'Night0']:\n",
    "        if ds_name not in prepared_data:\n",
    "            print(f\"Skipping {ds_name} - not in prepared_data\")\n",
    "            continue\n",
    "        \n",
    "        nf_train = prepared_data[ds_name]['train']\n",
    "        nf_test = prepared_data[ds_name]['test']\n",
    "        n_series = prepared_data[ds_name]['n_series']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NF - {ds_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train: {len(nf_train)}, Test: {len(nf_test)}, Series: {n_series}\")\n",
    "        print(f\"NaN check - train: {nf_train['y'].isna().sum()}, test: {nf_test['y'].isna().sum()}\")\n",
    "        \n",
    "        # Store in splits for compatibility\n",
    "        splits[ds_name]['nf_train'] = nf_train\n",
    "        splits[ds_name]['nf_test'] = nf_test\n",
    "        splits[ds_name]['n_series'] = n_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_models(hist_exog):\n",
    "    common = dict(\n",
    "        h=72, input_size=INPUT_SIZE, max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=EARLY_STOP_PATIENCE,\n",
    "        learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE,\n",
    "        scaler_type='robust', random_seed=42,\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "    return [\n",
    "        ('NBEATSx', NBEATSx(hist_exog_list=hist_exog, stack_types=['identity','trend','seasonality'], **common)),\n",
    "        ('NHITS', NHITS(hist_exog_list=hist_exog, **common)),\n",
    "        ('TFT', TFT(hist_exog_list=hist_exog, hidden_size=64, **common)),\n",
    "        ('TiDE', TiDE(hist_exog_list=hist_exog, **common)),\n",
    "        ('BiTCN', BiTCN(hist_exog_list=hist_exog, **common)),\n",
    "    ]\n",
    "\n",
    "if HAS_NF:\n",
    "    print(f\"NF models: NBEATSx, NHITS, TFT, TiDE, BiTCN\")\n",
    "    print(f\"hist_exog: {len(ALL_FEATURES)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n",
    "    for ds_name in ['Full24h', 'Night0']:\n",
    "        if 'nf_train' not in splits[ds_name]:\n",
    "            continue\n",
    "            \n",
    "        nf_train = splits[ds_name]['nf_train']\n",
    "        nf_test = splits[ds_name]['nf_test']\n",
    "        \n",
    "        min_len = nf_train.groupby('unique_id').size().min()\n",
    "        val_size = max(24, min(min_len // 5, 168))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NF - {ds_name} (val_size={val_size})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for model_name, model in get_nf_models(ALL_FEATURES):\n",
    "            print(f\"\\n  {model_name}...\")\n",
    "            try:\n",
    "                t0 = time.time()\n",
    "                nf = NeuralForecast(models=[model], freq='h')\n",
    "                nf.fit(df=nf_train, val_size=val_size)\n",
    "                elapsed = time.time() - t0\n",
    "                \n",
    "                preds = nf.predict(df=nf_train).reset_index()\n",
    "                pred_col = [c for c in preds.columns if c not in ['unique_id','ds']][0]\n",
    "                \n",
    "                merged = nf_test.merge(preds[['unique_id','ds',pred_col]], on=['unique_id','ds'], how='inner')\n",
    "                print(f\"    Matched: {len(merged)}/{len(nf_test)}\")\n",
    "                \n",
    "                if len(merged) == 0:\n",
    "                    raise ValueError(\"No predictions matched\")\n",
    "                \n",
    "                y_true = merged['y'].values\n",
    "                y_pred = np.clip(merged[pred_col].values, 0, None)\n",
    "                \n",
    "                m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "                \n",
    "                eval_df = merged.copy()\n",
    "                eval_df['beach'] = eval_df['unique_id']\n",
    "                eval_df['count'] = eval_df['y']\n",
    "                beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "                beach_df['model'] = model_name\n",
    "                beach_df['dataset'] = ds_name\n",
    "                all_beach_results.append(beach_df)\n",
    "                \n",
    "                avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                    'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "                })\n",
    "                print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: {e}\")\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan,\n",
    "                    'AvgRelMAE': np.nan, 'Time': np.nan\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28328e27",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "beach_df = pd.concat(all_beach_results, ignore_index=True) if all_beach_results else pd.DataFrame()\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'results.csv', index=False)\n",
    "if len(beach_df) > 0:\n",
    "    beach_df.to_csv(save_dir / 'beach_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS BY DATASET\")\n",
    "print(\"=\"*70)\n",
    "for ds in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds].sort_values('AvgRelMAE')\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    print(f\"\\n{ds}:\")\n",
    "    print(sub[['Model','Type','MAE','R2','AvgRelMAE','Time']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ec410",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\n",
    "print(\"\\nRelMAE (%) by Model x Dataset:\")\n",
    "print(pivot.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37323f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # RelMAE bar chart\n",
    "    pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\n",
    "    pivot = pivot.loc[pivot.mean(axis=1).sort_values().index]\n",
    "    pivot.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "    axes[0].set_ylabel('Avg RelMAE (%)')\n",
    "    axes[0].set_title('Model Performance (lower is better)')\n",
    "    axes[0].legend(title='Dataset')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # R2 bar chart\n",
    "    pivot_r2 = results_df.pivot_table(index='Model', columns='Dataset', values='R2')\n",
    "    pivot_r2 = pivot_r2.loc[pivot_r2.mean(axis=1).sort_values(ascending=False).index]\n",
    "    pivot_r2.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "    axes[1].set_ylabel('R²')\n",
    "    axes[1].set_title('R² Score (higher is better)')\n",
    "    axes[1].legend(title='Dataset')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61622ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for ds in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds].dropna()\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best = sub.loc[sub['AvgRelMAE'].idxmin()]\n",
    "    print(f\"\\n{ds}: Best = {best['Model']} ({best['Type']}) - RelMAE={best['AvgRelMAE']:.1f}%, R2={best['R2']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
