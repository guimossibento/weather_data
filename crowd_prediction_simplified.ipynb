{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3001746d",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — 3-Dataset Comparison\n",
    "\n",
    "Compare model performance across three dataset strategies:\n",
    "1. **Daytime only** — remove night hours, sklearn models only\n",
    "2. **Full 24h** — keep all data including noisy night counts  \n",
    "3. **Night = 0** — keep 24h but replace night counts with 0\n",
    "\n",
    "Models with `stat_exog_list` (NBEATSx, NHITS, TFT, TiDE, BiTCN) can use beach metadata for zero-shot prediction on new beaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b840dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T14:11:35.472062Z",
     "start_time": "2026-02-08T14:11:35.469830Z"
    }
   },
   "outputs": [],
   "source": [
    "# === PATHS ===\n",
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/dataset_comparison\"\n",
    "\n",
    "# === SAMPLING (for quick testing) ===\n",
    "SAMPLE_FRAC = 1.0\n",
    "MAX_BEACHES = None\n",
    "\n",
    "# === MODEL PARAMETERS ===\n",
    "MAX_STEPS = 500\n",
    "EARLY_STOP_PATIENCE = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 24\n",
    "\n",
    "# === TIME ===\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6\n",
    "\n",
    "# === FLAGS ===\n",
    "RUN_SKLEARN = True\n",
    "RUN_NEURALFORECAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f569a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator: mps\n",
      "XGB: True, LGBM: True, CatBoost: True, NF: True\n"
     ]
    }
   ],
   "source": [
    "import json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except: HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except: HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except: HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import NBEATSx, NHITS, TFT, TiDE, BiTCN\n",
    "    HAS_NF = True\n",
    "except Exception as e:\n",
    "    print(f\"NeuralForecast error: {e}\")\n",
    "    HAS_NF = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    accelerator = 'mps'\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "\n",
    "print(f\"Accelerator: {accelerator}\")\n",
    "print(f\"XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, max_count):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_mae = (mae / max_count) * 100 if max_count > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE': rel_mae}\n",
    "\n",
    "def eval_per_beach(df, y_pred, beach_col='unique_id'):\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        y_true = df.loc[mask, 'y'].values if 'y' in df.columns else df.loc[mask, 'count'].values\n",
    "        y_p = y_pred[mask.values]\n",
    "        max_count = y_true.max()\n",
    "        m = calc_metrics(y_true, y_p, max_count)\n",
    "        m['camera'] = b\n",
    "        m['max_count'] = max_count\n",
    "        m['n'] = mask.sum()\n",
    "        results.append(m)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b1a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 169241 rows, 41 beaches\n",
      "Date range: 1970-01-01 01:00:00 to 2023-01-27 08:00:00\n"
     ]
    }
   ],
   "source": [
    "def load_cache(cache_dir, model):\n",
    "    cache_path = Path(cache_dir) / model\n",
    "    records = []\n",
    "    for jf in cache_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(jf) as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    \n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {\n",
    "            'beach': r.get('beach') or r.get('beach_folder'),\n",
    "            'beach_folder': r.get('beach_folder'),\n",
    "            'datetime': r.get('datetime'),\n",
    "            'count': r.get('count')\n",
    "        }\n",
    "        for k, v in r.get('weather', {}).items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, COUNTING_MODEL)\n",
    "print(f\"Loaded: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")\n",
    "print(f\"Date range: {df_raw['datetime'].min()} to {df_raw['datetime'].max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6787a76-170c-4b3b-b136-307c21518da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               beach    beach_folder  \\\n",
      "0                                        Camp de Mar  livecampro/070   \n",
      "1                                       Port Andratx  livecampro/001   \n",
      "2                                         Cala Major  livecampro/002   \n",
      "3  Son Serra Marina (des de El Sol Sunshine Bar &...  livecampro/035   \n",
      "4              Playa de Muro (Hotel Playa Esperanza)  livecampro/024   \n",
      "\n",
      "             datetime      count  ae_ta  ae_hr  ae_prec  ae_vv  ae_dv  \\\n",
      "0 1970-01-01 01:00:00  29.308895    NaN    NaN      NaN    NaN    NaN   \n",
      "1 1970-01-01 01:00:00  13.774030    NaN    NaN      NaN    NaN    NaN   \n",
      "2 1970-01-01 01:00:00  69.105835    NaN    NaN      NaN    NaN    NaN   \n",
      "3 1970-01-01 01:00:00  40.888012    NaN    NaN      NaN    NaN    NaN   \n",
      "4 1970-01-01 01:00:00  36.653721    NaN    NaN      NaN    NaN    NaN   \n",
      "\n",
      "   ae_pres  ...  om_wind_direction_10m  om_wind_gusts_10m  om_cloud_cover  \\\n",
      "0      NaN  ...               262.0000            42.7333         70.6667   \n",
      "1      NaN  ...               262.0000            42.7333         70.6667   \n",
      "2      NaN  ...               263.0000            30.1333         65.6667   \n",
      "3      NaN  ...               251.6667            20.7333         70.3333   \n",
      "4      NaN  ...               246.6667            25.7000         76.6667   \n",
      "\n",
      "   om_cloud_cover_low  om_cloud_cover_mid  om_cloud_cover_high  \\\n",
      "0             15.3333             36.6667              62.3333   \n",
      "1             15.3333             36.6667              62.3333   \n",
      "2             14.6667             32.6667              55.6667   \n",
      "3             19.0000             20.6667              62.3333   \n",
      "4             25.6667             21.6667              68.0000   \n",
      "\n",
      "   om_sunshine_duration  om_vapour_pressure_deficit  om_direct_radiation  \\\n",
      "0                   0.0                      0.3333                  0.0   \n",
      "1                   0.0                      0.3333                  0.0   \n",
      "2                   0.0                      0.2967                  0.0   \n",
      "3                   0.0                      0.1867                  0.0   \n",
      "4                   0.0                      0.1433                  0.0   \n",
      "\n",
      "   om_shortwave_radiation  \n",
      "0                     0.0  \n",
      "1                     0.0  \n",
      "2                     0.0  \n",
      "3                     0.0  \n",
      "4                     0.0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d20f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 169241 -> 70501\n"
     ]
    }
   ],
   "source": [
    "EXCLUDE = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols']\n",
    "EXCLUDE_PREFIX = ['ibred', 'ClubNauticSoller', 'Guenthoer', 'youtube']\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw['beach_folder'].isin(EXCLUDE)]\n",
    "for p in EXCLUDE_PREFIX:\n",
    "    df_raw = df_raw[~df_raw['beach_folder'].str.startswith(p, na=False)]\n",
    "print(f\"Filtered: {before} -> {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bffe88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 70501 rows, 14 beaches\n"
     ]
    }
   ],
   "source": [
    "if SAMPLE_FRAC < 1.0:\n",
    "    df_raw = df_raw.sample(frac=SAMPLE_FRAC, random_state=42).sort_values('datetime').reset_index(drop=True)\n",
    "    print(f\"Sampled to {len(df_raw)}\")\n",
    "\n",
    "if MAX_BEACHES:\n",
    "    top = df_raw['beach'].value_counts().head(MAX_BEACHES).index.tolist()\n",
    "    df_raw = df_raw[df_raw['beach'].isin(top)].reset_index(drop=True)\n",
    "    print(f\"Limited to {MAX_BEACHES} beaches: {len(df_raw)} rows\")\n",
    "\n",
    "print(f\"Final: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea38de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 70090 rows, 14 beaches\n",
      "Features: 35\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "WEATHER_COLS = [c for c in df.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_COLS = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night']\n",
    "ALL_FEATURES = WEATHER_COLS + TEMPORAL_COLS\n",
    "\n",
    "df = df.dropna(subset=ALL_FEATURES + ['count']).reset_index(drop=True)\n",
    "good = df.groupby('beach')['count'].max()\n",
    "good = good[good > 20].index.tolist()\n",
    "df = df[df['beach'].isin(good)].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {len(df)} rows, {len(good)} beaches\")\n",
    "print(f\"Features: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7f4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Daytime:\n",
      "  Total rows:     37643\n",
      "  Beaches:        14\n",
      "  Night rows:     0 (0.0%)\n",
      "  Day rows:       37643 (100.0%)\n",
      "  Count - mean:   57.2\n",
      "  Count - median: 28.9\n",
      "  Count - std:    69.0\n",
      "  Count - min:    3.9\n",
      "  Count - max:    538.0\n",
      "  Day mean:       57.2\n",
      "  Zeros:          0 (0.0%)\n",
      "\n",
      "Full24h:\n",
      "  Total rows:     70090\n",
      "  Beaches:        14\n",
      "  Night rows:     32447 (46.3%)\n",
      "  Day rows:       37643 (53.7%)\n",
      "  Count - mean:   53.8\n",
      "  Count - median: 26.9\n",
      "  Count - std:    72.7\n",
      "  Count - min:    3.9\n",
      "  Count - max:    538.0\n",
      "  Day mean:       57.2\n",
      "  Night mean:     49.7\n",
      "  Zeros:          0 (0.0%)\n",
      "\n",
      "Night0:\n",
      "  Total rows:     70090\n",
      "  Beaches:        14\n",
      "  Night rows:     32447 (46.3%)\n",
      "  Day rows:       37643 (53.7%)\n",
      "  Count - mean:   30.7\n",
      "  Count - median: 13.5\n",
      "  Count - std:    58.0\n",
      "  Count - min:    0.0\n",
      "  Count - max:    538.0\n",
      "  Day mean:       57.2\n",
      "  Night mean:     0.0\n",
      "  Zeros:          32447 (46.3%)\n"
     ]
    }
   ],
   "source": [
    "ds_daytime = df[df['is_night'] == 0].copy().reset_index(drop=True)\n",
    "ds_full24h = df.copy()\n",
    "ds_night0 = df.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "datasets = {'Daytime': ds_daytime, 'Full24h': ds_full24h, 'Night0': ds_night0}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, d in datasets.items():\n",
    "    night_rows = d[d['is_night'] == 1] if 'is_night' in d.columns else pd.DataFrame()\n",
    "    day_rows = d[d['is_night'] == 0] if 'is_night' in d.columns else d\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total rows:     {len(d)}\")\n",
    "    print(f\"  Beaches:        {d['beach'].nunique()}\")\n",
    "    print(f\"  Night rows:     {len(night_rows)} ({len(night_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Day rows:       {len(day_rows)} ({len(day_rows)/len(d)*100:.1f}%)\")\n",
    "    print(f\"  Count - mean:   {d['count'].mean():.1f}\")\n",
    "    print(f\"  Count - median: {d['count'].median():.1f}\")\n",
    "    print(f\"  Count - std:    {d['count'].std():.1f}\")\n",
    "    print(f\"  Count - min:    {d['count'].min():.1f}\")\n",
    "    print(f\"  Count - max:    {d['count'].max():.1f}\")\n",
    "    if len(day_rows) > 0:\n",
    "        print(f\"  Day mean:       {day_rows['count'].mean():.1f}\")\n",
    "    if len(night_rows) > 0:\n",
    "        print(f\"  Night mean:     {night_rows['count'].mean():.1f}\")\n",
    "    print(f\"  Zeros:          {(d['count'] == 0).sum()} ({(d['count'] == 0).sum()/len(d)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d8854ab-64c8-419e-aca2-1c9b09dc9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       beach                     beach_folder  \\\n",
      "0                    sant-antoni-de-portmany  skyline/sant-antoni-de-portmany   \n",
      "1  Platja d'or (Can Pastilla, desde BonaOna)                   livecampro/043   \n",
      "2                  Platja dor (Can Pastilla)            HeliosHotel/frontline   \n",
      "3                             Badia dAlcúdia      Monnaber/webcam-alcudia000M   \n",
      "4                               Cala Vedella       skyline/cala-vadella-ibiza   \n",
      "\n",
      "             datetime      count    ae_ta    ae_hr  ae_prec   ae_vv     ae_dv  \\\n",
      "0 2022-07-14 11:00:00  54.128101  32.5101  39.9054      0.0  3.2962  143.4848   \n",
      "1 2022-07-14 11:00:00  59.684418  30.0154  56.8880      0.0  4.4184  218.8209   \n",
      "2 2022-07-14 11:00:00  82.135231  30.0154  56.8880      0.0  4.4184  218.8209   \n",
      "3 2022-07-14 11:00:00  33.591919  31.2604  50.3663      0.0  2.0734   61.8588   \n",
      "4 2022-07-14 11:00:00  86.202827  31.5483  46.2622      0.0  3.1689  145.1612   \n",
      "\n",
      "     ae_pres  ...  om_sunshine_duration  om_vapour_pressure_deficit  \\\n",
      "0  1015.4355  ...                3600.0                      2.9933   \n",
      "1  1018.7906  ...                3600.0                      2.8367   \n",
      "2  1018.7906  ...                3600.0                      2.8367   \n",
      "3  1015.1991  ...                3600.0                      3.6667   \n",
      "4  1015.6711  ...                3600.0                      2.1167   \n",
      "\n",
      "   om_direct_radiation  om_shortwave_radiation  hour  day_of_week  month  \\\n",
      "0             610.3333                745.6667    11            3      7   \n",
      "1             625.0000                754.6667    11            3      7   \n",
      "2             625.0000                754.6667    11            3      7   \n",
      "3             626.0000                757.0000    11            3      7   \n",
      "4             607.6667                742.0000    11            3      7   \n",
      "\n",
      "   is_weekend  is_summer  is_night  \n",
      "0           0          1         0  \n",
      "1           0          1         0  \n",
      "2           0          1         0  \n",
      "3           0          1         0  \n",
      "4           0          1         0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ds_night0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547b0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daytime: train=26350, val=5646, test=5647\n",
      "Full24h: train=49063, val=10513, test=10514\n",
      "Night0: train=49063, val=10513, test=10514\n"
     ]
    }
   ],
   "source": [
    "def split_data(df, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(df)\n",
    "    t1 = int(n * train_frac)\n",
    "    t2 = int(n * (train_frac + val_frac))\n",
    "    return df.iloc[:t1], df.iloc[t1:t2], df.iloc[t2:]\n",
    "\n",
    "splits = {}\n",
    "for name, d in datasets.items():\n",
    "    train, val, test = split_data(d)\n",
    "    splits[name] = {'train': train, 'val': val, 'test': test}\n",
    "    print(f\"{name}: train={len(train)}, val={len(val)}, test={len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caed89c",
   "metadata": {},
   "source": [
    "## Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491de871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn models: ['Lasso', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']\n"
     ]
    }
   ],
   "source": [
    "def get_sklearn_models():\n",
    "    models = {\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),\n",
    "    }\n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = XGBRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbosity=0)\n",
    "    if HAS_LGBM:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    if HAS_CATBOOST:\n",
    "        models['CatBoost'] = CatBoostRegressor(n_estimators=200, max_depth=6, random_state=42, verbose=0)\n",
    "    return models\n",
    "\n",
    "print(f\"Sklearn models: {list(get_sklearn_models().keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8555298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SKLEARN - Daytime ===\n",
      "  Lasso                |   0.3s | MAE=73.8 | RelMAE=182.0%\n",
      "  RandomForest         |   3.3s | MAE=64.5 | RelMAE=166.0%\n",
      "  GradientBoosting     |  24.9s | MAE=50.2 | RelMAE=122.5%\n",
      "  XGBoost              |   0.5s | MAE=55.1 | RelMAE=130.1%\n",
      "  LightGBM             |   1.2s | MAE=30.7 | RelMAE=68.7%\n",
      "  CatBoost             |   0.5s | MAE=27.9 | RelMAE=57.7%\n",
      "\n",
      "=== SKLEARN - Full24h ===\n",
      "  Lasso                |   0.3s | MAE=47.3 | RelMAE=104.8%\n",
      "  RandomForest         |   6.1s | MAE=29.4 | RelMAE=60.3%\n",
      "  GradientBoosting     |  46.3s | MAE=42.7 | RelMAE=98.2%\n",
      "  XGBoost              |   0.6s | MAE=45.8 | RelMAE=105.2%\n",
      "  LightGBM             |   1.3s | MAE=30.7 | RelMAE=69.3%\n",
      "  CatBoost             |   0.6s | MAE=30.5 | RelMAE=67.2%\n",
      "\n",
      "=== SKLEARN - Night0 ===\n",
      "  Lasso                |   0.4s | MAE=40.2 | RelMAE=89.7%\n",
      "  RandomForest         |   3.6s | MAE=33.6 | RelMAE=85.3%\n",
      "  GradientBoosting     |  43.5s | MAE=50.2 | RelMAE=119.3%\n",
      "  XGBoost              |   0.6s | MAE=46.6 | RelMAE=113.9%\n",
      "  LightGBM             |   1.3s | MAE=21.3 | RelMAE=49.1%\n",
      "  CatBoost             |   0.7s | MAE=24.6 | RelMAE=55.3%\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    for ds_name in datasets.keys():\n",
    "        s = splits[ds_name]\n",
    "        X_train = pd.concat([s['train'], s['val']])[ALL_FEATURES]\n",
    "        y_train = pd.concat([s['train'], s['val']])['count']\n",
    "        X_test = s['test'][ALL_FEATURES]\n",
    "        y_test = s['test']['count']\n",
    "\n",
    "        print(f\"\\n=== SKLEARN - {ds_name} ===\")\n",
    "        \n",
    "        for model_name, model in get_sklearn_models().items():\n",
    "            t0 = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = np.clip(model.predict(X_test), 0, None)\n",
    "            elapsed = time.time() - t0\n",
    "\n",
    "            m = calc_metrics(y_test.values, y_pred, y_test.max())\n",
    "            beach_df = eval_per_beach(s['test'], y_pred, 'beach')\n",
    "            beach_df['model'] = model_name\n",
    "            beach_df['dataset'] = ds_name\n",
    "            all_beach_results.append(beach_df)\n",
    "\n",
    "            avg_rel = beach_df['RelMAE'].mean()\n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Dataset': ds_name, 'Type': 'Sklearn',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "            })\n",
    "            print(f\"  {model_name:20s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b896b",
   "metadata": {},
   "source": [
    "## NeuralForecast Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9701815c-3913-4ae8-94a8-0b68ddc5da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLForecast imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlforecast\", \"optuna\", \"window-ops\", \"-q\"])\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.auto import (\n",
    "    AutoMLForecast,\n",
    "    AutoLightGBM,\n",
    "    AutoXGBoost,\n",
    "    AutoCatboost,\n",
    "    AutoRidge,\n",
    ")\n",
    "\n",
    "print(\"MLForecast imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd991e49-0f1e-4277-8de4-f5741dcc0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING DATASETS WITH FILLED GAPS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing: Daytime\n",
      "======================================================================\n",
      "  Raw: train=31996, test=5647\n",
      "  After dedup: train=31996, test=5647\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 2109 rows, max_gap=23h, gaps>1h=193\n",
      "    HeliosHotel/frontline-2: 2125 rows, max_gap=23h, gaps>1h=182\n",
      "    Monnaber/webcam-alcudia000M: 2111 rows, max_gap=23h, gaps>1h=192\n",
      "\n",
      "  After fill_gaps: train=63728, test=10916\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3983 rows, max_gap=1h, gaps>1h=0, NaN=1874\n",
      "    HeliosHotel/frontline-2: 3983 rows, max_gap=1h, gaps>1h=0, NaN=1858\n",
      "    Monnaber/webcam-alcudia000M: 3983 rows, max_gap=1h, gaps>1h=0, NaN=1872\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=63728, test=10916, series=16\n",
      "\n",
      "======================================================================\n",
      "Processing: Full24h\n",
      "======================================================================\n",
      "  Raw: train=59576, test=10514\n",
      "  After dedup: train=59560, test=10514\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 3901 rows, max_gap=13h, gaps>1h=58\n",
      "    HeliosHotel/frontline-2: 3907 rows, max_gap=13h, gaps>1h=51\n",
      "    Monnaber/webcam-alcudia000M: 3868 rows, max_gap=13h, gaps>1h=93\n",
      "\n",
      "  After fill_gaps: train=63648, test=11056\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3978 rows, max_gap=1h, gaps>1h=0, NaN=77\n",
      "    HeliosHotel/frontline-2: 3978 rows, max_gap=1h, gaps>1h=0, NaN=71\n",
      "    Monnaber/webcam-alcudia000M: 3978 rows, max_gap=1h, gaps>1h=0, NaN=110\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=63648, test=11056, series=16\n",
      "\n",
      "======================================================================\n",
      "Processing: Night0\n",
      "======================================================================\n",
      "  Raw: train=59576, test=10514\n",
      "  After dedup: train=59560, test=10514\n",
      "\n",
      "  Gaps BEFORE fill_gaps:\n",
      "    HeliosHotel/frontline: 3901 rows, max_gap=13h, gaps>1h=58\n",
      "    HeliosHotel/frontline-2: 3907 rows, max_gap=13h, gaps>1h=51\n",
      "    Monnaber/webcam-alcudia000M: 3868 rows, max_gap=13h, gaps>1h=93\n",
      "\n",
      "  After fill_gaps: train=63648, test=11056\n",
      "\n",
      "  Gaps AFTER fill_gaps:\n",
      "    HeliosHotel/frontline: 3978 rows, max_gap=1h, gaps>1h=0, NaN=77\n",
      "    HeliosHotel/frontline-2: 3978 rows, max_gap=1h, gaps>1h=0, NaN=71\n",
      "    Monnaber/webcam-alcudia000M: 3978 rows, max_gap=1h, gaps>1h=0, NaN=110\n",
      "\n",
      "  After interpolation:\n",
      "    Train NaN: 0\n",
      "    Test NaN: 0\n",
      "\n",
      "  Final: train=63648, test=11056, series=16\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Daytime:\n",
      "  Train: 63728 rows\n",
      "  Test: 10916 rows\n",
      "  Series: 16\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n",
      "\n",
      "Full24h:\n",
      "  Train: 63648 rows\n",
      "  Test: 11056 rows\n",
      "  Series: 16\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n",
      "\n",
      "Night0:\n",
      "  Train: 63648 rows\n",
      "  Test: 11056 rows\n",
      "  Series: 16\n",
      "  Features: ['ae_ta', 'ae_hr', 'ae_prec', 'ae_vv', 'ae_dv']...\n"
     ]
    }
   ],
   "source": [
    "# === PREPARE DATASETS WITH FILLED GAPS ===\n",
    "# This cell creates clean, gap-filled datasets for all models (NeuralForecast, MLForecast, etc.)\n",
    "\n",
    "from utilsforecast.preprocessing import fill_gaps\n",
    "\n",
    "def to_nf_format(df, id_col='beach_folder'):\n",
    "    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    nf_df = df[cols].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def prepare_dataset_with_filled_gaps(train_df, test_df, freq='h'):\n",
    "    \"\"\"Prepare train/test with filled gaps and interpolation\"\"\"\n",
    "    \n",
    "    nf_train = to_nf_format(train_df)\n",
    "    nf_test = to_nf_format(test_df)\n",
    "    \n",
    "    print(f\"  Raw: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 1: Deduplicate (multiple images per hour)\n",
    "    nf_train = nf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    nf_test = nf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    print(f\"  After dedup: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 2: Check gaps before filling\n",
    "    print(f\"\\n  Gaps BEFORE fill_gaps:\")\n",
    "    for uid in nf_train['unique_id'].unique()[:3]:\n",
    "        s = nf_train[nf_train['unique_id'] == uid].sort_values('ds')\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        n_gaps_gt1 = (gaps > 1).sum()\n",
    "        max_gap = gaps.max()\n",
    "        print(f\"    {uid[:40]}: {len(s)} rows, max_gap={max_gap:.0f}h, gaps>1h={n_gaps_gt1}\")\n",
    "    \n",
    "    # Step 3: Fill gaps to create continuous hourly series\n",
    "    nf_train = fill_gaps(nf_train, freq=freq)\n",
    "    nf_test = fill_gaps(nf_test, freq=freq)\n",
    "    print(f\"\\n  After fill_gaps: train={len(nf_train)}, test={len(nf_test)}\")\n",
    "    \n",
    "    # Step 4: Check gaps after filling\n",
    "    print(f\"\\n  Gaps AFTER fill_gaps:\")\n",
    "    for uid in nf_train['unique_id'].unique()[:3]:\n",
    "        s = nf_train[nf_train['unique_id'] == uid].sort_values('ds')\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        n_gaps_gt1 = (gaps > 1).sum()\n",
    "        max_gap = gaps.max()\n",
    "        nan_count = s['y'].isna().sum()\n",
    "        print(f\"    {uid[:40]}: {len(s)} rows, max_gap={max_gap:.0f}h, gaps>1h={n_gaps_gt1}, NaN={nan_count}\")\n",
    "    \n",
    "    # Step 5: Interpolate NaN values (linear interpolation + ffill/bfill for edges)\n",
    "    numeric_cols = nf_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c in nf_train.columns]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        nf_train[col] = nf_train.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "        nf_test[col] = nf_test.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n  After interpolation:\")\n",
    "    print(f\"    Train NaN: {nf_train['y'].isna().sum()}\")\n",
    "    print(f\"    Test NaN: {nf_test['y'].isna().sum()}\")\n",
    "    \n",
    "    # Step 6: Keep only series present in both train and test\n",
    "    common_ids = set(nf_train['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n",
    "    nf_train = nf_train[nf_train['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common_ids)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n  Final: train={len(nf_train)}, test={len(nf_test)}, series={len(common_ids)}\")\n",
    "    \n",
    "    return nf_train, nf_test, list(common_ids)\n",
    "\n",
    "# Process all datasets\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATASETS WITH FILLED GAPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prepared_data = {}\n",
    "\n",
    "for ds_name in ['Daytime', 'Full24h', 'Night0']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing: {ds_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    s = splits[ds_name]\n",
    "    train_val = pd.concat([s['train'], s['val']])\n",
    "    \n",
    "    nf_train, nf_test, series_ids = prepare_dataset_with_filled_gaps(train_val, s['test'])\n",
    "    \n",
    "    prepared_data[ds_name] = {\n",
    "        'train': nf_train,\n",
    "        'test': nf_test,\n",
    "        'series_ids': series_ids,\n",
    "        'n_series': len(series_ids),\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for ds_name, data in prepared_data.items():\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    print(f\"  Train: {len(data['train'])} rows\")\n",
    "    print(f\"  Test: {len(data['test'])} rows\")\n",
    "    print(f\"  Series: {data['n_series']}\")\n",
    "    print(f\"  Features: {[c for c in data['train'].columns if c not in ['unique_id', 'ds', 'y']][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4232b78c-37bb-4c60-9c9f-a9983450620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SKLEARN - Daytime\n",
      "============================================================\n",
      "Train: 63728, Test: 10916, Features: 35\n",
      "  Lasso                |   0.4s | MAE=51.1 | RelMAE=124.9%\n",
      "  RandomForest         |   7.9s | MAE=32.4 | RelMAE=74.6%\n",
      "  GradientBoosting     |  60.4s | MAE=37.6 | RelMAE=93.7%\n",
      "  XGBoost              |   0.6s | MAE=29.2 | RelMAE=61.1%\n",
      "  LightGBM             |   1.3s | MAE=24.4 | RelMAE=52.9%\n",
      "  CatBoost             |   0.6s | MAE=34.3 | RelMAE=81.7%\n",
      "\n",
      "============================================================\n",
      "SKLEARN - Full24h\n",
      "============================================================\n",
      "Train: 63648, Test: 11056, Features: 35\n",
      "  Lasso                |   0.4s | MAE=44.9 | RelMAE=101.1%\n",
      "  RandomForest         |   6.2s | MAE=30.8 | RelMAE=65.8%\n",
      "  GradientBoosting     |  51.6s | MAE=46.2 | RelMAE=109.4%\n",
      "  XGBoost              |   0.6s | MAE=39.2 | RelMAE=90.3%\n",
      "  LightGBM             |   1.3s | MAE=31.1 | RelMAE=71.5%\n",
      "  CatBoost             |   0.7s | MAE=30.0 | RelMAE=67.0%\n",
      "\n",
      "============================================================\n",
      "SKLEARN - Night0\n",
      "============================================================\n",
      "Train: 63648, Test: 11056, Features: 35\n",
      "  Lasso                |   0.5s | MAE=36.0 | RelMAE=81.1%\n",
      "  RandomForest         |   5.5s | MAE=25.5 | RelMAE=61.4%\n",
      "  GradientBoosting     |  51.5s | MAE=58.4 | RelMAE=147.3%\n",
      "  XGBoost              |   0.6s | MAE=38.3 | RelMAE=92.9%\n",
      "  LightGBM             |   1.3s | MAE=20.7 | RelMAE=47.8%\n",
      "  CatBoost             |   0.7s | MAE=19.0 | RelMAE=43.2%\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    for ds_name in datasets.keys():\n",
    "        if ds_name not in prepared_data:\n",
    "            print(f\"Skipping {ds_name} - not in prepared_data\")\n",
    "            continue\n",
    "        \n",
    "        # Use prepared clean data\n",
    "        train_df = prepared_data[ds_name]['train']\n",
    "        test_df = prepared_data[ds_name]['test']\n",
    "        \n",
    "        # Get features (exclude unique_id, ds, y)\n",
    "        feature_cols = [c for c in train_df.columns if c not in ['unique_id', 'ds', 'y']]\n",
    "        \n",
    "        X_train = train_df[feature_cols]\n",
    "        y_train = train_df['y']\n",
    "        X_test = test_df[feature_cols]\n",
    "        y_test = test_df['y']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SKLEARN - {ds_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train: {len(X_train)}, Test: {len(X_test)}, Features: {len(feature_cols)}\")\n",
    "        \n",
    "        for model_name, model in get_sklearn_models().items():\n",
    "            t0 = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = np.clip(model.predict(X_test), 0, None)\n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            m = calc_metrics(y_test.values, y_pred, y_test.max())\n",
    "            \n",
    "            # Eval per beach (camera)\n",
    "            eval_df = test_df[['unique_id', 'y']].copy()\n",
    "            eval_df['count'] = eval_df['y']\n",
    "            eval_df['beach'] = eval_df['unique_id']\n",
    "            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "            beach_df['model'] = model_name\n",
    "            beach_df['dataset'] = ds_name\n",
    "            all_beach_results.append(beach_df)\n",
    "            \n",
    "            avg_rel = beach_df['RelMAE'].mean()\n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Dataset': ds_name, 'Type': 'Sklearn',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "            })\n",
    "            print(f\"  {model_name:20s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90eb99b-3134-40d6-8335-67cf9b628416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NeuralForecast\n",
    "nf_train = prepared_data['Night0']['train']\n",
    "nf_test = prepared_data['Night0']['test']\n",
    "\n",
    "# For AutoMLForecast (only needs unique_id, ds, y)\n",
    "mlf_train = prepared_data['Night0']['train'][['unique_id', 'ds', 'y']]\n",
    "mlf_test = prepared_data['Night0']['test'][['unique_id', 'ds', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bf79e00-843b-4eb3-8e41-78a9c13e0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-08 17:15:22,019]\u001b[0m A new study created in memory with name: no-name-124e82c3-fbe0-42f6-8c87-37d9b9576d8b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AutoMLForecast - Night0\n",
      "============================================================\n",
      "Train: 63648, Test: 11056, Series: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-08 17:15:26,098]\u001b[0m Trial 0 finished with value: 0.5054185619083243 and parameters: {'n_estimators': 169, 'lambda_l1': 0.02733406969031059, 'lambda_l2': 0.002659931083868188, 'num_leaves': 112, 'feature_fraction': 0.7118273996694524, 'bagging_fraction': 0.8229470565333281, 'objective': 'l2', 'target_transforms_idx': 0, 'lags_idx': 5, 'lag_transforms_idx': 1, 'use_date_features': 0}. Best is trial 0 with value: 0.5054185619083243.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:29,168]\u001b[0m Trial 1 finished with value: 0.9507998397129005 and parameters: {'n_estimators': 805, 'lambda_l1': 0.000497318821411805, 'lambda_l2': 5.3945777886906086e-05, 'num_leaves': 12, 'feature_fraction': 0.8871168447171083, 'bagging_fraction': 0.7280751661082743, 'objective': 'l1', 'target_transforms_idx': 3, 'lags_idx': 0, 'lag_transforms_idx': 3, 'use_date_features': 0}. Best is trial 0 with value: 0.5054185619083243.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:29,555]\u001b[0m Trial 2 finished with value: 0.5018635560508669 and parameters: {'n_estimators': 44, 'lambda_l1': 2.8299949527313245e-07, 'lambda_l2': 0.0075504863920363065, 'num_leaves': 11, 'feature_fraction': 0.7331553864281531, 'bagging_fraction': 0.6222127960008014, 'objective': 'l1', 'target_transforms_idx': 6, 'lags_idx': 3, 'lag_transforms_idx': 2, 'use_date_features': 0}. Best is trial 2 with value: 0.5018635560508669.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:32,787]\u001b[0m Trial 3 finished with value: 0.5113971292557388 and parameters: {'n_estimators': 100, 'lambda_l1': 3.778569956294474e-08, 'lambda_l2': 0.017070611332063324, 'num_leaves': 133, 'feature_fraction': 0.6326947454697227, 'bagging_fraction': 0.7616240267333498, 'objective': 'l2', 'target_transforms_idx': 0, 'lags_idx': 2, 'lag_transforms_idx': 0, 'use_date_features': 1}. Best is trial 2 with value: 0.5018635560508669.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:37,103]\u001b[0m Trial 4 finished with value: 0.5047883403456598 and parameters: {'n_estimators': 47, 'lambda_l1': 3.756134227775984, 'lambda_l2': 0.00010571300239371601, 'num_leaves': 1215, 'feature_fraction': 0.8497396376587522, 'bagging_fraction': 0.6487184754275668, 'objective': 'l1', 'target_transforms_idx': 6, 'lags_idx': 5, 'lag_transforms_idx': 0, 'use_date_features': 1}. Best is trial 2 with value: 0.5018635560508669.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:45,999]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:50,912]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:15:51,166]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:09,081]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:09,082]\u001b[0m Trial 9 finished with value: inf and parameters: {'n_estimators': 33, 'lambda_l1': 0.028296787326479292, 'lambda_l2': 3.668912254060719e-05, 'num_leaves': 132, 'feature_fraction': 0.5916399181070393, 'bagging_fraction': 0.5724238796716886, 'objective': 'l1', 'target_transforms_idx': 0, 'lags_idx': 0, 'lag_transforms_idx': 0, 'use_date_features': 0}. Best is trial 2 with value: 0.5018635560508669.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:09,203]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:09,671]\u001b[0m Trial 11 finished with value: 0.49751163631776607 and parameters: {'n_estimators': 38, 'lambda_l1': 9.838610270006994, 'lambda_l2': 0.49374243352828834, 'num_leaves': 19, 'feature_fraction': 0.8622226958172667, 'bagging_fraction': 0.6290402306024396, 'objective': 'l1', 'target_transforms_idx': 6, 'lags_idx': 5, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 11 with value: 0.49751163631776607.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:10,020]\u001b[0m Trial 12 finished with value: 0.5014928168802607 and parameters: {'n_estimators': 20, 'lambda_l1': 1.1540269514377878e-06, 'lambda_l2': 1.5439685966245928, 'num_leaves': 17, 'feature_fraction': 0.9252401123298061, 'bagging_fraction': 0.6004362045554316, 'objective': 'l1', 'target_transforms_idx': 6, 'lags_idx': 4, 'lag_transforms_idx': 2, 'use_date_features': 0}. Best is trial 11 with value: 0.49751163631776607.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:10,454]\u001b[0m Trial 13 finished with value: 0.5001675492487445 and parameters: {'n_estimators': 22, 'lambda_l1': 1.8091681257440395e-06, 'lambda_l2': 6.205258418870362, 'num_leaves': 27, 'feature_fraction': 0.9940832060967414, 'bagging_fraction': 0.5500844282173721, 'objective': 'l1', 'target_transforms_idx': 5, 'lags_idx': 4, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 11 with value: 0.49751163631776607.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:11,004]\u001b[0m Trial 14 finished with value: 0.5006520740099877 and parameters: {'n_estimators': 30, 'lambda_l1': 5.373263810426762e-06, 'lambda_l2': 0.23583119705486794, 'num_leaves': 35, 'feature_fraction': 0.9983795782625572, 'bagging_fraction': 0.5254341444076155, 'objective': 'l1', 'target_transforms_idx': 5, 'lags_idx': 4, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 11 with value: 0.49751163631776607.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:11,209]\u001b[0m A new study created in memory with name: no-name-5a11894b-81d0-4389-b6ce-d58885b9b731\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:15,997]\u001b[0m Trial 0 finished with value: 0.49354779524029335 and parameters: {'n_estimators': 979, 'depth': 9, 'learning_rate': 0.005999483889855459, 'subsample': 0.9654131390873487, 'colsample_bylevel': 0.3085314638240841, 'min_data_in_leaf': 94.98256341915246, 'target_transforms_idx': 0, 'lags_idx': 3, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:16,642]\u001b[0m Trial 1 finished with value: 0.4975738374676319 and parameters: {'n_estimators': 263, 'depth': 1, 'learning_rate': 0.09636704535868337, 'subsample': 0.9756275401208173, 'colsample_bylevel': 0.9647511922567001, 'min_data_in_leaf': 90.74899442289671, 'target_transforms_idx': 0, 'lags_idx': 2, 'lag_transforms_idx': 3, 'use_date_features': 1}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:17,146]\u001b[0m Trial 2 finished with value: 0.49600874985756316 and parameters: {'n_estimators': 307, 'depth': 2, 'learning_rate': 0.001340966635327829, 'subsample': 0.3714387710328482, 'colsample_bylevel': 0.33590633431571043, 'min_data_in_leaf': 46.15791611324749, 'target_transforms_idx': 5, 'lags_idx': 1, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:17,649]\u001b[0m Trial 3 finished with value: 0.9406944754731549 and parameters: {'n_estimators': 123, 'depth': 6, 'learning_rate': 0.0050813515464354884, 'subsample': 0.6197886539482379, 'colsample_bylevel': 0.9634900067500827, 'min_data_in_leaf': 64.91145420114438, 'target_transforms_idx': 4, 'lags_idx': 1, 'lag_transforms_idx': 3, 'use_date_features': 1}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:19,232]\u001b[0m Trial 4 finished with value: 0.5048475592099151 and parameters: {'n_estimators': 715, 'depth': 4, 'learning_rate': 0.055127761372937775, 'subsample': 0.6724549499024272, 'colsample_bylevel': 0.3160182460417386, 'min_data_in_leaf': 16.893343426040385, 'target_transforms_idx': 1, 'lags_idx': 2, 'lag_transforms_idx': 3, 'use_date_features': 0}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:19,940]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:22,881]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:24,446]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:28,009]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:28,528]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:29,518]\u001b[0m Trial 10 finished with value: 0.4948178235735731 and parameters: {'n_estimators': 789, 'depth': 8, 'learning_rate': 0.0018790839150855374, 'subsample': 0.7753273192225394, 'colsample_bylevel': 0.12569941955166913, 'min_data_in_leaf': 98.03874734074742, 'target_transforms_idx': 6, 'lags_idx': 3, 'lag_transforms_idx': 1, 'use_date_features': 0}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:30,363]\u001b[0m Trial 11 finished with value: 0.4946882835678524 and parameters: {'n_estimators': 759, 'depth': 8, 'learning_rate': 0.0014542009394702612, 'subsample': 0.7587605518327948, 'colsample_bylevel': 0.10253340232295381, 'min_data_in_leaf': 99.20188896182259, 'target_transforms_idx': 6, 'lags_idx': 3, 'lag_transforms_idx': 1, 'use_date_features': 0}. Best is trial 0 with value: 0.49354779524029335.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:31,134]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:32,008]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:33,831]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:38,432]\u001b[0m A new study created in memory with name: no-name-e5dd1712-9faf-44e7-aa52-331a3e311895\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:39,272]\u001b[0m Trial 0 finished with value: 0.9740482718830102 and parameters: {'n_estimators': 553, 'max_depth': 3, 'learning_rate': 0.0011737408929499537, 'subsample': 0.739303146076792, 'colsample_bytree': 0.10709569315759634, 'reg_lambda': 9.581479143120674e-06, 'reg_alpha': 0.00017550832299575772, 'min_child_weight': 10, 'target_transforms_idx': 4, 'lags_idx': 4, 'lag_transforms_idx': 2, 'use_date_features': 1}. Best is trial 0 with value: 0.9740482718830102.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:39,273]\u001b[0m Trial 1 finished with value: inf and parameters: {'n_estimators': 818, 'max_depth': 6, 'learning_rate': 0.005867636242929072, 'subsample': 0.8117561469046645, 'colsample_bytree': 0.18751863306918218, 'reg_lambda': 3.437835578589196e-05, 'reg_alpha': 0.00014441722528782652, 'min_child_weight': 8, 'target_transforms_idx': 4, 'lags_idx': 0, 'lag_transforms_idx': 0, 'use_date_features': 0}. Best is trial 0 with value: 0.9740482718830102.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:40,867]\u001b[0m Trial 2 finished with value: 0.672501046491691 and parameters: {'n_estimators': 954, 'max_depth': 4, 'learning_rate': 0.002340134729399809, 'subsample': 0.897674199477904, 'colsample_bytree': 0.5017549739348827, 'reg_lambda': 0.18323344834129368, 'reg_alpha': 1.913567254316368e-07, 'min_child_weight': 7, 'target_transforms_idx': 2, 'lags_idx': 6, 'lag_transforms_idx': 3, 'use_date_features': 0}. Best is trial 2 with value: 0.672501046491691.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:42,037]\u001b[0m Trial 3 finished with value: 0.6960107480551146 and parameters: {'n_estimators': 341, 'max_depth': 7, 'learning_rate': 0.001397792242064932, 'subsample': 0.6465244366103869, 'colsample_bytree': 0.5298818525887745, 'reg_lambda': 1.8706813396056685e-06, 'reg_alpha': 8.078044613988488e-07, 'min_child_weight': 6, 'target_transforms_idx': 3, 'lags_idx': 3, 'lag_transforms_idx': 1, 'use_date_features': 1}. Best is trial 2 with value: 0.672501046491691.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:42,639]\u001b[0m Trial 4 finished with value: 0.49489843810208073 and parameters: {'n_estimators': 349, 'max_depth': 2, 'learning_rate': 0.001393565293753485, 'subsample': 0.3177115337813364, 'colsample_bytree': 0.4890533330631688, 'reg_lambda': 0.0001499581897260337, 'reg_alpha': 0.015299189798878633, 'min_child_weight': 10, 'target_transforms_idx': 6, 'lags_idx': 5, 'lag_transforms_idx': 1, 'use_date_features': 1}. Best is trial 4 with value: 0.49489843810208073.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:42,984]\u001b[0m Trial 5 finished with value: 0.4947601543719537 and parameters: {'n_estimators': 77, 'max_depth': 5, 'learning_rate': 0.0018473003457162686, 'subsample': 0.5113528851982306, 'colsample_bytree': 0.9819660937080784, 'reg_lambda': 2.4527361251856927e-05, 'reg_alpha': 0.0719447884714015, 'min_child_weight': 3, 'target_transforms_idx': 5, 'lags_idx': 4, 'lag_transforms_idx': 0, 'use_date_features': 0}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:43,539]\u001b[0m Trial 6 finished with value: 0.6226123174324024 and parameters: {'n_estimators': 49, 'max_depth': 8, 'learning_rate': 0.0017890287963389761, 'subsample': 0.6456773197405766, 'colsample_bytree': 0.7328957468204942, 'reg_lambda': 0.0011975415301588928, 'reg_alpha': 0.4711270235636093, 'min_child_weight': 2, 'target_transforms_idx': 0, 'lags_idx': 2, 'lag_transforms_idx': 2, 'use_date_features': 0}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:44,565]\u001b[0m Trial 7 finished with value: 0.5781432869664668 and parameters: {'n_estimators': 860, 'max_depth': 2, 'learning_rate': 0.019401977823268017, 'subsample': 0.7961099900041275, 'colsample_bytree': 0.51076860880516, 'reg_lambda': 1.6863628367848146e-07, 'reg_alpha': 3.951565062229025e-07, 'min_child_weight': 5, 'target_transforms_idx': 4, 'lags_idx': 3, 'lag_transforms_idx': 3, 'use_date_features': 1}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:45,133]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:46,232]\u001b[0m Trial 9 finished with value: 0.5166088140652221 and parameters: {'n_estimators': 349, 'max_depth': 9, 'learning_rate': 0.14946343546935884, 'subsample': 0.9927012962391665, 'colsample_bytree': 0.4390671402648881, 'reg_lambda': 0.5360182358032055, 'reg_alpha': 0.02162900579760745, 'min_child_weight': 8, 'target_transforms_idx': 3, 'lags_idx': 1, 'lag_transforms_idx': 3, 'use_date_features': 0}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:46,583]\u001b[0m Trial 10 finished with value: 0.4965941298115486 and parameters: {'n_estimators': 99, 'max_depth': 4, 'learning_rate': 0.023650613412169553, 'subsample': 0.46705446022761005, 'colsample_bytree': 0.9864295287023385, 'reg_lambda': 0.004709883049648374, 'reg_alpha': 0.502703472000772, 'min_child_weight': 4, 'target_transforms_idx': 5, 'lags_idx': 4, 'lag_transforms_idx': 0, 'use_date_features': 0}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:47,141]\u001b[0m Trial 11 finished with value: 0.49495805615591143 and parameters: {'n_estimators': 495, 'max_depth': 1, 'learning_rate': 0.0057978293194515095, 'subsample': 0.3968493070058663, 'colsample_bytree': 0.7700489949990389, 'reg_lambda': 0.0005024978711812554, 'reg_alpha': 0.006334927381367325, 'min_child_weight': 10, 'target_transforms_idx': 6, 'lags_idx': 5, 'lag_transforms_idx': 1, 'use_date_features': 1}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:47,633]\u001b[0m Trial 12 finished with value: 0.5319360679094512 and parameters: {'n_estimators': 183, 'max_depth': 4, 'learning_rate': 0.005452423893868923, 'subsample': 0.1273075837369539, 'colsample_bytree': 0.32518624386590667, 'reg_lambda': 6.656285339512301e-08, 'reg_alpha': 0.007799621235685501, 'min_child_weight': 4, 'target_transforms_idx': 1, 'lags_idx': 5, 'lag_transforms_idx': 1, 'use_date_features': 1}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:48,219]\u001b[0m Trial 13 finished with value: 0.4975997919537289 and parameters: {'n_estimators': 566, 'max_depth': 1, 'learning_rate': 0.04515520784981497, 'subsample': 0.34843819318113156, 'colsample_bytree': 0.7008023447620036, 'reg_lambda': 1.5004897557700426e-06, 'reg_alpha': 0.04342953461111199, 'min_child_weight': 2, 'target_transforms_idx': 6, 'lags_idx': 5, 'lag_transforms_idx': 0, 'use_date_features': 1}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:16:49,213]\u001b[0m Trial 14 finished with value: 0.4987476320835335 and parameters: {'n_estimators': 225, 'max_depth': 10, 'learning_rate': 0.0033276167903148876, 'subsample': 0.5250346701049298, 'colsample_bytree': 0.9863966779089791, 'reg_lambda': 0.011112015744008216, 'reg_alpha': 0.0008024319301222848, 'min_child_weight': 9, 'target_transforms_idx': 5, 'lags_idx': 4, 'lag_transforms_idx': 1, 'use_date_features': 0}. Best is trial 5 with value: 0.4947601543719537.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 87.3s\n",
      "Predictions shape: (16, 5)\n",
      "Predictions sample:\n",
      "                       unique_id                  ds  AutoLightGBM  \\\n",
      "0          HeliosHotel/frontline 2022-12-27 05:00:00 -3.936101e-09   \n",
      "1        HeliosHotel/frontline-2 2022-12-27 05:00:00  1.205581e-07   \n",
      "2    Monnaber/webcam-alcudia000M 2022-12-27 05:00:00 -1.156302e-07   \n",
      "3  SeeTheWorld/pollenca_panorama 2022-12-27 05:00:00  2.234863e-14   \n",
      "4     SeeTheWorld/port_de_soller 2022-12-27 05:00:00  1.018613e-10   \n",
      "\n",
      "   AutoCatboost  AutoXGBoost  \n",
      "0      0.320934    -0.002657  \n",
      "1      0.206547    -0.003624  \n",
      "2      0.355483    -0.000797  \n",
      "3      0.075555    -0.003503  \n",
      "4      0.103132    -0.004309  \n",
      "Matched predictions: 15/11056\n",
      "Model columns: ['AutoLightGBM', 'AutoCatboost', 'AutoXGBoost']\n",
      "  AutoLightGBM: MAE=0.0 | RelMAE=nan% | R2=0.000\n",
      "  AutoCatboost: MAE=0.3 | RelMAE=nan% | R2=0.000\n",
      "  AutoXGBoost: MAE=0.0 | RelMAE=nan% | R2=0.000\n",
      "\n",
      "Best models found:\n",
      "  AutoLightGBM: MLForecast\n",
      "  AutoCatboost: MLForecast\n",
      "  AutoXGBoost: MLForecast\n"
     ]
    }
   ],
   "source": [
    "def to_mlf_format(df, id_col='beach_folder'):\n",
    "    mlf_df = df[['datetime', id_col, 'count']].copy()\n",
    "    mlf_df = mlf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return mlf_df\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    for ds_name in ['Night0']:\n",
    "        s = splits[ds_name]\n",
    "        \n",
    "        train_val = pd.concat([s['train'], s['val']])\n",
    "        \n",
    "        mlf_train = mlf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "        mlf_test = mlf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"AutoMLForecast - {ds_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train: {len(mlf_train)}, Test: {len(mlf_test)}, Series: {mlf_train['unique_id'].nunique()}\")\n",
    "        \n",
    "        auto_mlf = AutoMLForecast(\n",
    "            models={\n",
    "                'AutoLightGBM': AutoLightGBM(),\n",
    "                'AutoCatboost': AutoCatboost(),\n",
    "                'AutoXGBoost': AutoXGBoost(),\n",
    "            },\n",
    "            freq='h',\n",
    "            season_length=24,\n",
    "            num_threads=-1,\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        auto_mlf.fit(\n",
    "            df=mlf_train,\n",
    "            n_windows=3,\n",
    "            h=72,\n",
    "            num_samples=15,\n",
    "            optimize_kwargs={'timeout': 180, 'show_progress_bar': False},\n",
    "        )\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"Training time: {elapsed:.1f}s\")\n",
    "        \n",
    "        preds = auto_mlf.predict(h=1)\n",
    "        print(f\"Predictions shape: {preds.shape}\")\n",
    "        print(f\"Predictions sample:\\n{preds.head()}\")\n",
    "        \n",
    "        merged = mlf_test.merge(preds, on=['unique_id', 'ds'], how='inner')\n",
    "        print(f\"Matched predictions: {len(merged)}/{len(mlf_test)}\")\n",
    "        \n",
    "        # Get actual model column names from predictions\n",
    "        model_cols = [c for c in preds.columns if c not in ['unique_id', 'ds']]\n",
    "        print(f\"Model columns: {model_cols}\")\n",
    "        \n",
    "        for model_name in model_cols:\n",
    "            y_true = merged['y'].values\n",
    "            y_pred = np.clip(merged[model_name].values, 0, None)\n",
    "            \n",
    "            m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "            \n",
    "            eval_df = merged[['unique_id', 'y']].copy()\n",
    "            eval_df['count'] = eval_df['y']\n",
    "            eval_df['beach'] = eval_df['unique_id']\n",
    "            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "            avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Dataset': ds_name, 'Type': 'AutoMLForecast',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed / len(model_cols)\n",
    "            })\n",
    "            \n",
    "            print(f\"  {model_name}: MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nBest models found:\")\n",
    "        for name, model in auto_mlf.models_.items():\n",
    "            print(f\"  {name}: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b1abb-ac2f-488a-82a5-98f840ba7701",
   "metadata": {},
   "source": [
    "\n",
    "Models with stat_exog_list: NBEATSx, NHITS, TFT, TiDE, BiTCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a626a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NF - Full24h\n",
      "============================================================\n",
      "Train: 63648, Test: 11056, Series: 16\n",
      "NaN check - train: 0, test: 0\n",
      "\n",
      "============================================================\n",
      "NF - Night0\n",
      "============================================================\n",
      "Train: 63648, Test: 11056, Series: 16\n",
      "NaN check - train: 0, test: 0\n"
     ]
    }
   ],
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n",
    "    for ds_name in ['Full24h', 'Night0']:\n",
    "        if ds_name not in prepared_data:\n",
    "            print(f\"Skipping {ds_name} - not in prepared_data\")\n",
    "            continue\n",
    "        \n",
    "        nf_train = prepared_data[ds_name]['train']\n",
    "        nf_test = prepared_data[ds_name]['test']\n",
    "        n_series = prepared_data[ds_name]['n_series']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NF - {ds_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train: {len(nf_train)}, Test: {len(nf_test)}, Series: {n_series}\")\n",
    "        print(f\"NaN check - train: {nf_train['y'].isna().sum()}, test: {nf_test['y'].isna().sum()}\")\n",
    "        \n",
    "        # Store in splits for compatibility\n",
    "        splits[ds_name]['nf_train'] = nf_train\n",
    "        splits[ds_name]['nf_test'] = nf_test\n",
    "        splits[ds_name]['n_series'] = n_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da0b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF models: NBEATSx, NHITS, TFT, TiDE, BiTCN\n",
      "hist_exog: 35 features\n"
     ]
    }
   ],
   "source": [
    "def get_nf_models(hist_exog, horizon):\n",
    "    # Detect device properly\n",
    "    if torch.cuda.is_available():\n",
    "        accelerator = 'gpu'\n",
    "        devices = 1  # Use single GPU to avoid DDP issues\n",
    "    else:\n",
    "        accelerator = 'cpu'\n",
    "        devices = 1\n",
    "    \n",
    "    common = dict(\n",
    "        h=horizon, \n",
    "        input_size=INPUT_SIZE, \n",
    "        max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=EARLY_STOP_PATIENCE,\n",
    "        learning_rate=LEARNING_RATE, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        scaler_type='robust', \n",
    "        random_seed=42,\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "    )\n",
    "    return [\n",
    "        ('NBEATSx', NBEATSx(hist_exog_list=hist_exog, stack_types=['identity', 'identity', 'identity'], **common)),\n",
    "        ('NHITS', NHITS(hist_exog_list=hist_exog, **common)),\n",
    "        ('TFT', TFT(hist_exog_list=hist_exog, hidden_size=64, **common)),\n",
    "        ('TiDE', TiDE(hist_exog_list=hist_exog, **common)),\n",
    "        ('BiTCN', BiTCN(hist_exog_list=hist_exog, **common)),\n",
    "    ]\n",
    "\n",
    "if HAS_NF:\n",
    "    print(f\"NF models: NBEATSx, NHITS, TFT, TiDE, BiTCN\")\n",
    "    print(f\"hist_exog: {len(ALL_FEATURES)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "690f02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NF - Full24h (test_horizon=689)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_nf_models() got an unexpected keyword argument 'horizon'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNF - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (test_horizon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_horizon\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_nf_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mALL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_horizon\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: get_nf_models() got an unexpected keyword argument 'horizon'"
     ]
    }
   ],
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n",
    "    for ds_name in ['Full24h', 'Night0']:\n",
    "        if ds_name not in prepared_data:\n",
    "            continue\n",
    "        \n",
    "        # Combine train + test for proper evaluation\n",
    "        nf_train = prepared_data[ds_name]['train']\n",
    "        nf_test = prepared_data[ds_name]['test']\n",
    "        nf_all = pd.concat([nf_train, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        \n",
    "        # Calculate test horizon per series\n",
    "        test_horizon = 24*7\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NF - {ds_name} (test_horizon={test_horizon})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for model_name, model in get_nf_models(ALL_FEATURES, horizon=test_horizon):\n",
    "            print(f\"\\n  {model_name}...\")\n",
    "            try:\n",
    "                t0 = time.time()\n",
    "                nf = NeuralForecast(models=[model], freq='h')\n",
    "                \n",
    "                # Use cross_validation for proper evaluation\n",
    "                cv_results = nf.cross_validation(\n",
    "                    df=nf_all,\n",
    "                    n_windows=1,\n",
    "                    step_size=test_horizon,\n",
    "                )\n",
    "                elapsed = time.time() - t0\n",
    "                \n",
    "                pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "                \n",
    "                print(f\"    CV results: {len(cv_results)} rows\")\n",
    "                \n",
    "                y_true = cv_results['y'].values\n",
    "                y_pred = np.clip(cv_results[pred_col].values, 0, None)\n",
    "                \n",
    "                m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "                \n",
    "                eval_df = cv_results.copy()\n",
    "                eval_df['beach'] = eval_df['unique_id']\n",
    "                eval_df['count'] = eval_df['y']\n",
    "                beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "                avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "                \n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                    'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "                })\n",
    "                print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28328e27",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "beach_df = pd.concat(all_beach_results, ignore_index=True) if all_beach_results else pd.DataFrame()\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'results.csv', index=False)\n",
    "if len(beach_df) > 0:\n",
    "    beach_df.to_csv(save_dir / 'beach_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS BY DATASET\")\n",
    "print(\"=\"*70)\n",
    "for ds in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds].sort_values('AvgRelMAE')\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    print(f\"\\n{ds}:\")\n",
    "    print(sub[['Model','Type','MAE','R2','AvgRelMAE','Time']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ec410",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\n",
    "print(\"\\nRelMAE (%) by Model x Dataset:\")\n",
    "print(pivot.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37323f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # RelMAE bar chart\n",
    "    pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\n",
    "    pivot = pivot.loc[pivot.mean(axis=1).sort_values().index]\n",
    "    pivot.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "    axes[0].set_ylabel('Avg RelMAE (%)')\n",
    "    axes[0].set_title('Model Performance (lower is better)')\n",
    "    axes[0].legend(title='Dataset')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # R2 bar chart\n",
    "    pivot_r2 = results_df.pivot_table(index='Model', columns='Dataset', values='R2')\n",
    "    pivot_r2 = pivot_r2.loc[pivot_r2.mean(axis=1).sort_values(ascending=False).index]\n",
    "    pivot_r2.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "    axes[1].set_ylabel('R²')\n",
    "    axes[1].set_title('R² Score (higher is better)')\n",
    "    axes[1].legend(title='Dataset')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61622ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for ds in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds].dropna()\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best = sub.loc[sub['AvgRelMAE'].idxmin()]\n",
    "    print(f\"\\n{ds}: Best = {best['Model']} ({best['Type']}) - RelMAE={best['AvgRelMAE']:.1f}%, R2={best['R2']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
