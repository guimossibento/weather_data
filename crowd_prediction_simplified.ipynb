{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction \u2014 LSTM, TFT, TiDE + Optuna Optimization\n\nThis notebook compares models across **three dataset strategies**:\n1. **Daytime only** \u2014 remove night hours (6PM\u20136AM)\n2. **Full 24h** \u2014 keep all data including noisy night counts\n3. **Night = 0** \u2014 keep 24h but replace night counts with 0\n\nModels tested:\n- **Sklearn baselines**: Lasso, RandomForest, XGBoost, LightGBM, CatBoost\n- **NeuralForecast**: LSTM, TFT, TiDE\n- **Optuna-optimized**: XGBoost, LightGBM, CatBoost, LSTM"
   ],
   "id": "7acafb40"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === PATHS ===\nCACHE_DIR = \"cache/predictions\"\nCOUNTING_MODEL = \"bayesian_vgg19\"\nSAVE_DIR = \"models/optuna_comparison\"\n\n# === SAMPLING ===\nSAMPLE_FRAC = 1.0\nMAX_BEACHES = None\n\n# === MODEL PARAMETERS ===\nMAX_STEPS = 500\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nINPUT_SIZE = 24\n\n# === TIME ===\nNIGHT_START = 20\nNIGHT_END = 6\n\n# === OPTUNA ===\nOPTUNA_TRIALS = 30\nOPTUNA_TIMEOUT = 300\n\n# === FLAGS ===\nRUN_SKLEARN = True\nRUN_NEURALFORECAST = True\nRUN_OPTUNA = True"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "ed5d0ac8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import subprocess, sys\npkgs = [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\", \"optuna\"]\nfor pkg in pkgs:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\nprint(\"Packages installed\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "2ec0f596"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nimport torch\n\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\ntry:\n    from xgboost import XGBRegressor\n    HAS_XGB = True\nexcept: HAS_XGB = False\n\ntry:\n    from lightgbm import LGBMRegressor\n    HAS_LGBM = True\nexcept: HAS_LGBM = False\n\ntry:\n    from catboost import CatBoostRegressor\n    HAS_CATBOOST = True\nexcept: HAS_CATBOOST = False\n\ntry:\n    from neuralforecast import NeuralForecast\n    from neuralforecast.models import LSTM, TFT, TiDE\n    from neuralforecast.losses.pytorch import MAE\n    HAS_NF = True\nexcept Exception as e:\n    print(f\"NeuralForecast error: {e}\")\n    HAS_NF = False\n\nif torch.cuda.is_available():\n    ACCELERATOR = 'gpu'\n    DEVICES = 1\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    ACCELERATOR = 'mps'\n    DEVICES = 1\nelse:\n    ACCELERATOR = 'cpu'\n    DEVICES = 1\n\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cfaae9d5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_metrics(y_true, y_pred, max_count):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    rel_mae = (mae / max_count) * 100 if max_count > 0 else 0\n    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE': rel_mae}\n\ndef eval_per_beach(df, y_pred, beach_col='unique_id'):\n    results = []\n    for b in df[beach_col].unique():\n        mask = df[beach_col] == b\n        if mask.sum() < 3:\n            continue\n        y_true = df.loc[mask, 'y'].values if 'y' in df.columns else df.loc[mask, 'count'].values\n        y_p = y_pred[mask.values] if hasattr(mask, 'values') else y_pred[mask]\n        max_count = y_true.max()\n        m = calc_metrics(y_true, y_p, max_count)\n        m['camera'] = b\n        m['max_count'] = max_count\n        m['n'] = mask.sum()\n        results.append(m)\n    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "13539953"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ],
   "id": "4980944f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_cache(cache_dir, model):\n    cache_path = Path(cache_dir) / model\n    records = []\n    for jf in cache_path.rglob(\"*.json\"):\n        try:\n            with open(jf) as f:\n                r = json.load(f)\n            if 'error' not in r:\n                records.append(r)\n        except: pass\n    \n    rows = []\n    for r in records:\n        row = {\n            'beach': r.get('beach') or r.get('beach_folder'),\n            'beach_folder': r.get('beach_folder'),\n            'datetime': r.get('datetime'),\n            'count': r.get('count')\n        }\n        for k, v in r.get('weather', {}).items():\n            row[k] = v\n        rows.append(row)\n    \n    df = pd.DataFrame(rows)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    df = df.sort_values('datetime').reset_index(drop=True)\n    return df\n\ndf_raw = load_cache(CACHE_DIR, COUNTING_MODEL)\nprint(f\"Loaded: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "529f5950"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EXCLUDE = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols']\nEXCLUDE_PREFIX = ['ibred', 'ClubNauticSoller', 'Guenthoer', 'youtube']\n\nbefore = len(df_raw)\ndf_raw = df_raw[~df_raw['beach_folder'].isin(EXCLUDE)]\nfor p in EXCLUDE_PREFIX:\n    df_raw = df_raw[~df_raw['beach_folder'].str.startswith(p, na=False)]\nprint(f\"Filtered: {before} -> {len(df_raw)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "a26af23f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if SAMPLE_FRAC < 1.0:\n    df_raw = df_raw.sample(frac=SAMPLE_FRAC, random_state=42).sort_values('datetime').reset_index(drop=True)\n\nif MAX_BEACHES:\n    top = df_raw['beach'].value_counts().head(MAX_BEACHES).index.tolist()\n    df_raw = df_raw[df_raw['beach'].isin(top)].reset_index(drop=True)\n\nprint(f\"Final: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "53950688"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = df_raw.copy()\ndf['hour'] = df['datetime'].dt.hour\ndf['day_of_week'] = df['datetime'].dt.dayofweek\ndf['month'] = df['datetime'].dt.month\ndf['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\ndf['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\ndf['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n\nWEATHER_COLS = [c for c in df.columns if c.startswith('ae_') or c.startswith('om_')]\nTEMPORAL_COLS = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night']\nALL_FEATURES = WEATHER_COLS + TEMPORAL_COLS\n\ndf = df.dropna(subset=ALL_FEATURES + ['count']).reset_index(drop=True)\ngood = df.groupby('beach')['count'].max()\ngood = good[good > 20].index.tolist()\ndf = df[df['beach'].isin(good)].reset_index(drop=True)\n\nprint(f\"After cleaning: {len(df)} rows, {len(good)} beaches\")\nprint(f\"Features: {len(ALL_FEATURES)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "684c17ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Three Dataset Strategies"
   ],
   "id": "eff774dc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds_daytime = df[df['is_night'] == 0].copy().reset_index(drop=True)\nds_full24h = df.copy()\nds_night0 = df.copy()\nds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n\ndatasets = {'Daytime': ds_daytime, 'Full24h': ds_full24h, 'Night0': ds_night0}\n\nprint(\"=\" * 80)\nprint(\"DATASET COMPARISON\")\nprint(\"=\" * 80)\n\nfor name, d in datasets.items():\n    night_rows = d[d['is_night'] == 1] if 'is_night' in d.columns else pd.DataFrame()\n    day_rows = d[d['is_night'] == 0] if 'is_night' in d.columns else d\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Total rows:     {len(d)}\")\n    print(f\"  Beaches:        {d['beach'].nunique()}\")\n    print(f\"  Night rows:     {len(night_rows)} ({len(night_rows)/len(d)*100:.1f}%)\")\n    print(f\"  Day rows:       {len(day_rows)} ({len(day_rows)/len(d)*100:.1f}%)\")\n    print(f\"  Count mean:     {d['count'].mean():.1f}\")\n    print(f\"  Count max:      {d['count'].max():.1f}\")\n    print(f\"  Zeros:          {(d['count'] == 0).sum()} ({(d['count'] == 0).sum()/len(d)*100:.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "308ff4db"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def split_data(df, train_frac=0.7, val_frac=0.15):\n    n = len(df)\n    t1 = int(n * train_frac)\n    t2 = int(n * (train_frac + val_frac))\n    return df.iloc[:t1], df.iloc[t1:t2], df.iloc[t2:]\n\nsplits = {}\nfor name, d in datasets.items():\n    train, val, test = split_data(d)\n    splits[name] = {'train': train, 'val': val, 'test': test}\n    print(f\"{name}: train={len(train)}, val={len(val)}, test={len(test)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "eccdb07f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets with Gap Filling and Interpolation"
   ],
   "id": "23b8455f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from utilsforecast.preprocessing import fill_gaps\n\ndef to_nf_format(df, id_col='beach_folder'):\n    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n    cols = [c for c in cols if c in df.columns]\n    nf_df = df[cols].copy()\n    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n    return nf_df\n\ndef prepare_dataset_with_filled_gaps(train_df, test_df, freq='h'):\n    nf_train = to_nf_format(train_df)\n    nf_test = to_nf_format(test_df)\n    \n    nf_train = nf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n    nf_test = nf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n    \n    nf_train = fill_gaps(nf_train, freq=freq)\n    nf_test = fill_gaps(nf_test, freq=freq)\n    \n    numeric_cols = nf_train.select_dtypes(include=[np.number]).columns.tolist()\n    for col in numeric_cols:\n        nf_train[col] = nf_train.groupby('unique_id')[col].transform(\n            lambda x: x.interpolate(method='linear').ffill().bfill()\n        )\n        nf_test[col] = nf_test.groupby('unique_id')[col].transform(\n            lambda x: x.interpolate(method='linear').ffill().bfill()\n        )\n    \n    common_ids = set(nf_train['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n    nf_train = nf_train[nf_train['unique_id'].isin(common_ids)].reset_index(drop=True)\n    nf_test = nf_test[nf_test['unique_id'].isin(common_ids)].reset_index(drop=True)\n    \n    return nf_train, nf_test, list(common_ids)\n\nprepared_data = {}\n\nprint(\"=\" * 70)\nprint(\"PREPARING DATASETS WITH FILLED GAPS + INTERPOLATION\")\nprint(\"=\" * 70)\n\nfor ds_name in ['Daytime', 'Full24h', 'Night0']:\n    print(f\"\\nProcessing: {ds_name}\")\n    s = splits[ds_name]\n    train_val = pd.concat([s['train'], s['val']])\n    \n    nf_train, nf_test, series_ids = prepare_dataset_with_filled_gaps(train_val, s['test'])\n    \n    prepared_data[ds_name] = {\n        'train': nf_train,\n        'test': nf_test,\n        'series_ids': series_ids,\n        'n_series': len(series_ids),\n    }\n    \n    print(f\"  Train: {len(nf_train)}, Test: {len(nf_test)}, Series: {len(series_ids)}\")\n    print(f\"  NaN check - train: {nf_train['y'].isna().sum()}, test: {nf_test['y'].isna().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "fa8efd66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Baseline Models (across all datasets)"
   ],
   "id": "46bee08a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_results = []\nall_beach_results = []\n\nif RUN_SKLEARN:\n    for ds_name in datasets.keys():\n        data = prepared_data[ds_name]\n        train_df = data['train']\n        test_df = data['test']\n        \n        feature_cols = [c for c in train_df.columns if c not in ['unique_id', 'ds', 'y']]\n        \n        X_train = train_df[feature_cols]\n        y_train = train_df['y']\n        X_test = test_df[feature_cols]\n        y_test = test_df['y']\n        \n        sklearn_models = {\n            'Lasso': Lasso(alpha=0.1),\n            'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n        }\n        if HAS_XGB:\n            sklearn_models['XGBoost'] = XGBRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbosity=0)\n        if HAS_LGBM:\n            sklearn_models['LightGBM'] = LGBMRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)\n        if HAS_CATBOOST:\n            sklearn_models['CatBoost'] = CatBoostRegressor(n_estimators=200, max_depth=6, random_state=42, verbose=0)\n        \n        print(f\"\\n{'=' * 60}\")\n        print(f\"SKLEARN - {ds_name}\")\n        print(f\"{'=' * 60}\")\n        print(f\"Train: {len(X_train)}, Test: {len(X_test)}, Features: {len(feature_cols)}\")\n        \n        for name, model in sklearn_models.items():\n            t0 = time.time()\n            model.fit(X_train, y_train)\n            y_pred = np.clip(model.predict(X_test), 0, None)\n            elapsed = time.time() - t0\n            \n            m = calc_metrics(y_test.values, y_pred, y_test.max())\n            \n            eval_df = test_df[['unique_id', 'y']].copy()\n            eval_df['count'] = eval_df['y']\n            eval_df['beach'] = eval_df['unique_id']\n            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n            beach_df['model'] = name\n            beach_df['dataset'] = ds_name\n            all_beach_results.append(beach_df)\n            \n            avg_rel = beach_df['RelMAE'].mean()\n            all_results.append({\n                'Model': name, 'Dataset': ds_name, 'Type': 'Sklearn',\n                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n                'AvgRelMAE': avg_rel, 'Time': elapsed\n            })\n            print(f\"  {name:15s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "43dbef42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralForecast Models: LSTM, TFT, TiDE\n\n| Model | Strengths | Best For |\n|-------|-----------|----------|\n| **LSTM** | Classic baseline, proven | Sequential patterns |\n| **TFT** | Static features, interpretable attention | New beach prediction |\n| **TiDE** | Fast, lightweight, exogenous support | Production speed |"
   ],
   "id": "21aa0ca0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_nf_models(hist_exog, horizon):\n    common = dict(\n        h=horizon,\n        input_size=INPUT_SIZE,\n        max_steps=MAX_STEPS,\n        early_stop_patience_steps=-1,\n        learning_rate=LEARNING_RATE,\n        batch_size=BATCH_SIZE,\n        scaler_type='robust',\n        random_seed=42,\n        accelerator=ACCELERATOR,\n        devices=DEVICES,\n        loss=MAE(),\n    )\n    \n    return [\n        ('LSTM', LSTM(\n            hist_exog_list=hist_exog,\n            encoder_hidden_size=64,\n            encoder_n_layers=2,\n            **common\n        )),\n        ('TFT', TFT(\n            hist_exog_list=hist_exog,\n            hidden_size=64,\n            n_head=4,\n            **common\n        )),\n        ('TiDE', TiDE(\n            hist_exog_list=hist_exog,\n            hidden_size=128,\n            decoder_output_dim=16,\n            **common\n        )),\n    ]\n\nprint(\"NF Models: LSTM, TFT, TiDE\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "583182c6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n    for ds_name in ['Full24h', 'Night0']:\n        nf_train = prepared_data[ds_name]['train']\n        nf_test = prepared_data[ds_name]['test']\n        \n        nf_all = pd.concat([nf_train, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n        test_horizon = nf_test.groupby('unique_id').size().min()\n        horizon = min(72, test_horizon)\n        \n        print(f\"\\n{'=' * 60}\")\n        print(f\"NEURALFORECAST - {ds_name} (horizon={horizon})\")\n        print(f\"{'=' * 60}\")\n        \n        for model_name, model in get_nf_models(ALL_FEATURES, horizon=horizon):\n            print(f\"\\n  {model_name}...\")\n            try:\n                t0 = time.time()\n                nf = NeuralForecast(models=[model], freq='h')\n                \n                cv_results = nf.cross_validation(\n                    df=nf_all,\n                    n_windows=1,\n                    step_size=horizon,\n                )\n                elapsed = time.time() - t0\n                \n                pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n                \n                y_true = cv_results['y'].values\n                y_pred = np.clip(cv_results[pred_col].values, 0, None)\n                \n                m = calc_metrics(y_true, y_pred, y_true.max())\n                \n                eval_df = cv_results.copy()\n                eval_df['beach'] = eval_df['unique_id']\n                beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n                avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n                \n                all_results.append({\n                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n                    'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n                    'AvgRelMAE': avg_rel, 'Time': elapsed\n                })\n                print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n                \n            except Exception as e:\n                print(f\"    ERROR: {e}\")\n                import traceback\n                traceback.print_exc()"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "76fc291f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimization\n\nOptimize across datasets:\n- **XGBoost**: n_estimators, max_depth, learning_rate, subsample, colsample_bytree\n- **LightGBM**: n_estimators, num_leaves, learning_rate, feature_fraction, bagging_fraction\n- **CatBoost**: iterations, depth, learning_rate, l2_leaf_reg\n- **LSTM** (via NeuralForecast): hidden_size, n_layers, learning_rate, dropout"
   ],
   "id": "71b206f0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_avg_rel_mae(y_true, y_pred, groups):\n    rel_maes = []\n    for g in np.unique(groups):\n        mask = groups == g\n        if mask.sum() < 3:\n            continue\n        yt = y_true[mask]\n        yp = y_pred[mask]\n        max_count = yt.max()\n        if max_count > 0:\n            rel_maes.append(mean_absolute_error(yt, yp) / max_count * 100)\n    return np.mean(rel_maes) if rel_maes else 999.0\n\ndef create_xgb_objective(X_train, y_train, X_val, y_val, groups_val):\n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n            'random_state': 42, 'n_jobs': -1, 'verbosity': 0,\n        }\n        model = XGBRegressor(**params)\n        model.fit(X_train, y_train)\n        y_pred = np.clip(model.predict(X_val), 0, None)\n        return calc_avg_rel_mae(y_val.values, y_pred, groups_val.values)\n    return objective\n\ndef create_lgbm_objective(X_train, y_train, X_val, y_val, groups_val):\n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n            'random_state': 42, 'n_jobs': -1, 'verbose': -1,\n        }\n        model = LGBMRegressor(**params)\n        model.fit(X_train, y_train)\n        y_pred = np.clip(model.predict(X_val), 0, None)\n        return calc_avg_rel_mae(y_val.values, y_pred, groups_val.values)\n    return objective\n\ndef create_catboost_objective(X_train, y_train, X_val, y_val, groups_val):\n    def objective(trial):\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 1000),\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n            'border_count': trial.suggest_int('border_count', 32, 255),\n            'random_state': 42, 'verbose': 0,\n        }\n        model = CatBoostRegressor(**params)\n        model.fit(X_train, y_train)\n        y_pred = np.clip(model.predict(X_val), 0, None)\n        return calc_avg_rel_mae(y_val.values, y_pred, groups_val.values)\n    return objective"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "43817587"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_models_per_dataset = {}\n\nif RUN_OPTUNA:\n    for ds_name in ['Night0']:\n        data = prepared_data[ds_name]\n        \n        feature_cols = [c for c in data['train'].columns if c not in ['unique_id', 'ds', 'y']]\n        \n        train_nf = data['train']\n        \n        split_idx = int(len(train_nf) * 0.82)\n        X_tr = train_nf[feature_cols].iloc[:split_idx]\n        y_tr = train_nf['y'].iloc[:split_idx]\n        X_va = train_nf[feature_cols].iloc[split_idx:]\n        y_va = train_nf['y'].iloc[split_idx:]\n        groups_va = train_nf['unique_id'].iloc[split_idx:]\n        \n        best_models = {}\n        \n        print(f\"\\n{'=' * 60}\")\n        print(f\"OPTUNA - {ds_name} (optimizing AvgRelMAE, trials={OPTUNA_TRIALS})\")\n        print(f\"{'=' * 60}\")\n        \n        if HAS_XGB:\n            print(\"\\n  XGBoost...\")\n            study = optuna.create_study(direction='minimize')\n            study.optimize(create_xgb_objective(X_tr, y_tr, X_va, y_va, groups_va),\n                          n_trials=OPTUNA_TRIALS, timeout=OPTUNA_TIMEOUT, show_progress_bar=True)\n            best_models['XGBoost_Optuna'] = study.best_params\n            print(f\"    Best AvgRelMAE: {study.best_value:.2f}%\")\n        \n        if HAS_LGBM:\n            print(\"\\n  LightGBM...\")\n            study = optuna.create_study(direction='minimize')\n            study.optimize(create_lgbm_objective(X_tr, y_tr, X_va, y_va, groups_va),\n                          n_trials=OPTUNA_TRIALS, timeout=OPTUNA_TIMEOUT, show_progress_bar=True)\n            best_models['LightGBM_Optuna'] = study.best_params\n            print(f\"    Best AvgRelMAE: {study.best_value:.2f}%\")\n        \n        if HAS_CATBOOST:\n            print(\"\\n  CatBoost...\")\n            study = optuna.create_study(direction='minimize')\n            study.optimize(create_catboost_objective(X_tr, y_tr, X_va, y_va, groups_va),\n                          n_trials=OPTUNA_TRIALS, timeout=OPTUNA_TIMEOUT, show_progress_bar=True)\n            best_models['CatBoost_Optuna'] = study.best_params\n            print(f\"    Best AvgRelMAE: {study.best_value:.2f}%\")\n        \n        best_models_per_dataset[ds_name] = best_models"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e3cb6722"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if RUN_OPTUNA and best_models_per_dataset:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"EVALUATING OPTUNA MODELS ACROSS ALL DATASETS\")\n    print(\"=\" * 60)\n    \n    for ds_name in datasets.keys():\n        data = prepared_data[ds_name]\n        feature_cols = [c for c in data['train'].columns if c not in ['unique_id', 'ds', 'y']]\n        \n        X_train_full = data['train'][feature_cols]\n        y_train_full = data['train']['y']\n        X_test = data['test'][feature_cols]\n        y_test = data['test']['y']\n        \n        optuna_ds = 'Night0'\n        best_models = best_models_per_dataset.get(optuna_ds, {})\n        \n        print(f\"\\n  --- {ds_name} ---\")\n        \n        for name, params in best_models.items():\n            t0 = time.time()\n            \n            if 'XGBoost' in name:\n                model = XGBRegressor(**params, random_state=42, n_jobs=-1, verbosity=0)\n            elif 'LightGBM' in name:\n                model = LGBMRegressor(**params, random_state=42, n_jobs=-1, verbose=-1)\n            elif 'CatBoost' in name:\n                model = CatBoostRegressor(**params, random_state=42, verbose=0)\n            else:\n                continue\n            \n            model.fit(X_train_full, y_train_full)\n            y_pred = np.clip(model.predict(X_test), 0, None)\n            elapsed = time.time() - t0\n            \n            m = calc_metrics(y_test.values, y_pred, y_test.max())\n            \n            eval_df = data['test'][['unique_id', 'y']].copy()\n            eval_df['count'] = eval_df['y']\n            eval_df['beach'] = eval_df['unique_id']\n            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n            avg_rel = beach_df['RelMAE'].mean()\n            \n            all_results.append({\n                'Model': name, 'Dataset': ds_name, 'Type': 'Optuna',\n                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n                'AvgRelMAE': avg_rel, 'Time': elapsed\n            })\n            print(f\"    {name:20s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "854e3c09"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_lstm_objective(nf_train, nf_val, hist_exog, horizon):\n    def objective(trial):\n        params = {\n            'h': horizon,\n            'input_size': INPUT_SIZE,\n            'encoder_hidden_size': trial.suggest_int('hidden_size', 32, 256),\n            'encoder_n_layers': trial.suggest_int('n_layers', 1, 3),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n            'encoder_dropout': trial.suggest_float('dropout', 0.0, 0.5),\n            'max_steps': 200,\n            'early_stop_patience_steps': -1,\n            'batch_size': BATCH_SIZE,\n            'scaler_type': 'robust',\n            'random_seed': 42,\n            'accelerator': ACCELERATOR,\n            'devices': DEVICES,\n            'hist_exog_list': hist_exog,\n            'loss': MAE(),\n        }\n        \n        model = LSTM(**params)\n        nf = NeuralForecast(models=[model], freq='h')\n        \n        nf_all = pd.concat([nf_train, nf_val]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n        cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n        \n        pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n        y_true = cv_results['y'].values\n        y_pred = np.clip(cv_results[pred_col].values, 0, None)\n        groups = cv_results['unique_id'].values\n        \n        return calc_avg_rel_mae(y_true, y_pred, groups)\n    return objective\n\nif RUN_OPTUNA and HAS_NF:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"OPTUNA LSTM OPTIMIZATION (optimizing AvgRelMAE)\")\n    print(\"=\" * 60)\n    \n    nf_tr = prepared_data['Night0']['train']\n    \n    split_idx = int(len(nf_tr) * 0.82)\n    nf_tr_sub = nf_tr.iloc[:split_idx]\n    nf_va_sub = nf_tr.iloc[split_idx:]\n    \n    horizon_opt = min(24, nf_va_sub.groupby('unique_id').size().min())\n    \n    print(f\"  Optimizing LSTM (horizon={horizon_opt})...\")\n    study = optuna.create_study(direction='minimize')\n    \n    try:\n        study.optimize(\n            create_lstm_objective(nf_tr_sub, nf_va_sub, ALL_FEATURES, horizon_opt),\n            n_trials=min(10, OPTUNA_TRIALS),\n            timeout=OPTUNA_TIMEOUT,\n            show_progress_bar=True\n        )\n        \n        print(f\"    Best AvgRelMAE: {study.best_value:.2f}%\")\n        print(f\"    Best params: {study.best_params}\")\n        best_models_per_dataset.setdefault('Night0', {})['LSTM_Optuna'] = study.best_params\n    except Exception as e:\n        print(f\"    ERROR: {e}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "1d572742"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lstm_params = best_models_per_dataset.get('Night0', {}).get('LSTM_Optuna')\n\nif RUN_OPTUNA and HAS_NF and lstm_params:\n    for ds_name in ['Full24h', 'Night0']:\n        nf_train = prepared_data[ds_name]['train']\n        nf_test = prepared_data[ds_name]['test']\n        nf_all = pd.concat([nf_train, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n        \n        horizon = min(72, nf_test.groupby('unique_id').size().min())\n        \n        print(f\"\\n  Evaluating LSTM_Optuna on {ds_name} (horizon={horizon})...\")\n        \n        t0 = time.time()\n        model = LSTM(\n            h=horizon,\n            input_size=INPUT_SIZE,\n            encoder_hidden_size=lstm_params.get('hidden_size', 64),\n            encoder_n_layers=lstm_params.get('n_layers', 2),\n            learning_rate=lstm_params.get('learning_rate', LEARNING_RATE),\n            encoder_dropout=lstm_params.get('dropout', 0.1),\n            max_steps=MAX_STEPS,\n            early_stop_patience_steps=-1,\n            batch_size=BATCH_SIZE,\n            scaler_type='robust',\n            random_seed=42,\n            accelerator=ACCELERATOR,\n            devices=DEVICES,\n            hist_exog_list=ALL_FEATURES,\n            loss=MAE(),\n        )\n        \n        nf = NeuralForecast(models=[model], freq='h')\n        cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n        elapsed = time.time() - t0\n        \n        pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n        y_true = cv_results['y'].values\n        y_pred = np.clip(cv_results[pred_col].values, 0, None)\n        \n        m = calc_metrics(y_true, y_pred, y_true.max())\n        eval_df = cv_results.copy()\n        eval_df['beach'] = eval_df['unique_id']\n        beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n        avg_rel = beach_df['RelMAE'].mean()\n        \n        all_results.append({\n            'Model': 'LSTM_Optuna', 'Dataset': ds_name, 'Type': 'Optuna+NF',\n            'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n            'AvgRelMAE': avg_rel, 'Time': elapsed\n        })\n        print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "6a2d92c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "id": "ce557d2b"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_df = pd.DataFrame(all_results)\nbeach_results_df = pd.concat(all_beach_results, ignore_index=True) if all_beach_results else pd.DataFrame()\n\nsave_dir = Path(SAVE_DIR)\nsave_dir.mkdir(parents=True, exist_ok=True)\nresults_df.to_csv(save_dir / 'results.csv', index=False)\nif len(beach_results_df) > 0:\n    beach_results_df.to_csv(save_dir / 'beach_results.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS BY DATASET\")\nprint(\"=\" * 70)\nfor ds in datasets.keys():\n    sub = results_df[results_df['Dataset'] == ds].sort_values('AvgRelMAE')\n    if len(sub) == 0:\n        continue\n    print(f\"\\n{ds}:\")\n    print(sub[['Model', 'Type', 'MAE', 'R2', 'AvgRelMAE', 'Time']].to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "6b498851"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\nprint(\"\\nRelMAE (%) by Model x Dataset:\")\nprint(pivot.round(1).to_string())"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "4fe6ff82"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if len(results_df) > 0:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    pivot = results_df.pivot_table(index='Model', columns='Dataset', values='AvgRelMAE')\n    pivot = pivot.loc[pivot.mean(axis=1).sort_values().index]\n    pivot.plot(kind='bar', ax=axes[0], width=0.8)\n    axes[0].set_ylabel('Avg RelMAE (%)')\n    axes[0].set_title('Model Performance (lower is better)')\n    axes[0].legend(title='Dataset')\n    axes[0].tick_params(axis='x', rotation=45)\n    \n    pivot_r2 = results_df.pivot_table(index='Model', columns='Dataset', values='R2')\n    pivot_r2 = pivot_r2.loc[pivot_r2.mean(axis=1).sort_values(ascending=False).index]\n    pivot_r2.plot(kind='bar', ax=axes[1], width=0.8)\n    axes[1].set_ylabel('R\u00b2')\n    axes[1].set_title('R\u00b2 Score (higher is better)')\n    axes[1].legend(title='Dataset')\n    axes[1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig(save_dir / 'comparison.png', dpi=150)\n    plt.show()"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "fcfb5ccb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 70)\nprint(\"BEST MODEL PER DATASET\")\nprint(\"=\" * 70)\n\nfor ds in datasets.keys():\n    sub = results_df[results_df['Dataset'] == ds].dropna(subset=['AvgRelMAE'])\n    if len(sub) == 0:\n        continue\n    best = sub.loc[sub['AvgRelMAE'].idxmin()]\n    print(f\"\\n{ds}: Best = {best['Model']} ({best['Type']})\")\n    print(f\"  MAE: {best['MAE']:.2f}\")\n    print(f\"  RelMAE: {best['AvgRelMAE']:.1f}%\")\n    print(f\"  R\u00b2: {best['R2']:.3f}\")\n    print(f\"  Time: {best['Time']:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "9969ec9b"
  }
 ]
}