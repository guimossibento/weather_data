{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3001746d",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — LSTM, TFT, TiDE + Optuna Optimization\n",
    "\n",
    "This notebook compares:\n",
    "1. **Sklearn baseline models** (quick comparison)\n",
    "2. **NeuralForecast models**: LSTM, TFT, TiDE\n",
    "3. **Optuna-optimized models**: LSTM, XGBoost, CatBoost, LightGBM\n",
    "\n",
    "**Model Selection Rationale:**\n",
    "- **TFT**: Best for new beach prediction via static features (lat/lon/capacity)\n",
    "- **TiDE**: Fast, lightweight encoder-decoder with exogenous support\n",
    "- **LSTM**: Classic baseline for sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATHS ===\n",
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/optuna_comparison\"\n",
    "\n",
    "# === SAMPLING ===\n",
    "SAMPLE_FRAC = 1.0\n",
    "MAX_BEACHES = None\n",
    "\n",
    "# === MODEL PARAMETERS ===\n",
    "MAX_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 24\n",
    "\n",
    "# === TIME ===\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6\n",
    "\n",
    "# === OPTUNA ===\n",
    "OPTUNA_TRIALS = 30\n",
    "OPTUNA_TIMEOUT = 300\n",
    "\n",
    "# === FLAGS ===\n",
    "RUN_SKLEARN = True\n",
    "RUN_NEURALFORECAST = True\n",
    "RUN_OPTUNA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "pkgs = [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\", \"optuna\"]\n",
    "for pkg in pkgs:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except: HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except: HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except: HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import LSTM, TFT, TiDE\n",
    "    from neuralforecast.losses.pytorch import MAE\n",
    "    HAS_NF = True\n",
    "except Exception as e:\n",
    "    print(f\"NeuralForecast error: {e}\")\n",
    "    HAS_NF = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ACCELERATOR = 'gpu'\n",
    "    DEVICES = 1\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    ACCELERATOR = 'mps'\n",
    "    DEVICES = 1\n",
    "else:\n",
    "    ACCELERATOR = 'cpu'\n",
    "    DEVICES = 1\n",
    "\n",
    "print(f\"Accelerator: {ACCELERATOR}\")\n",
    "print(f\"XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, max_count):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_mae = (mae / max_count) * 100 if max_count > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE': rel_mae}\n",
    "\n",
    "def eval_per_beach(df, y_pred, beach_col='unique_id'):\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        y_true = df.loc[mask, 'y'].values if 'y' in df.columns else df.loc[mask, 'count'].values\n",
    "        y_p = y_pred[mask.values] if hasattr(mask, 'values') else y_pred[mask]\n",
    "        max_count = y_true.max()\n",
    "        m = calc_metrics(y_true, y_p, max_count)\n",
    "        m['camera'] = b\n",
    "        m['max_count'] = max_count\n",
    "        m['n'] = mask.sum()\n",
    "        results.append(m)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(cache_dir, model):\n",
    "    cache_path = Path(cache_dir) / model\n",
    "    records = []\n",
    "    for jf in cache_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(jf) as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    \n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {\n",
    "            'beach': r.get('beach') or r.get('beach_folder'),\n",
    "            'beach_folder': r.get('beach_folder'),\n",
    "            'datetime': r.get('datetime'),\n",
    "            'count': r.get('count')\n",
    "        }\n",
    "        for k, v in r.get('weather', {}).items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, COUNTING_MODEL)\n",
    "print(f\"Loaded: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d20f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols']\n",
    "EXCLUDE_PREFIX = ['ibred', 'ClubNauticSoller', 'Guenthoer', 'youtube']\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw['beach_folder'].isin(EXCLUDE)]\n",
    "for p in EXCLUDE_PREFIX:\n",
    "    df_raw = df_raw[~df_raw['beach_folder'].str.startswith(p, na=False)]\n",
    "print(f\"Filtered: {before} -> {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_FRAC < 1.0:\n",
    "    df_raw = df_raw.sample(frac=SAMPLE_FRAC, random_state=42).sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "if MAX_BEACHES:\n",
    "    top = df_raw['beach'].value_counts().head(MAX_BEACHES).index.tolist()\n",
    "    df_raw = df_raw[df_raw['beach'].isin(top)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final: {len(df_raw)} rows, {df_raw['beach'].nunique()} beaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "\n",
    "WEATHER_COLS = [c for c in df.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_COLS = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night']\n",
    "ALL_FEATURES = WEATHER_COLS + TEMPORAL_COLS\n",
    "\n",
    "df = df.dropna(subset=ALL_FEATURES + ['count']).reset_index(drop=True)\n",
    "good = df.groupby('beach')['count'].max()\n",
    "good = good[good > 20].index.tolist()\n",
    "df = df[df['beach'].isin(good)].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {len(df)} rows, {len(good)} beaches\")\n",
    "print(f\"Features: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Night=0 dataset (best performer in previous experiments)\n",
    "ds_night0 = df.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "print(f\"Night0 dataset: {len(ds_night0)} rows\")\n",
    "print(f\"  Night rows (set to 0): {(ds_night0['is_night'] == 1).sum()}\")\n",
    "print(f\"  Day rows: {(ds_night0['is_night'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(df)\n",
    "    t1 = int(n * train_frac)\n",
    "    t2 = int(n * (train_frac + val_frac))\n",
    "    return df.iloc[:t1], df.iloc[t1:t2], df.iloc[t2:]\n",
    "\n",
    "train, val, test = split_data(ds_night0)\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sklearn_header",
   "metadata": {},
   "source": [
    "## Sklearn Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491de871",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "X_train = pd.concat([train, val])[ALL_FEATURES]\n",
    "y_train = pd.concat([train, val])['count']\n",
    "X_test = test[ALL_FEATURES]\n",
    "y_test = test['count']\n",
    "\n",
    "sklearn_models = {\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "}\n",
    "if HAS_XGB:\n",
    "    sklearn_models['XGBoost'] = XGBRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbosity=0)\n",
    "if HAS_LGBM:\n",
    "    sklearn_models['LightGBM'] = LGBMRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)\n",
    "if HAS_CATBOOST:\n",
    "    sklearn_models['CatBoost'] = CatBoostRegressor(n_estimators=200, max_depth=6, random_state=42, verbose=0)\n",
    "\n",
    "if RUN_SKLEARN:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SKLEARN BASELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, model in sklearn_models.items():\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = np.clip(model.predict(X_test), 0, None)\n",
    "        elapsed = time.time() - t0\n",
    "        \n",
    "        m = calc_metrics(y_test.values, y_pred, y_test.max())\n",
    "        beach_df = eval_per_beach(test, y_pred, 'beach')\n",
    "        avg_rel = beach_df['RelMAE'].mean()\n",
    "        \n",
    "        all_results.append({\n",
    "            'Model': name, 'Type': 'Sklearn',\n",
    "            'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "            'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "        })\n",
    "        print(f\"  {name:15s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nf_header",
   "metadata": {},
   "source": [
    "## NeuralForecast Models: LSTM, TFT, TiDE\n",
    "\n",
    "**Model Comparison:**\n",
    "\n",
    "| Model | Strengths | Best For |\n",
    "|-------|-----------|----------|\n",
    "| **LSTM** | Classic baseline, proven | Sequential patterns |\n",
    "| **TFT** | Static features, interpretable attention | New beach prediction |\n",
    "| **TiDE** | Fast, lightweight, exogenous support | Production speed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_nf_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.preprocessing import fill_gaps\n",
    "\n",
    "def to_nf_format(df, id_col='beach_folder'):\n",
    "    cols = ['datetime', id_col, 'count'] + ALL_FEATURES\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    nf_df = df[cols].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', 'count': 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def prepare_nf_data(train_df, test_df, freq='h'):\n",
    "    nf_train = to_nf_format(train_df)\n",
    "    nf_test = to_nf_format(test_df)\n",
    "    \n",
    "    # Deduplicate\n",
    "    nf_train = nf_train.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    nf_test = nf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    \n",
    "    # Fill gaps\n",
    "    nf_train = fill_gaps(nf_train, freq=freq)\n",
    "    nf_test = fill_gaps(nf_test, freq=freq)\n",
    "    \n",
    "    # Interpolate\n",
    "    for col in nf_train.select_dtypes(include=[np.number]).columns:\n",
    "        nf_train[col] = nf_train.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "        nf_test[col] = nf_test.groupby('unique_id')[col].transform(\n",
    "            lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    "        )\n",
    "    \n",
    "    # Keep common series\n",
    "    common = set(nf_train['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n",
    "    nf_train = nf_train[nf_train['unique_id'].isin(common)].reset_index(drop=True)\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common)].reset_index(drop=True)\n",
    "    \n",
    "    return nf_train, nf_test, list(common)\n",
    "\n",
    "train_val = pd.concat([train, val])\n",
    "nf_train, nf_test, series_ids = prepare_nf_data(train_val, test)\n",
    "print(f\"NF Data: train={len(nf_train)}, test={len(nf_test)}, series={len(series_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nf_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_models(hist_exog, horizon):\n",
    "    common = dict(\n",
    "        h=horizon,\n",
    "        input_size=INPUT_SIZE,\n",
    "        max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=-1,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        scaler_type='robust',\n",
    "        random_seed=42,\n",
    "        accelerator=ACCELERATOR,\n",
    "        devices=DEVICES,\n",
    "        loss=MAE(),\n",
    "    )\n",
    "    \n",
    "    return [\n",
    "        ('LSTM', LSTM(\n",
    "            hist_exog_list=hist_exog,\n",
    "            encoder_hidden_size=64,\n",
    "            encoder_n_layers=2,\n",
    "            **common\n",
    "        )),\n",
    "        ('TFT', TFT(\n",
    "            hist_exog_list=hist_exog,\n",
    "            hidden_size=64,\n",
    "            n_head=4,\n",
    "            **common\n",
    "        )),\n",
    "        ('TiDE', TiDE(\n",
    "            hist_exog_list=hist_exog,\n",
    "            hidden_size=128,\n",
    "            decoder_output_dim=16,\n",
    "            **common\n",
    "        )),\n",
    "    ]\n",
    "\n",
    "print(\"NF Models: LSTM, TFT, TiDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_nf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_NEURALFORECAST and HAS_NF:\n",
    "    nf_all = pd.concat([nf_train, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    test_horizon = nf_test.groupby('unique_id').size().min()\n",
    "    \n",
    "    # Use smaller horizon for faster training\n",
    "    horizon = min(72, test_horizon)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"NEURALFORECAST (horizon={horizon})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, model in get_nf_models(ALL_FEATURES, horizon=horizon):\n",
    "        print(f\"\\n  {model_name}...\")\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            nf = NeuralForecast(models=[model], freq='h')\n",
    "            \n",
    "            # Use cross_validation for proper evaluation\n",
    "            cv_results = nf.cross_validation(\n",
    "                df=nf_all,\n",
    "                n_windows=1,\n",
    "                step_size=horizon,\n",
    "            )\n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "            \n",
    "            y_true = cv_results['y'].values\n",
    "            y_pred = np.clip(cv_results[pred_col].values, 0, None)\n",
    "            \n",
    "            m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "            \n",
    "            eval_df = cv_results.copy()\n",
    "            eval_df['beach'] = eval_df['unique_id']\n",
    "            beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "            avg_rel = beach_df['RelMAE'].mean() if len(beach_df) > 0 else np.nan\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name, 'Type': 'NeuralForecast',\n",
    "                'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "                'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "            })\n",
    "            print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optuna_header",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimization\n",
    "\n",
    "Optimize:\n",
    "- **XGBoost**: n_estimators, max_depth, learning_rate, subsample, colsample_bytree\n",
    "- **LightGBM**: n_estimators, num_leaves, learning_rate, feature_fraction, bagging_fraction\n",
    "- **CatBoost**: iterations, depth, learning_rate, l2_leaf_reg\n",
    "- **LSTM** (via NeuralForecast): hidden_size, n_layers, learning_rate, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_objective(X_train, y_train, X_val, y_val):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0,\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        return mean_absolute_error(y_val, y_pred)\n",
    "    return objective\n",
    "\n",
    "def create_lgbm_objective(X_train, y_train, X_val, y_val):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        return mean_absolute_error(y_val, y_pred)\n",
    "    return objective\n",
    "\n",
    "def create_catboost_objective(X_train, y_train, X_val, y_val):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'random_state': 42,\n",
    "            'verbose': 0,\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        return mean_absolute_error(y_val, y_pred)\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_optuna_gbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTUNA:\n",
    "    X_tr = train[ALL_FEATURES]\n",
    "    y_tr = train['count']\n",
    "    X_va = val[ALL_FEATURES]\n",
    "    y_va = val['count']\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"OPTUNA OPTIMIZATION (trials={OPTUNA_TRIALS}, timeout={OPTUNA_TIMEOUT}s)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # XGBoost\n",
    "    if HAS_XGB:\n",
    "        print(\"\\n  XGBoost...\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(\n",
    "            create_xgb_objective(X_tr, y_tr, X_va, y_va),\n",
    "            n_trials=OPTUNA_TRIALS,\n",
    "            timeout=OPTUNA_TIMEOUT,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        best_models['XGBoost_Optuna'] = study.best_params\n",
    "        print(f\"    Best MAE: {study.best_value:.2f}\")\n",
    "        print(f\"    Best params: {study.best_params}\")\n",
    "    \n",
    "    # LightGBM\n",
    "    if HAS_LGBM:\n",
    "        print(\"\\n  LightGBM...\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(\n",
    "            create_lgbm_objective(X_tr, y_tr, X_va, y_va),\n",
    "            n_trials=OPTUNA_TRIALS,\n",
    "            timeout=OPTUNA_TIMEOUT,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        best_models['LightGBM_Optuna'] = study.best_params\n",
    "        print(f\"    Best MAE: {study.best_value:.2f}\")\n",
    "        print(f\"    Best params: {study.best_params}\")\n",
    "    \n",
    "    # CatBoost\n",
    "    if HAS_CATBOOST:\n",
    "        print(\"\\n  CatBoost...\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(\n",
    "            create_catboost_objective(X_tr, y_tr, X_va, y_va),\n",
    "            n_trials=OPTUNA_TRIALS,\n",
    "            timeout=OPTUNA_TIMEOUT,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        best_models['CatBoost_Optuna'] = study.best_params\n",
    "        print(f\"    Best MAE: {study.best_value:.2f}\")\n",
    "        print(f\"    Best params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_optuna_gbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTUNA and best_models:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATING OPTUNA-OPTIMIZED MODELS ON TEST SET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train_full = pd.concat([train, val])[ALL_FEATURES]\n",
    "    y_train_full = pd.concat([train, val])['count']\n",
    "    \n",
    "    for name, params in best_models.items():\n",
    "        t0 = time.time()\n",
    "        \n",
    "        if 'XGBoost' in name:\n",
    "            model = XGBRegressor(**params, random_state=42, n_jobs=-1, verbosity=0)\n",
    "        elif 'LightGBM' in name:\n",
    "            model = LGBMRegressor(**params, random_state=42, n_jobs=-1, verbose=-1)\n",
    "        elif 'CatBoost' in name:\n",
    "            model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "        \n",
    "        model.fit(X_train_full, y_train_full)\n",
    "        y_pred = np.clip(model.predict(X_test), 0, None)\n",
    "        elapsed = time.time() - t0\n",
    "        \n",
    "        m = calc_metrics(y_test.values, y_pred, y_test.max())\n",
    "        beach_df = eval_per_beach(test, y_pred, 'beach')\n",
    "        avg_rel = beach_df['RelMAE'].mean()\n",
    "        \n",
    "        all_results.append({\n",
    "            'Model': name, 'Type': 'Optuna',\n",
    "            'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "            'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "        })\n",
    "        print(f\"  {name:20s} | {elapsed:5.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_objective(nf_train, nf_val, hist_exog, horizon):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'h': horizon,\n",
    "            'input_size': INPUT_SIZE,\n",
    "            'encoder_hidden_size': trial.suggest_int('hidden_size', 32, 256),\n",
    "            'encoder_n_layers': trial.suggest_int('n_layers', 1, 3),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'dropout_prob_theta': trial.suggest_float('dropout', 0.0, 0.5),\n",
    "            'max_steps': 200,\n",
    "            'early_stop_patience_steps': -1,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'scaler_type': 'robust',\n",
    "            'random_seed': 42,\n",
    "            'accelerator': ACCELERATOR,\n",
    "            'devices': DEVICES,\n",
    "            'hist_exog_list': hist_exog,\n",
    "            'loss': MAE(),\n",
    "        }\n",
    "        \n",
    "        model = LSTM(**params)\n",
    "        nf = NeuralForecast(models=[model], freq='h')\n",
    "        \n",
    "        nf_all = pd.concat([nf_train, nf_val]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n",
    "        \n",
    "        pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "        y_true = cv_results['y'].values\n",
    "        y_pred = np.clip(cv_results[pred_col].values, 0, None)\n",
    "        \n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    return objective\n",
    "\n",
    "if RUN_OPTUNA and HAS_NF:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTUNA LSTM OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare smaller dataset for LSTM optimization\n",
    "    nf_tr, _, _ = prepare_nf_data(train, val)\n",
    "    nf_va, _, _ = prepare_nf_data(val, test)\n",
    "    \n",
    "    horizon_opt = min(24, nf_va.groupby('unique_id').size().min())\n",
    "    \n",
    "    print(f\"  Optimizing LSTM (horizon={horizon_opt})...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            create_lstm_objective(nf_tr, nf_va, ALL_FEATURES, horizon_opt),\n",
    "            n_trials=min(10, OPTUNA_TRIALS),\n",
    "            timeout=OPTUNA_TIMEOUT,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(f\"    Best MAE: {study.best_value:.2f}\")\n",
    "        print(f\"    Best params: {study.best_params}\")\n",
    "        best_models['LSTM_Optuna'] = study.best_params\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_optuna_lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTUNA and HAS_NF and 'LSTM_Optuna' in best_models:\n",
    "    print(\"\\n  Evaluating LSTM_Optuna on test set...\")\n",
    "    \n",
    "    params = best_models['LSTM_Optuna']\n",
    "    horizon = min(72, nf_test.groupby('unique_id').size().min())\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model = LSTM(\n",
    "        h=horizon,\n",
    "        input_size=INPUT_SIZE,\n",
    "        encoder_hidden_size=params.get('hidden_size', 64),\n",
    "        encoder_n_layers=params.get('n_layers', 2),\n",
    "        learning_rate=params.get('learning_rate', LEARNING_RATE),\n",
    "        dropout_prob_theta=params.get('dropout', 0.1),\n",
    "        max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=-1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        scaler_type='robust',\n",
    "        random_seed=42,\n",
    "        accelerator=ACCELERATOR,\n",
    "        devices=DEVICES,\n",
    "        hist_exog_list=ALL_FEATURES,\n",
    "        loss=MAE(),\n",
    "    )\n",
    "    \n",
    "    nf = NeuralForecast(models=[model], freq='h')\n",
    "    nf_all = pd.concat([nf_train, nf_test]).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    cv_results = nf.cross_validation(df=nf_all, n_windows=1, step_size=horizon)\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    pred_col = [c for c in cv_results.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']][0]\n",
    "    y_true = cv_results['y'].values\n",
    "    y_pred = np.clip(cv_results[pred_col].values, 0, None)\n",
    "    \n",
    "    m = calc_metrics(y_true, y_pred, y_true.max())\n",
    "    eval_df = cv_results.copy()\n",
    "    eval_df['beach'] = eval_df['unique_id']\n",
    "    beach_df = eval_per_beach(eval_df, y_pred, 'beach')\n",
    "    avg_rel = beach_df['RelMAE'].mean()\n",
    "    \n",
    "    all_results.append({\n",
    "        'Model': 'LSTM_Optuna', 'Type': 'Optuna+NF',\n",
    "        'MAE': m['MAE'], 'RMSE': m['RMSE'], 'R2': m['R2'],\n",
    "        'AvgRelMAE': avg_rel, 'Time': elapsed\n",
    "    })\n",
    "    print(f\"    {elapsed:.1f}s | MAE={m['MAE']:.1f} | RelMAE={avg_rel:.1f}% | R2={m['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_header",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "display_df = results_df.sort_values('AvgRelMAE')\n",
    "print(display_df[['Model', 'Type', 'MAE', 'R2', 'AvgRelMAE', 'Time']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Sort by RelMAE\n",
    "    sorted_df = results_df.dropna().sort_values('AvgRelMAE')\n",
    "    colors = {'Sklearn': 'steelblue', 'NeuralForecast': 'coral', 'Optuna': 'green', 'Optuna+NF': 'purple'}\n",
    "    bar_colors = [colors.get(t, 'gray') for t in sorted_df['Type']]\n",
    "    \n",
    "    # RelMAE bar chart\n",
    "    axes[0].barh(sorted_df['Model'], sorted_df['AvgRelMAE'], color=bar_colors)\n",
    "    axes[0].set_xlabel('Avg RelMAE (%)')\n",
    "    axes[0].set_title('Model Performance (lower is better)')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # R2 bar chart\n",
    "    sorted_r2 = results_df.dropna().sort_values('R2', ascending=False)\n",
    "    bar_colors_r2 = [colors.get(t, 'gray') for t in sorted_r2['Type']]\n",
    "    axes[1].barh(sorted_r2['Model'], sorted_r2['R2'], color=bar_colors_r2)\n",
    "    axes[1].set_xlabel('R²')\n",
    "    axes[1].set_title('R² Score (higher is better)')\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=c, label=t) for t, c in colors.items()]\n",
    "    fig.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "valid_results = results_df.dropna(subset=['AvgRelMAE'])\n",
    "if len(valid_results) > 0:\n",
    "    best = valid_results.loc[valid_results['AvgRelMAE'].idxmin()]\n",
    "    print(f\"\\nBest: {best['Model']} ({best['Type']})\")\n",
    "    print(f\"  MAE: {best['MAE']:.2f}\")\n",
    "    print(f\"  RelMAE: {best['AvgRelMAE']:.1f}%\")\n",
    "    print(f\"  R²: {best['R2']:.3f}\")\n",
    "    print(f\"  Time: {best['Time']:.1f}s\")\n",
    "    \n",
    "    if 'Optuna' in best['Model'] and best['Model'].replace('_Optuna', '') in best_models:\n",
    "        print(f\"\\nBest hyperparameters:\")\n",
    "        for k, v in best_models[best['Model']].items():\n",
    "            print(f\"  {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
