{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3892f3a",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — 3-Dataset Comparison\n",
    "\n",
    "Compare model performance across three dataset strategies:\n",
    "1. **Daytime only** — remove night hours (08:00–18:00), sklearn models only\n",
    "2. **Full 24h** — keep all data including noisy night counts\n",
    "3. **Night = 0** — keep 24h but replace night counts with 0\n",
    "\n",
    "Sklearn models run on all 3 datasets. NeuralForecast models run on datasets 2 and 3 only (they need continuous hourly series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc3cd75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T15:07:33.397909Z",
     "start_time": "2026-02-05T15:07:33.396152Z"
    }
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/dataset_comparison\"\n",
    "\n",
    "MAX_STEPS = 500\n",
    "EARLY_STOP_PATIENCE = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 24\n",
    "MAX_GAP_HOURS = 48\n",
    "MIN_SEGMENT_HOURS = 72\n",
    "\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db28435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3.13 install --upgrade pip\u001B[0m\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3.13 install --upgrade pip\u001B[0m\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3.13 install --upgrade pip\u001B[0m\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3.13 install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3.13 install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b62f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps, XGB: True, LGBM: True, CatBoost: True, NF: True\n"
     ]
    }
   ],
   "source": [
    "import copy, json, pickle, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except: HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except: HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except: HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import (\n",
    "        NHITS, NBEATSx, TFT, PatchTST, iTransformer,\n",
    "        TimeMixer, TSMixerx, TCN, TiDE, MLP as NF_MLP, LSTM as NF_LSTM\n",
    "    )\n",
    "    from utilsforecast.preprocessing import fill_gaps as uf_fill_gaps\n",
    "    HAS_NF = True\n",
    "except Exception as e:\n",
    "    print(f\"NeuralForecast error: {e}\")\n",
    "    HAS_NF = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Device: {device}, XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbc9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics defined\n"
     ]
    }
   ],
   "source": [
    "def beach_metrics(y_true, y_pred, max_cap):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    if max_cap == 0 or len(y_true) == 0:\n",
    "        return None\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    rel_mae = (mae / max_cap) * 100\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE (%)': rel_mae}\n",
    "\n",
    "def evaluate_per_beach(df, y_pred, beach_col='beach'):\n",
    "    beach_max = df.groupby(beach_col)['count'].max().to_dict()\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        m = beach_metrics(df.loc[mask, 'count'].values, y_pred[mask.values], beach_max.get(b, 1))\n",
    "        if m:\n",
    "            m['beach'] = b\n",
    "            m['max_count'] = beach_max.get(b, 0)\n",
    "            m['n_samples'] = mask.sum()\n",
    "            results.append(m)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Metrics defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be2b32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169241 JSON files\n",
      "Loaded 169241 valid records\n"
     ]
    }
   ],
   "source": [
    "def load_cache(cache_dir, model=None):\n",
    "    cache_path = Path(cache_dir)\n",
    "    if model:\n",
    "        cache_path = cache_path / model\n",
    "    records = []\n",
    "    json_files = list(cache_path.rglob(\"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with open(jf, 'r') as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    print(f\"Loaded {len(records)} valid records\")\n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {'filename': r.get('filename'), 'beach': r.get('beach') or r.get('name'),\n",
    "               'beach_folder': r.get('beach_folder'), 'datetime': r.get('datetime'), 'count': r.get('count')}\n",
    "        weather = r.get('weather', {})\n",
    "        for k, v in weather.items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, model=COUNTING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87832c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered cameras: 169241 -> 169241\n"
     ]
    }
   ],
   "source": [
    "EXCLUDE_FOLDERS = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols', 'youtube/mCxR-gnn6iA',\n",
    "    'youtube/TbttHwabtfE', 'youtube/WvZWS3D1tHw', 'youtube/Z9F_jN6xpFs', 'youtube/DsrQa_tZoWw']\n",
    "EXCLUDE_PREFIXES = ['ibred', 'ClubNauticSoller', 'Guenthoer']\n",
    "\n",
    "beach_col = 'beach' if 'beach' in df_raw.columns else 'beach_folder'\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw[beach_col].isin(EXCLUDE_FOLDERS)].copy()\n",
    "for prefix in EXCLUDE_PREFIXES:\n",
    "    df_raw = df_raw[~df_raw[beach_col].str.startswith(prefix, na=False)].copy()\n",
    "print(f\"Filtered cameras: {before} -> {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01b9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 39 | Beaches: 38 | Samples: 157384\n",
      "Date range: 2022-07-14 11:00:00 to 2023-01-24 23:00:00\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "    df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "    if 'om_temperature_2m' in df.columns:\n",
    "        df['temp_x_summer'] = df['om_temperature_2m'] * df['is_summer']\n",
    "        df['temp_x_winter'] = df['om_temperature_2m'] * (df['month'].isin([12, 1, 2])).astype(int)\n",
    "    df['weekend_x_summer'] = df['is_weekend'] * df['is_summer']\n",
    "    df['hour_x_summer'] = df['hour'] * df['is_summer']\n",
    "    return df\n",
    "\n",
    "df_all = add_features(df_raw)\n",
    "\n",
    "WEATHER_FEATURES = [c for c in df_all.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_FEATURES = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night',\n",
    "                     'temp_x_summer', 'temp_x_winter', 'weekend_x_summer', 'hour_x_summer']\n",
    "TEMPORAL_FEATURES = [f for f in TEMPORAL_FEATURES if f in df_all.columns]\n",
    "ALL_FEATURES = [f for f in WEATHER_FEATURES + TEMPORAL_FEATURES if f in df_all.columns]\n",
    "\n",
    "df_all = df_all.dropna(subset=ALL_FEATURES + ['count']).copy()\n",
    "beach_max_global = df_all.groupby(beach_col)['count'].max().to_dict()\n",
    "good_beaches = [b for b, m in beach_max_global.items() if m > 20]\n",
    "df_all = df_all[df_all[beach_col].isin(good_beaches)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Features: {len(ALL_FEATURES)} | Beaches: {len(good_beaches)} | Samples: {len(df_all)}\")\n",
    "print(f\"Date range: {df_all['datetime'].min()} to {df_all['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78fe73",
   "metadata": {},
   "source": [
    "## Create 3 Datasets\n",
    "\n",
    "1. **Daytime** — filter to 08:00–18:00\n",
    "2. **Full 24h** — keep everything (noisy night counts)\n",
    "3. **Night = 0** — keep 24h, set count=0 when hour ∈ [20:00–06:00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "371f1658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daytime     :  87492 samples | night%: 0.0% | mean count: 46.2\n",
      "Full 24h    : 157384 samples | night%: 44.4% | mean count: 45.5\n",
      "Night=0     : 157384 samples | night%: 44.4% | mean count: 25.7\n"
     ]
    }
   ],
   "source": [
    "ds_daytime = df_all[~df_all['is_night'].astype(bool)].copy().reset_index(drop=True)\n",
    "ds_full24h = df_all.copy()\n",
    "ds_night0 = df_all.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "datasets = {\n",
    "    'Daytime': ds_daytime,\n",
    "    'Full 24h': ds_full24h,\n",
    "    'Night=0': ds_night0,\n",
    "}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    night_pct = ds['is_night'].mean() * 100 if 'is_night' in ds.columns else 0\n",
    "    print(f\"{name:12s}: {len(ds):6d} samples | night%: {night_pct:.1f}% | mean count: {ds['count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc0325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daytime     : train=61244, val=13124, test=13124\n",
      "Full 24h    : train=110168, val=23608, test=23608\n",
      "Night=0     : train=110168, val=23608, test=23608\n"
     ]
    }
   ],
   "source": [
    "def split_data(df, features, train_frac=0.7, val_frac=0.15):\n",
    "    X = df[features]\n",
    "    y = df['count']\n",
    "    n = len(X)\n",
    "    t1, t2 = int(n * train_frac), int(n * (train_frac + val_frac))\n",
    "    return {\n",
    "        'X_train': X.iloc[:t1], 'X_val': X.iloc[t1:t2], 'X_test': X.iloc[t2:],\n",
    "        'y_train': y.iloc[:t1], 'y_val': y.iloc[t1:t2], 'y_test': y.iloc[t2:],\n",
    "        'df_train': df.iloc[:t1], 'df_val': df.iloc[t1:t2], 'df_test': df.iloc[t2:],\n",
    "    }\n",
    "\n",
    "splits = {}\n",
    "for name, ds in datasets.items():\n",
    "    splits[name] = split_data(ds, ALL_FEATURES)\n",
    "    s = splits[name]\n",
    "    print(f\"{name:12s}: train={len(s['X_train'])}, val={len(s['X_val'])}, test={len(s['X_test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00b8d7",
   "metadata": {},
   "source": [
    "## Sklearn Models — All 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffb6169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn models: ['Lasso', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']\n"
     ]
    }
   ],
   "source": [
    "def get_sklearn_models():\n",
    "    models = {\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=200, max_depth=8, random_state=42),\n",
    "    }\n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, n_jobs=-1, verbosity=0)\n",
    "    if HAS_LGBM:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    if HAS_CATBOOST:\n",
    "        models['CatBoost'] = CatBoostRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    return models\n",
    "\n",
    "print(f\"Sklearn models: {list(get_sklearn_models().keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee563817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SKLEARN — Daytime (13124 test samples)\n",
      "======================================================================\n",
      "  Lasso                |   0.6s | MAE:   51.33 | RelMAE: 122.38% | R2: -0.1751\n",
      "  RandomForest         |  20.1s | MAE:   40.16 | RelMAE:  85.46% | R2: 0.0070\n",
      "  GradientBoosting     | 168.8s | MAE:   60.43 | RelMAE: 153.74% | R2: -0.7950\n",
      "  XGBoost              |   2.2s | MAE:   45.28 | RelMAE: 101.83% | R2: -0.1385\n",
      "  LightGBM             |   2.2s | MAE:   33.08 | RelMAE:  61.58% | R2: 0.1207\n",
      "  CatBoost             |   1.6s | MAE:   32.80 | RelMAE:  61.31% | R2: 0.1318\n",
      "\n",
      "======================================================================\n",
      "SKLEARN — Full 24h (23608 test samples)\n",
      "======================================================================\n",
      "  Lasso                |   1.2s | MAE:   46.91 | RelMAE: 100.98% | R2: -0.0382\n",
      "  RandomForest         |  36.8s | MAE:   40.53 | RelMAE:  86.23% | R2: 0.0621\n",
      "  GradientBoosting     | 305.3s | MAE:   56.38 | RelMAE: 138.48% | R2: -0.4825\n",
      "  XGBoost              |   1.7s | MAE:   50.82 | RelMAE: 121.92% | R2: -0.2117\n",
      "  LightGBM             |   2.4s | MAE:   34.08 | RelMAE:  64.72% | R2: 0.1744\n",
      "  CatBoost             |   2.5s | MAE:   33.34 | RelMAE:  60.98% | R2: 0.1578\n",
      "\n",
      "======================================================================\n",
      "SKLEARN — Night=0 (23608 test samples)\n",
      "======================================================================\n",
      "  Lasso                |   1.7s | MAE:   26.49 | RelMAE:  55.73% | R2: 0.1622\n",
      "  RandomForest         |  23.8s | MAE:   22.03 | RelMAE:  45.27% | R2: 0.1952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model \u001B[38;5;129;01min\u001B[39;00m get_sklearn_models().items():\n\u001B[32m     14\u001B[39m     start = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_trainval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_trainval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m     train_time = time.time() - start\n\u001B[32m     18\u001B[39m     y_pred = np.clip(model.predict(s[\u001B[33m'\u001B[39m\u001B[33mX_test\u001B[39m\u001B[33m'\u001B[39m]), \u001B[32m0\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1336\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1329\u001B[39m     estimator._validate_params()\n\u001B[32m   1331\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1332\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1333\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1334\u001B[39m     )\n\u001B[32m   1335\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:795\u001B[39m, in \u001B[36mBaseGradientBoosting.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, monitor)\u001B[39m\n\u001B[32m    792\u001B[39m     \u001B[38;5;28mself\u001B[39m._resize_state()\n\u001B[32m    794\u001B[39m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m795\u001B[39m n_stages = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    799\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    801\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    803\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    804\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    805\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    806\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    808\u001B[39m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[32m    809\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_stages != \u001B[38;5;28mself\u001B[39m.estimators_.shape[\u001B[32m0\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:891\u001B[39m, in \u001B[36mBaseGradientBoosting._fit_stages\u001B[39m\u001B[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[39m\n\u001B[32m    884\u001B[39m         initial_loss = factor * \u001B[38;5;28mself\u001B[39m._loss(\n\u001B[32m    885\u001B[39m             y_true=y_oob_masked,\n\u001B[32m    886\u001B[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001B[32m    887\u001B[39m             sample_weight=sample_weight_oob_masked,\n\u001B[32m    888\u001B[39m         )\n\u001B[32m    890\u001B[39m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m891\u001B[39m raw_predictions = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    896\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    897\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    903\u001B[39m \u001B[38;5;66;03m# track loss\u001B[39;00m\n\u001B[32m    904\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:497\u001B[39m, in \u001B[36mBaseGradientBoosting._fit_stage\u001B[39m\u001B[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[39m\n\u001B[32m    494\u001B[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001B[32m    496\u001B[39m X = X_csc \u001B[38;5;28;01mif\u001B[39;00m X_csc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m \u001B[43mtree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_g_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m    499\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[32m    502\u001B[39m X_for_tree_update = X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1336\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1329\u001B[39m     estimator._validate_params()\n\u001B[32m   1331\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1332\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1333\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1334\u001B[39m     )\n\u001B[32m   1335\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/tree/_classes.py:1407\u001B[39m, in \u001B[36mDecisionTreeRegressor.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, check_input)\u001B[39m\n\u001B[32m   1377\u001B[39m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   1378\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight=\u001B[38;5;28;01mNone\u001B[39;00m, check_input=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m   1379\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[32m   1380\u001B[39m \n\u001B[32m   1381\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1404\u001B[39m \u001B[33;03m        Fitted estimator.\u001B[39;00m\n\u001B[32m   1405\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1407\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1408\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1409\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1410\u001B[39m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1411\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1412\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/tree/_classes.py:475\u001B[39m, in \u001B[36mBaseDecisionTree._fit\u001B[39m\u001B[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[39m\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    465\u001B[39m     builder = BestFirstTreeBuilder(\n\u001B[32m    466\u001B[39m         splitter,\n\u001B[32m    467\u001B[39m         min_samples_split,\n\u001B[32m   (...)\u001B[39m\u001B[32m    472\u001B[39m         \u001B[38;5;28mself\u001B[39m.min_impurity_decrease,\n\u001B[32m    473\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m \u001B[43mbuilder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_outputs_ == \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    478\u001B[39m     \u001B[38;5;28mself\u001B[39m.n_classes_ = \u001B[38;5;28mself\u001B[39m.n_classes_[\u001B[32m0\u001B[39m]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "for ds_name, ds in datasets.items():\n",
    "    s = splits[ds_name]\n",
    "    X_trainval = pd.concat([s['X_train'], s['X_val']])\n",
    "    y_trainval = pd.concat([s['y_train'], s['y_val']])\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SKLEARN — {ds_name} ({len(s['X_test'])} test samples)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    for model_name, model in get_sklearn_models().items():\n",
    "        start = time.time()\n",
    "        model.fit(X_trainval, y_trainval)\n",
    "        train_time = time.time() - start\n",
    "\n",
    "        y_pred = np.clip(model.predict(s['X_test']), 0, None)\n",
    "        y_true = s['y_test'].values\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        beach_df = evaluate_per_beach(s['df_test'], y_pred, beach_col)\n",
    "        beach_df['model'] = model_name\n",
    "        beach_df['dataset'] = ds_name\n",
    "        beach_df['model_type'] = 'Sklearn'\n",
    "        all_beach_results.append(beach_df)\n",
    "\n",
    "        avg_rel_mae = beach_df['RelMAE (%)'].mean()\n",
    "\n",
    "        all_results.append({\n",
    "            'Model': model_name, 'Dataset': ds_name, 'Type': 'Sklearn',\n",
    "            'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Avg RelMAE (%)': avg_rel_mae,\n",
    "            'Time (s)': train_time, 'N_test': len(y_true)\n",
    "        })\n",
    "        print(f\"  {model_name:20s} | {train_time:5.1f}s | MAE: {mae:7.2f} | RelMAE: {avg_rel_mae:6.2f}% | R2: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nSklearn done: {len(all_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f2e90",
   "metadata": {},
   "source": [
    "## NeuralForecast Models — Full 24h and Night=0 Only\n",
    "\n",
    "Daytime dataset has 14h gaps every night — NF models need continuous hourly series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nf_format(df, features, target='count', id_col='beach'):\n",
    "    nf_df = df[['datetime', id_col, target] + features].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', target: 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def split_at_long_gaps(nf_df, max_gap_hours=MAX_GAP_HOURS, min_segment_hours=MIN_SEGMENT_HOURS):\n",
    "    result = []\n",
    "    for uid in nf_df['unique_id'].unique():\n",
    "        s = nf_df[nf_df['unique_id'] == uid].sort_values('ds').copy()\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        split_points = gaps[gaps > max_gap_hours].index.tolist()\n",
    "\n",
    "        segments = []\n",
    "        prev = s.index[0]\n",
    "        for sp in split_points:\n",
    "            segments.append(s.loc[prev:s.index[s.index.get_loc(sp) - 1]])\n",
    "            prev = sp\n",
    "        segments.append(s.loc[prev:])\n",
    "\n",
    "        for i, seg in enumerate(segments):\n",
    "            if len(seg) >= min_segment_hours:\n",
    "                seg = seg.copy()\n",
    "                seg['unique_id'] = f\"{uid}__seg{i}\" if len(segments) > 1 else uid\n",
    "                result.append(seg)\n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "def prepare_nf_data(ds_name):\n",
    "    s = splits[ds_name]\n",
    "    trainval = pd.concat([s['df_train'], s['df_val']])\n",
    "    nf_trainval = to_nf_format(trainval, ALL_FEATURES, id_col=beach_col)\n",
    "    nf_test = to_nf_format(s['df_test'], ALL_FEATURES, id_col=beach_col)\n",
    "\n",
    "    nf_trainval = split_at_long_gaps(nf_trainval)\n",
    "    nf_test = split_at_long_gaps(nf_test)\n",
    "\n",
    "    nf_trainval = nf_trainval.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "    nf_test = nf_test.groupby(['unique_id', 'ds']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    nf_trainval = uf_fill_gaps(nf_trainval, freq='h')\n",
    "    nf_test = uf_fill_gaps(nf_test, freq='h')\n",
    "\n",
    "    for col in ['y'] + ALL_FEATURES:\n",
    "        if col in nf_trainval.columns:\n",
    "            nf_trainval[col] = nf_trainval.groupby('unique_id')[col].ffill().bfill()\n",
    "        if col in nf_test.columns:\n",
    "            nf_test[col] = nf_test.groupby('unique_id')[col].ffill().bfill()\n",
    "\n",
    "    common = set(nf_trainval['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n",
    "    nf_trainval = nf_trainval[nf_trainval['unique_id'].isin(common)]\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common)]\n",
    "\n",
    "    return nf_trainval, nf_test, len(common)\n",
    "\n",
    "nf_datasets = {}\n",
    "for ds_name in ['Full 24h', 'Night=0']:\n",
    "    tv, te, ns = prepare_nf_data(ds_name)\n",
    "    nf_datasets[ds_name] = {'trainval': tv, 'test': te, 'n_series': ns}\n",
    "    print(f\"{ds_name}: train+val={len(tv)}, test={len(te)}, series={ns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_models(n_series, hist_exog):\n",
    "    common = dict(\n",
    "        h=1, input_size=INPUT_SIZE, max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=EARLY_STOP_PATIENCE, scaler_type='robust',\n",
    "        learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE,\n",
    "        val_check_steps=50, random_seed=42, start_padding_enabled=True,\n",
    "    )\n",
    "    return [\n",
    "        ('LSTM', NF_LSTM(hist_exog_list=hist_exog, encoder_n_layers=2, encoder_hidden_size=128, decoder_hidden_size=128, decoder_layers=2, **common)),\n",
    "        ('NHITS', NHITS(hist_exog_list=hist_exog, **common)),\n",
    "        ('NBEATSx', NBEATSx(hist_exog_list=hist_exog, stack_types=['identity', 'identity', 'identity'], **common)),\n",
    "        ('TFT', TFT(hist_exog_list=hist_exog, hidden_size=64, **common)),\n",
    "        ('TCN', TCN(hist_exog_list=hist_exog, **common)),\n",
    "        ('TiDE', TiDE(hist_exog_list=hist_exog, **common)),\n",
    "        ('NF_MLP', NF_MLP(hist_exog_list=hist_exog, **common)),\n",
    "        ('PatchTST', PatchTST(**common)),\n",
    "        ('TimeMixer', TimeMixer(n_series=n_series, **common)),\n",
    "        ('TSMixerx', TSMixerx(n_series=n_series, **common)),\n",
    "        ('iTransformer', iTransformer(n_series=n_series, **common)),\n",
    "    ]\n",
    "\n",
    "if HAS_NF:\n",
    "    for ds_name, nf_data in nf_datasets.items():\n",
    "        tv, te, ns = nf_data['trainval'], nf_data['test'], nf_data['n_series']\n",
    "        min_len = tv.groupby('unique_id').size().min()\n",
    "        val_size = max(24, min(min_len // 5, 200))\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"NEURALFORECAST — {ds_name} (series={ns}, val_size={val_size})\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        for model_name, model in get_nf_models(ns, ALL_FEATURES):\n",
    "            print(f\"\\n  {model_name}...\")\n",
    "            try:\n",
    "                start = time.time()\n",
    "                nf = NeuralForecast(models=[model], freq='h')\n",
    "                nf.fit(df=tv, val_size=val_size)\n",
    "                train_time = time.time() - start\n",
    "\n",
    "                preds = nf.predict(df=tv).reset_index()\n",
    "                pred_col = [c for c in preds.columns if c not in ['unique_id', 'ds']][0]\n",
    "                merged = te.merge(preds[['unique_id', 'ds', pred_col]], on=['unique_id', 'ds'], how='inner')\n",
    "\n",
    "                if len(merged) == 0:\n",
    "                    raise ValueError(\"No matching predictions\")\n",
    "\n",
    "                y_pred = np.clip(merged[pred_col].values, 0, None)\n",
    "                y_true = merged['y'].values\n",
    "\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "                eval_df = merged.rename(columns={'unique_id': beach_col, 'y': 'count'}).copy()\n",
    "                beach_df = evaluate_per_beach(eval_df, y_pred, beach_col)\n",
    "                beach_df['model'] = model_name\n",
    "                beach_df['dataset'] = ds_name\n",
    "                beach_df['model_type'] = 'NeuralForecast'\n",
    "                all_beach_results.append(beach_df)\n",
    "\n",
    "                avg_rel_mae = beach_df['RelMAE (%)'].mean()\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Avg RelMAE (%)': avg_rel_mae,\n",
    "                    'Time (s)': train_time, 'N_test': len(merged)\n",
    "                })\n",
    "                print(f\"    {train_time:.1f}s | MAE: {mae:.2f} | RelMAE: {avg_rel_mae:.2f}% | R2: {r2:.4f} | matched: {len(merged)}/{len(te)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: {e}\")\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan, 'Avg RelMAE (%)': np.nan,\n",
    "                    'Time (s)': np.nan, 'N_test': 0\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc38e96",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d55189",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results).sort_values(['Dataset', 'Avg RelMAE (%)'])\n",
    "beach_results_df = pd.concat(all_beach_results, ignore_index=True)\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'results_comparison.csv', index=False)\n",
    "beach_results_df.to_csv(save_dir / 'beach_results_comparison.csv', index=False)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].copy()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {ds_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(sub[['Model', 'Type', 'MAE', 'RMSE', 'R2', 'Avg RelMAE (%)', 'Time (s)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2eede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = results_df.pivot_table(index='Model', columns='Dataset', values='Avg RelMAE (%)', aggfunc='first')\n",
    "pivot = pivot.sort_values(pivot.columns[0], na_position='last')\n",
    "print(\"\\nRelMAE (%) — Model × Dataset:\")\n",
    "print(pivot.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e5b3",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f35a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "pivot_plot = results_df.pivot_table(index='Model', columns='Dataset', values='Avg RelMAE (%)', aggfunc='first')\n",
    "models_sorted = pivot_plot.mean(axis=1).sort_values().index\n",
    "pivot_plot = pivot_plot.loc[models_sorted]\n",
    "\n",
    "x = np.arange(len(pivot_plot))\n",
    "width = 0.25\n",
    "ds_names = list(datasets.keys())\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
    "\n",
    "for i, ds in enumerate(ds_names):\n",
    "    if ds in pivot_plot.columns:\n",
    "        vals = pivot_plot[ds].values\n",
    "        bars = ax.bar(x + i * width, vals, width, label=ds, color=colors[i], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Avg RelMAE (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Across Datasets — Relative MAE (lower is better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(pivot_plot.index, rotation=45, ha='right')\n",
    "ax.legend(title='Dataset')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'grouped_bar_relmae.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(datasets), figsize=(20, 8), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = beach_results_df[beach_results_df['dataset'] == ds_name]\n",
    "    if len(sub) == 0:\n",
    "        axes[idx].set_title(f'{ds_name}\\n(no NF results)')\n",
    "        continue\n",
    "    heatmap_data = sub.pivot_table(index='beach', columns='model', values='RelMAE (%)', aggfunc='first')\n",
    "    heatmap_data = heatmap_data.reindex(columns=heatmap_data.mean().sort_values().index)\n",
    "\n",
    "    im = axes[idx].imshow(heatmap_data.values, cmap='RdYlGn_r', aspect='auto')\n",
    "    axes[idx].set_xticks(range(len(heatmap_data.columns)))\n",
    "    axes[idx].set_xticklabels(heatmap_data.columns, rotation=45, ha='right', fontsize=8)\n",
    "    if idx == 0:\n",
    "        axes[idx].set_yticks(range(len(heatmap_data.index)))\n",
    "        axes[idx].set_yticklabels(heatmap_data.index, fontsize=8)\n",
    "    axes[idx].set_title(f'{ds_name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "    for i in range(len(heatmap_data.index)):\n",
    "        for j in range(len(heatmap_data.columns)):\n",
    "            val = heatmap_data.values[i, j]\n",
    "            if not np.isnan(val):\n",
    "                axes[idx].text(j, i, f'{val:.0f}', ha='center', va='center', fontsize=6)\n",
    "\n",
    "fig.colorbar(im, ax=axes, label='RelMAE (%)', shrink=0.6)\n",
    "fig.suptitle('Per-Beach Relative MAE (%) — Heatmap by Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'beach_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "pivot_r2 = results_df.pivot_table(index='Model', columns='Dataset', values='R2', aggfunc='first')\n",
    "pivot_r2 = pivot_r2.loc[models_sorted]\n",
    "\n",
    "x = np.arange(len(pivot_r2))\n",
    "for i, ds in enumerate(ds_names):\n",
    "    if ds in pivot_r2.columns:\n",
    "        vals = pivot_r2[ds].values\n",
    "        ax.bar(x + i * width, vals, width, label=ds, color=colors[i], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('R²', fontsize=12)\n",
    "ax.set_title('R² Score Across Datasets (higher is better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(pivot_r2.index, rotation=45, ha='right')\n",
    "ax.legend(title='Dataset')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'grouped_bar_r2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].dropna()\n",
    "    ax = axes[idx]\n",
    "    for _, row in sub.iterrows():\n",
    "        c = '#2196F3' if row['Type'] == 'Sklearn' else '#FF5722'\n",
    "        ax.scatter(row['Time (s)'], row['Avg RelMAE (%)'], s=100, c=c, edgecolor='black', zorder=5)\n",
    "        ax.annotate(row['Model'], (row['Time (s)'], row['Avg RelMAE (%)']),\n",
    "                   fontsize=7, ha='left', va='bottom', xytext=(4, 4), textcoords='offset points')\n",
    "    ax.set_xlabel('Training Time (s)')\n",
    "    ax.set_title(f'{ds_name}', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel('Avg RelMAE (%)')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "fig.legend(handles=[Patch(color='#2196F3', label='Sklearn'), Patch(color='#FF5722', label='NeuralForecast')],\n",
    "           loc='upper right', fontsize=10)\n",
    "fig.suptitle('Efficiency: RelMAE vs Training Time (lower-left is better)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'efficiency_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_both = results_df.groupby('Model').filter(lambda x: len(x) >= 2)['Model'].unique()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model_name in sorted(models_both):\n",
    "    sub = results_df[results_df['Model'] == model_name].sort_values('Dataset')\n",
    "    marker = 'o' if sub['Type'].iloc[0] == 'Sklearn' else 's'\n",
    "    ax.plot(sub['Dataset'], sub['Avg RelMAE (%)'], marker=marker, label=model_name, linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Dataset Strategy', fontsize=12)\n",
    "ax.set_ylabel('Avg RelMAE (%)', fontsize=12)\n",
    "ax.set_title('How Dataset Strategy Affects Each Model', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'dataset_impact.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c406e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = beach_results_df[beach_results_df['dataset'] == ds_name]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best_per_beach = sub.loc[sub.groupby('beach')['RelMAE (%)'].idxmin()]\n",
    "    counts = best_per_beach['model'].value_counts()\n",
    "\n",
    "    axes[idx].barh(counts.index, counts.values, color='steelblue', edgecolor='black')\n",
    "    axes[idx].set_xlabel('# Beaches where best')\n",
    "    axes[idx].set_title(f'{ds_name}', fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "fig.suptitle('Which Model Wins on Most Beaches? (by RelMAE)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'best_per_beach.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c79d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].dropna()\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best = sub.loc[sub['Avg RelMAE (%)'].idxmin()]\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    print(f\"  Best: {best['Model']} ({best['Type']}) — RelMAE: {best['Avg RelMAE (%)']:.2f}%, R2: {best['R2']:.4f}\")\n",
    "    sk = sub[sub['Type'] == 'Sklearn']\n",
    "    if len(sk) > 0:\n",
    "        best_sk = sk.loc[sk['Avg RelMAE (%)'].idxmin()]\n",
    "        print(f\"  Best Sklearn: {best_sk['Model']} — RelMAE: {best_sk['Avg RelMAE (%)']:.2f}%\")\n",
    "    nf = sub[sub['Type'] == 'NeuralForecast']\n",
    "    if len(nf) > 0:\n",
    "        best_nf = nf.loc[nf['Avg RelMAE (%)'].idxmin()]\n",
    "        print(f\"  Best NF: {best_nf['Model']} — RelMAE: {best_nf['Avg RelMAE (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal results: {len(results_df)}\")\n",
    "print(f\"Beach-level results: {len(beach_results_df)}\")\n",
    "print(f\"Saved to: {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
