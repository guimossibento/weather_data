{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beach Crowd Prediction — 3-Dataset Comparison\n",
    "\n",
    "Compare model performance across three dataset strategies:\n",
    "1. **Daytime only** — remove night hours (08:00–18:00), sklearn models only\n",
    "2. **Full 24h** — keep all data including noisy night counts\n",
    "3. **Night = 0** — keep 24h but replace night counts with 0\n",
    "\n",
    "Sklearn models run on all 3 datasets. NeuralForecast models run on datasets 2 and 3 only (they need continuous hourly series)."
   ],
   "id": "a3892f3a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T15:07:33.397909Z",
     "start_time": "2026-02-05T15:07:33.396152Z"
    }
   },
   "source": [
    "CACHE_DIR = \"cache/predictions\"\n",
    "COUNTING_MODEL = \"bayesian_vgg19\"\n",
    "SAVE_DIR = \"models/dataset_comparison\"\n",
    "\n",
    "MAX_STEPS = 500\n",
    "EARLY_STOP_PATIENCE = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_SIZE = 24\n",
    "MAX_GAP_HOURS = 48\n",
    "MIN_SEGMENT_HOURS = 72\n",
    "\n",
    "NIGHT_START = 20\n",
    "NIGHT_END = 6"
   ],
   "id": "bdc3cd75",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"neuralforecast\", \"xgboost\", \"lightgbm\", \"catboost\", \"utilsforecast\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "print(\"Done!\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "6db28435"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import copy, json, pickle, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except: HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except: HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except: HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import (\n",
    "        NHITS, NBEATSx, TFT, PatchTST, iTransformer,\n",
    "        TimeMixer, TSMixerx, TCN, TiDE, MLP as NF_MLP, LSTM as NF_LSTM\n",
    "    )\n",
    "    from utilsforecast.preprocessing import fill_gaps as uf_fill_gaps\n",
    "    HAS_NF = True\n",
    "except Exception as e:\n",
    "    print(f\"NeuralForecast error: {e}\")\n",
    "    HAS_NF = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Device: {device}, XGB: {HAS_XGB}, LGBM: {HAS_LGBM}, CatBoost: {HAS_CATBOOST}, NF: {HAS_NF}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "1b62f0cb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def beach_metrics(y_true, y_pred, max_cap):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    if max_cap == 0 or len(y_true) == 0:\n",
    "        return None\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    rel_mae = (mae / max_cap) * 100\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'RelMAE (%)': rel_mae}\n",
    "\n",
    "def evaluate_per_beach(df, y_pred, beach_col='beach'):\n",
    "    beach_max = df.groupby(beach_col)['count'].max().to_dict()\n",
    "    results = []\n",
    "    for b in df[beach_col].unique():\n",
    "        mask = df[beach_col] == b\n",
    "        if mask.sum() < 3:\n",
    "            continue\n",
    "        m = beach_metrics(df.loc[mask, 'count'].values, y_pred[mask.values], beach_max.get(b, 1))\n",
    "        if m:\n",
    "            m['beach'] = b\n",
    "            m['max_count'] = beach_max.get(b, 0)\n",
    "            m['n_samples'] = mask.sum()\n",
    "            results.append(m)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Metrics defined\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "9dbc9e46"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_cache(cache_dir, model=None):\n",
    "    cache_path = Path(cache_dir)\n",
    "    if model:\n",
    "        cache_path = cache_path / model\n",
    "    records = []\n",
    "    json_files = list(cache_path.rglob(\"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with open(jf, 'r') as f:\n",
    "                r = json.load(f)\n",
    "            if 'error' not in r:\n",
    "                records.append(r)\n",
    "        except: pass\n",
    "    print(f\"Loaded {len(records)} valid records\")\n",
    "    rows = []\n",
    "    for r in records:\n",
    "        row = {'filename': r.get('filename'), 'beach': r.get('beach') or r.get('name'),\n",
    "               'beach_folder': r.get('beach_folder'), 'datetime': r.get('datetime'), 'count': r.get('count')}\n",
    "        weather = r.get('weather', {})\n",
    "        for k, v in weather.items():\n",
    "            row[k] = v\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_cache(CACHE_DIR, model=COUNTING_MODEL)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "8be2b32a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EXCLUDE_FOLDERS = ['livecampro/001', 'livecampro/011', 'livecampro/018', 'livecampro/021',\n",
    "    'livecampro/030', 'livecampro/039', 'livecampro/070', 'MultimediaTres/PortAndratx',\n",
    "    'SeeTheWorld/mallorca_pancam', 'skyline/es-pujols', 'youtube/mCxR-gnn6iA',\n",
    "    'youtube/TbttHwabtfE', 'youtube/WvZWS3D1tHw', 'youtube/Z9F_jN6xpFs', 'youtube/DsrQa_tZoWw']\n",
    "EXCLUDE_PREFIXES = ['ibred', 'ClubNauticSoller', 'Guenthoer']\n",
    "\n",
    "beach_col = 'beach' if 'beach' in df_raw.columns else 'beach_folder'\n",
    "\n",
    "before = len(df_raw)\n",
    "df_raw = df_raw[~df_raw[beach_col].isin(EXCLUDE_FOLDERS)].copy()\n",
    "for prefix in EXCLUDE_PREFIXES:\n",
    "    df_raw = df_raw[~df_raw[beach_col].str.startswith(prefix, na=False)].copy()\n",
    "print(f\"Filtered cameras: {before} -> {len(df_raw)}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "87832c1c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "    df['is_night'] = ((df['hour'] >= NIGHT_START) | (df['hour'] <= NIGHT_END)).astype(int)\n",
    "    if 'om_temperature_2m' in df.columns:\n",
    "        df['temp_x_summer'] = df['om_temperature_2m'] * df['is_summer']\n",
    "        df['temp_x_winter'] = df['om_temperature_2m'] * (df['month'].isin([12, 1, 2])).astype(int)\n",
    "    df['weekend_x_summer'] = df['is_weekend'] * df['is_summer']\n",
    "    df['hour_x_summer'] = df['hour'] * df['is_summer']\n",
    "    return df\n",
    "\n",
    "df_all = add_features(df_raw)\n",
    "\n",
    "WEATHER_FEATURES = [c for c in df_all.columns if c.startswith('ae_') or c.startswith('om_')]\n",
    "TEMPORAL_FEATURES = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_summer', 'is_night',\n",
    "                     'temp_x_summer', 'temp_x_winter', 'weekend_x_summer', 'hour_x_summer']\n",
    "TEMPORAL_FEATURES = [f for f in TEMPORAL_FEATURES if f in df_all.columns]\n",
    "ALL_FEATURES = [f for f in WEATHER_FEATURES + TEMPORAL_FEATURES if f in df_all.columns]\n",
    "\n",
    "df_all = df_all.dropna(subset=ALL_FEATURES + ['count']).copy()\n",
    "beach_max_global = df_all.groupby(beach_col)['count'].max().to_dict()\n",
    "good_beaches = [b for b, m in beach_max_global.items() if m > 20]\n",
    "df_all = df_all[df_all[beach_col].isin(good_beaches)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Features: {len(ALL_FEATURES)} | Beaches: {len(good_beaches)} | Samples: {len(df_all)}\")\n",
    "print(f\"Date range: {df_all['datetime'].min()} to {df_all['datetime'].max()}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "a01b9951"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3 Datasets\n",
    "\n",
    "1. **Daytime** — filter to 08:00–18:00\n",
    "2. **Full 24h** — keep everything (noisy night counts)\n",
    "3. **Night = 0** — keep 24h, set count=0 when hour ∈ [20:00–06:00]"
   ],
   "id": "6a78fe73"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds_daytime = df_all[~df_all['is_night'].astype(bool)].copy().reset_index(drop=True)\n",
    "ds_full24h = df_all.copy()\n",
    "ds_night0 = df_all.copy()\n",
    "ds_night0.loc[ds_night0['is_night'] == 1, 'count'] = 0.0\n",
    "\n",
    "datasets = {\n",
    "    'Daytime': ds_daytime,\n",
    "    'Full 24h': ds_full24h,\n",
    "    'Night=0': ds_night0,\n",
    "}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    night_pct = ds['is_night'].mean() * 100 if 'is_night' in ds.columns else 0\n",
    "    print(f\"{name:12s}: {len(ds):6d} samples | night%: {night_pct:.1f}% | mean count: {ds['count'].mean():.1f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "371f1658"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def split_data(df, features, train_frac=0.7, val_frac=0.15):\n",
    "    X = df[features]\n",
    "    y = df['count']\n",
    "    n = len(X)\n",
    "    t1, t2 = int(n * train_frac), int(n * (train_frac + val_frac))\n",
    "    return {\n",
    "        'X_train': X.iloc[:t1], 'X_val': X.iloc[t1:t2], 'X_test': X.iloc[t2:],\n",
    "        'y_train': y.iloc[:t1], 'y_val': y.iloc[t1:t2], 'y_test': y.iloc[t2:],\n",
    "        'df_train': df.iloc[:t1], 'df_val': df.iloc[t1:t2], 'df_test': df.iloc[t2:],\n",
    "    }\n",
    "\n",
    "splits = {}\n",
    "for name, ds in datasets.items():\n",
    "    splits[name] = split_data(ds, ALL_FEATURES)\n",
    "    s = splits[name]\n",
    "    print(f\"{name:12s}: train={len(s['X_train'])}, val={len(s['X_val'])}, test={len(s['X_test'])}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "fdc0325c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Models — All 3 Datasets"
   ],
   "id": "3a00b8d7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_sklearn_models():\n",
    "    models = {\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=200, max_depth=8, random_state=42),\n",
    "    }\n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, n_jobs=-1, verbosity=0)\n",
    "    if HAS_LGBM:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    if HAS_CATBOOST:\n",
    "        models['CatBoost'] = CatBoostRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    return models\n",
    "\n",
    "print(f\"Sklearn models: {list(get_sklearn_models().keys())}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "7ffb6169"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_results = []\n",
    "all_beach_results = []\n",
    "\n",
    "for ds_name, ds in datasets.items():\n",
    "    s = splits[ds_name]\n",
    "    X_trainval = pd.concat([s['X_train'], s['X_val']])\n",
    "    y_trainval = pd.concat([s['y_train'], s['y_val']])\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SKLEARN — {ds_name} ({len(s['X_test'])} test samples)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    for model_name, model in get_sklearn_models().items():\n",
    "        start = time.time()\n",
    "        model.fit(X_trainval, y_trainval)\n",
    "        train_time = time.time() - start\n",
    "\n",
    "        y_pred = np.clip(model.predict(s['X_test']), 0, None)\n",
    "        y_true = s['y_test'].values\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        beach_df = evaluate_per_beach(s['df_test'], y_pred, beach_col)\n",
    "        beach_df['model'] = model_name\n",
    "        beach_df['dataset'] = ds_name\n",
    "        beach_df['model_type'] = 'Sklearn'\n",
    "        all_beach_results.append(beach_df)\n",
    "\n",
    "        avg_rel_mae = beach_df['RelMAE (%)'].mean()\n",
    "\n",
    "        all_results.append({\n",
    "            'Model': model_name, 'Dataset': ds_name, 'Type': 'Sklearn',\n",
    "            'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Avg RelMAE (%)': avg_rel_mae,\n",
    "            'Time (s)': train_time, 'N_test': len(y_true)\n",
    "        })\n",
    "        print(f\"  {model_name:20s} | {train_time:5.1f}s | MAE: {mae:7.2f} | RelMAE: {avg_rel_mae:6.2f}% | R2: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nSklearn done: {len(all_results)} results\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "ee563817"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralForecast Models — Full 24h and Night=0 Only\n",
    "\n",
    "Daytime dataset has 14h gaps every night — NF models need continuous hourly series."
   ],
   "id": "850f2e90"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def to_nf_format(df, features, target='count', id_col='beach'):\n",
    "    nf_df = df[['datetime', id_col, target] + features].copy()\n",
    "    nf_df = nf_df.rename(columns={'datetime': 'ds', id_col: 'unique_id', target: 'y'})\n",
    "    return nf_df\n",
    "\n",
    "def split_at_long_gaps(nf_df, max_gap_hours=MAX_GAP_HOURS, min_segment_hours=MIN_SEGMENT_HOURS):\n",
    "    result = []\n",
    "    for uid in nf_df['unique_id'].unique():\n",
    "        s = nf_df[nf_df['unique_id'] == uid].sort_values('ds').copy()\n",
    "        gaps = s['ds'].diff().dt.total_seconds() / 3600\n",
    "        split_points = gaps[gaps > max_gap_hours].index.tolist()\n",
    "\n",
    "        segments = []\n",
    "        prev = s.index[0]\n",
    "        for sp in split_points:\n",
    "            segments.append(s.loc[prev:s.index[s.index.get_loc(sp) - 1]])\n",
    "            prev = sp\n",
    "        segments.append(s.loc[prev:])\n",
    "\n",
    "        for i, seg in enumerate(segments):\n",
    "            if len(seg) >= min_segment_hours:\n",
    "                seg = seg.copy()\n",
    "                seg['unique_id'] = f\"{uid}__seg{i}\" if len(segments) > 1 else uid\n",
    "                result.append(seg)\n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "def prepare_nf_data(ds_name):\n",
    "    s = splits[ds_name]\n",
    "    trainval = pd.concat([s['df_train'], s['df_val']])\n",
    "    nf_trainval = to_nf_format(trainval, ALL_FEATURES, id_col=beach_col)\n",
    "    nf_test = to_nf_format(s['df_test'], ALL_FEATURES, id_col=beach_col)\n",
    "\n",
    "    nf_trainval = split_at_long_gaps(nf_trainval)\n",
    "    nf_test = split_at_long_gaps(nf_test)\n",
    "\n",
    "    nf_trainval = uf_fill_gaps(nf_trainval, freq='h')\n",
    "    nf_test = uf_fill_gaps(nf_test, freq='h')\n",
    "\n",
    "    for col in ['y'] + ALL_FEATURES:\n",
    "        if col in nf_trainval.columns:\n",
    "            nf_trainval[col] = nf_trainval.groupby('unique_id')[col].ffill().bfill()\n",
    "        if col in nf_test.columns:\n",
    "            nf_test[col] = nf_test.groupby('unique_id')[col].ffill().bfill()\n",
    "\n",
    "    common = set(nf_trainval['unique_id'].unique()) & set(nf_test['unique_id'].unique())\n",
    "    nf_trainval = nf_trainval[nf_trainval['unique_id'].isin(common)]\n",
    "    nf_test = nf_test[nf_test['unique_id'].isin(common)]\n",
    "\n",
    "    return nf_trainval, nf_test, len(common)\n",
    "\n",
    "nf_datasets = {}\n",
    "for ds_name in ['Full 24h', 'Night=0']:\n",
    "    tv, te, ns = prepare_nf_data(ds_name)\n",
    "    nf_datasets[ds_name] = {'trainval': tv, 'test': te, 'n_series': ns}\n",
    "    print(f\"{ds_name}: train+val={len(tv)}, test={len(te)}, series={ns}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "7645dda1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_nf_models(n_series, hist_exog):\n",
    "    common = dict(\n",
    "        h=1, input_size=INPUT_SIZE, max_steps=MAX_STEPS,\n",
    "        early_stop_patience_steps=EARLY_STOP_PATIENCE, scaler_type='robust',\n",
    "        learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE,\n",
    "        val_check_steps=50, random_seed=42, start_padding_enabled=True,\n",
    "    )\n",
    "    return [\n",
    "        ('LSTM', NF_LSTM(hist_exog_list=hist_exog, encoder_n_layers=2, encoder_hidden_size=128, decoder_hidden_size=128, decoder_layers=2, **common)),\n",
    "        ('NHITS', NHITS(hist_exog_list=hist_exog, **common)),\n",
    "        ('NBEATSx', NBEATSx(hist_exog_list=hist_exog, stack_types=['identity', 'identity', 'identity'], **common)),\n",
    "        ('TFT', TFT(hist_exog_list=hist_exog, hidden_size=64, **common)),\n",
    "        ('TCN', TCN(hist_exog_list=hist_exog, **common)),\n",
    "        ('TiDE', TiDE(hist_exog_list=hist_exog, **common)),\n",
    "        ('NF_MLP', NF_MLP(hist_exog_list=hist_exog, **common)),\n",
    "        ('PatchTST', PatchTST(**common)),\n",
    "        ('TimeMixer', TimeMixer(n_series=n_series, **common)),\n",
    "        ('TSMixerx', TSMixerx(n_series=n_series, **common)),\n",
    "        ('iTransformer', iTransformer(n_series=n_series, **common)),\n",
    "    ]\n",
    "\n",
    "if HAS_NF:\n",
    "    for ds_name, nf_data in nf_datasets.items():\n",
    "        tv, te, ns = nf_data['trainval'], nf_data['test'], nf_data['n_series']\n",
    "        min_len = tv.groupby('unique_id').size().min()\n",
    "        val_size = max(24, min(min_len // 5, 200))\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"NEURALFORECAST — {ds_name} (series={ns}, val_size={val_size})\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        for model_name, model in get_nf_models(ns, ALL_FEATURES):\n",
    "            print(f\"\\n  {model_name}...\")\n",
    "            try:\n",
    "                start = time.time()\n",
    "                nf = NeuralForecast(models=[model], freq='h')\n",
    "                nf.fit(df=tv, val_size=val_size)\n",
    "                train_time = time.time() - start\n",
    "\n",
    "                preds = nf.predict(df=tv).reset_index()\n",
    "                pred_col = [c for c in preds.columns if c not in ['unique_id', 'ds']][0]\n",
    "                merged = te.merge(preds[['unique_id', 'ds', pred_col]], on=['unique_id', 'ds'], how='inner')\n",
    "\n",
    "                if len(merged) == 0:\n",
    "                    raise ValueError(\"No matching predictions\")\n",
    "\n",
    "                y_pred = np.clip(merged[pred_col].values, 0, None)\n",
    "                y_true = merged['y'].values\n",
    "\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "                eval_df = merged.rename(columns={'unique_id': beach_col, 'y': 'count'}).copy()\n",
    "                beach_df = evaluate_per_beach(eval_df, y_pred, beach_col)\n",
    "                beach_df['model'] = model_name\n",
    "                beach_df['dataset'] = ds_name\n",
    "                beach_df['model_type'] = 'NeuralForecast'\n",
    "                all_beach_results.append(beach_df)\n",
    "\n",
    "                avg_rel_mae = beach_df['RelMAE (%)'].mean()\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Avg RelMAE (%)': avg_rel_mae,\n",
    "                    'Time (s)': train_time, 'N_test': len(merged)\n",
    "                })\n",
    "                print(f\"    {train_time:.1f}s | MAE: {mae:.2f} | RelMAE: {avg_rel_mae:.2f}% | R2: {r2:.4f} | matched: {len(merged)}/{len(te)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: {e}\")\n",
    "                all_results.append({\n",
    "                    'Model': model_name, 'Dataset': ds_name, 'Type': 'NeuralForecast',\n",
    "                    'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan, 'Avg RelMAE (%)': np.nan,\n",
    "                    'Time (s)': np.nan, 'N_test': 0\n",
    "                })"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e06c4c23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "id": "5fc38e96"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_df = pd.DataFrame(all_results).sort_values(['Dataset', 'Avg RelMAE (%)'])\n",
    "beach_results_df = pd.concat(all_beach_results, ignore_index=True)\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(save_dir / 'results_comparison.csv', index=False)\n",
    "beach_results_df.to_csv(save_dir / 'beach_results_comparison.csv', index=False)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].copy()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {ds_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(sub[['Model', 'Type', 'MAE', 'RMSE', 'R2', 'Avg RelMAE (%)', 'Time (s)']].to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d9d55189"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pivot = results_df.pivot_table(index='Model', columns='Dataset', values='Avg RelMAE (%)', aggfunc='first')\n",
    "pivot = pivot.sort_values(pivot.columns[0], na_position='last')\n",
    "print(\"\\nRelMAE (%) — Model × Dataset:\")\n",
    "print(pivot.round(2).to_string())"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "6f2eede1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ],
   "id": "df20e5b3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "pivot_plot = results_df.pivot_table(index='Model', columns='Dataset', values='Avg RelMAE (%)', aggfunc='first')\n",
    "models_sorted = pivot_plot.mean(axis=1).sort_values().index\n",
    "pivot_plot = pivot_plot.loc[models_sorted]\n",
    "\n",
    "x = np.arange(len(pivot_plot))\n",
    "width = 0.25\n",
    "ds_names = list(datasets.keys())\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
    "\n",
    "for i, ds in enumerate(ds_names):\n",
    "    if ds in pivot_plot.columns:\n",
    "        vals = pivot_plot[ds].values\n",
    "        bars = ax.bar(x + i * width, vals, width, label=ds, color=colors[i], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Avg RelMAE (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Across Datasets — Relative MAE (lower is better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(pivot_plot.index, rotation=45, ha='right')\n",
    "ax.legend(title='Dataset')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'grouped_bar_relmae.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d2f35a08"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, len(datasets), figsize=(20, 8), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = beach_results_df[beach_results_df['dataset'] == ds_name]\n",
    "    if len(sub) == 0:\n",
    "        axes[idx].set_title(f'{ds_name}\\n(no NF results)')\n",
    "        continue\n",
    "    heatmap_data = sub.pivot_table(index='beach', columns='model', values='RelMAE (%)', aggfunc='first')\n",
    "    heatmap_data = heatmap_data.reindex(columns=heatmap_data.mean().sort_values().index)\n",
    "\n",
    "    im = axes[idx].imshow(heatmap_data.values, cmap='RdYlGn_r', aspect='auto')\n",
    "    axes[idx].set_xticks(range(len(heatmap_data.columns)))\n",
    "    axes[idx].set_xticklabels(heatmap_data.columns, rotation=45, ha='right', fontsize=8)\n",
    "    if idx == 0:\n",
    "        axes[idx].set_yticks(range(len(heatmap_data.index)))\n",
    "        axes[idx].set_yticklabels(heatmap_data.index, fontsize=8)\n",
    "    axes[idx].set_title(f'{ds_name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "    for i in range(len(heatmap_data.index)):\n",
    "        for j in range(len(heatmap_data.columns)):\n",
    "            val = heatmap_data.values[i, j]\n",
    "            if not np.isnan(val):\n",
    "                axes[idx].text(j, i, f'{val:.0f}', ha='center', va='center', fontsize=6)\n",
    "\n",
    "fig.colorbar(im, ax=axes, label='RelMAE (%)', shrink=0.6)\n",
    "fig.suptitle('Per-Beach Relative MAE (%) — Heatmap by Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'beach_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "410dde8c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "pivot_r2 = results_df.pivot_table(index='Model', columns='Dataset', values='R2', aggfunc='first')\n",
    "pivot_r2 = pivot_r2.loc[models_sorted]\n",
    "\n",
    "x = np.arange(len(pivot_r2))\n",
    "for i, ds in enumerate(ds_names):\n",
    "    if ds in pivot_r2.columns:\n",
    "        vals = pivot_r2[ds].values\n",
    "        ax.bar(x + i * width, vals, width, label=ds, color=colors[i], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('R²', fontsize=12)\n",
    "ax.set_title('R² Score Across Datasets (higher is better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(pivot_r2.index, rotation=45, ha='right')\n",
    "ax.legend(title='Dataset')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'grouped_bar_r2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "459e547a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].dropna()\n",
    "    ax = axes[idx]\n",
    "    for _, row in sub.iterrows():\n",
    "        c = '#2196F3' if row['Type'] == 'Sklearn' else '#FF5722'\n",
    "        ax.scatter(row['Time (s)'], row['Avg RelMAE (%)'], s=100, c=c, edgecolor='black', zorder=5)\n",
    "        ax.annotate(row['Model'], (row['Time (s)'], row['Avg RelMAE (%)']),\n",
    "                   fontsize=7, ha='left', va='bottom', xytext=(4, 4), textcoords='offset points')\n",
    "    ax.set_xlabel('Training Time (s)')\n",
    "    ax.set_title(f'{ds_name}', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel('Avg RelMAE (%)')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "fig.legend(handles=[Patch(color='#2196F3', label='Sklearn'), Patch(color='#FF5722', label='NeuralForecast')],\n",
    "           loc='upper right', fontsize=10)\n",
    "fig.suptitle('Efficiency: RelMAE vs Training Time (lower-left is better)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'efficiency_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "be8cf5be"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "models_both = results_df.groupby('Model').filter(lambda x: len(x) >= 2)['Model'].unique()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model_name in sorted(models_both):\n",
    "    sub = results_df[results_df['Model'] == model_name].sort_values('Dataset')\n",
    "    marker = 'o' if sub['Type'].iloc[0] == 'Sklearn' else 's'\n",
    "    ax.plot(sub['Dataset'], sub['Avg RelMAE (%)'], marker=marker, label=model_name, linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Dataset Strategy', fontsize=12)\n",
    "ax.set_ylabel('Avg RelMAE (%)', fontsize=12)\n",
    "ax.set_title('How Dataset Strategy Affects Each Model', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'dataset_impact.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f152b9ea"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "\n",
    "for idx, ds_name in enumerate(datasets.keys()):\n",
    "    sub = beach_results_df[beach_results_df['dataset'] == ds_name]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best_per_beach = sub.loc[sub.groupby('beach')['RelMAE (%)'].idxmin()]\n",
    "    counts = best_per_beach['model'].value_counts()\n",
    "\n",
    "    axes[idx].barh(counts.index, counts.values, color='steelblue', edgecolor='black')\n",
    "    axes[idx].set_xlabel('# Beaches where best')\n",
    "    axes[idx].set_title(f'{ds_name}', fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "fig.suptitle('Which Model Wins on Most Beaches? (by RelMAE)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'best_per_beach.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "6c406e85"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    sub = results_df[results_df['Dataset'] == ds_name].dropna()\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    best = sub.loc[sub['Avg RelMAE (%)'].idxmin()]\n",
    "    print(f\"\\n{ds_name}:\")\n",
    "    print(f\"  Best: {best['Model']} ({best['Type']}) — RelMAE: {best['Avg RelMAE (%)']:.2f}%, R2: {best['R2']:.4f}\")\n",
    "    sk = sub[sub['Type'] == 'Sklearn']\n",
    "    if len(sk) > 0:\n",
    "        best_sk = sk.loc[sk['Avg RelMAE (%)'].idxmin()]\n",
    "        print(f\"  Best Sklearn: {best_sk['Model']} — RelMAE: {best_sk['Avg RelMAE (%)']:.2f}%\")\n",
    "    nf = sub[sub['Type'] == 'NeuralForecast']\n",
    "    if len(nf) > 0:\n",
    "        best_nf = nf.loc[nf['Avg RelMAE (%)'].idxmin()]\n",
    "        print(f\"  Best NF: {best_nf['Model']} — RelMAE: {best_nf['Avg RelMAE (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal results: {len(results_df)}\")\n",
    "print(f\"Beach-level results: {len(beach_results_df)}\")\n",
    "print(f\"Saved to: {save_dir}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "62c79d2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
